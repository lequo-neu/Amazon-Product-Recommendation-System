{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Import modules and working path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install flask flask-cors flask-jwt-extended polars numpy scipy scikit-learn python-dotenv gunicorn pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json, gzip, csv, urllib.request, shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow\n",
    "import numpy as np\n",
    "\n",
    "module_path = str((Path(\"..\") / \"utilities\").resolve())\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from logger import Logger\n",
    "from configurations import Configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Get configuration variables and initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logger\n",
    "LOG_FILE = Configurations.LOG_PATH\n",
    "logger = Logger(process_name=\"data_collection\", log_file=LOG_FILE)\n",
    "\n",
    "# Define the folder to store the raw data\n",
    "RAW_DIR = Path(Configurations.DATA_RAW_PATH)\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the folder to store the processed datain_path\n",
    "PROCESSED_DIR = Path(Configurations.DATA_PROCESSED_PATH)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Amazon Reviews dataset, collected in 2023 by McAuley Lab\n",
    "CATEGORIES = Configurations.CATEGORIES\n",
    "CORES = Configurations.CORES\n",
    "SPLITS = Configurations.SPLITS\n",
    "BASE_URL = Configurations.BASE_URL\n",
    "meta_base_url = \"https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_{category}.jsonl.gz\"\n",
    "\n",
    "# Build meta URL map from categories\n",
    "meta_urls = {cat: meta_base_url.format(category=cat) for cat in CATEGORIES}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build candidate subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_url(core: str, category: str, split: str) -> str:\n",
    "    return f\"{BASE_URL}/{core}/last_out_w_his/{category}.{split}.csv.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local path for parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_path_for_parquet(core: str, category: str, split: str, sample: str = None, raw_dir=RAW_DIR) -> Path:\n",
    "    safe_cat = category.replace(\"/\", \"-\")\n",
    "\n",
    "    if raw_dir == RAW_DIR:\n",
    "        return RAW_DIR / f\"{safe_cat}.{core}.{split}.csv.gz\"\n",
    "    elif raw_dir == PROCESSED_DIR:\n",
    "        if sample is None:\n",
    "            return PROCESSED_DIR / f\"{safe_cat}.{core}.{split}.parquet\"\n",
    "        else:\n",
    "            return PROCESSED_DIR / f\"{safe_cat}.{core}.{split}.{sample}.parquet\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid directory: {raw_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, out_path: Path, max_retries: int = 3) -> None:\n",
    "    if out_path.exists() and out_path.stat().st_size > 0:\n",
    "        logger.log_info(f\"Exists, skip: {out_path.name}\")\n",
    "        return\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            attempt += 1\n",
    "            logger.log_info(f\"Downloading (attempt {attempt}/{max_retries}): {url}\")\n",
    "            tmp = str(out_path) + \".part\"\n",
    "            urllib.request.urlretrieve(url, tmp)\n",
    "            os.replace(tmp, out_path)\n",
    "            logger.log_info(f\"Saved: {out_path.name}\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            logger.log_warning(f\"Failed attempt {attempt} for {url}: {e}\")\n",
    "    raise RuntimeError(f\"Exceeded retries: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, polars as pl\n",
    "\n",
    "def sample_users_maintain_5core(df: pl.DataFrame, n_users: int, seed: int = 42) -> pl.DataFrame:\n",
    "    \"\"\"Sample users while maintaining distribution\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    all_users = df['user_id'].unique().to_list()\n",
    "    if n_users >= len(all_users):\n",
    "        return df\n",
    "    \n",
    "    sampled = np.random.choice(all_users, size=n_users, replace=False).tolist()\n",
    "    df_sampled = df.filter(pl.col('user_id').is_in(sampled))\n",
    "    return df_sampled\n",
    "\n",
    "\n",
    "def enforce_5core(df: pl.DataFrame):\n",
    "    \"\"\"Enforce 5-core: each user/item has ≥5 ratings\"\"\"\n",
    "    for iteration in range(10):\n",
    "        n_before = len(df)\n",
    "        \n",
    "        # Remove sparse items\n",
    "        item_counts = df.group_by('parent_asin').agg(pl.len().alias('n'))\n",
    "        valid_items = item_counts.filter(pl.col('n') >= 5)['parent_asin'].to_list()\n",
    "        df = df.filter(pl.col('parent_asin').is_in(valid_items))\n",
    "        \n",
    "        # Remove sparse users\n",
    "        user_counts = df.group_by('user_id').agg(pl.len().alias('n'))\n",
    "        valid_users = user_counts.filter(pl.col('n') >= 5)['user_id'].to_list()\n",
    "        df = df.filter(pl.col('user_id').is_in(valid_users))\n",
    "        \n",
    "        if n_before == len(df):\n",
    "            break\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def save_dataset_to_parquet(csv_gz_path: Path, out_parquet_path: Path):   \n",
    "    # Check if exists\n",
    "    if out_parquet_path.exists():\n",
    "        logger.log_info(f\"Skip: {out_parquet_path.name}\")\n",
    "        return\n",
    "    \n",
    "    # Read CSV.GZ\n",
    "    logger.log_info(f\"Reading: {csv_gz_path.name}\")\n",
    "    df = pl.from_pandas(\n",
    "        pd.read_csv(csv_gz_path, compression='gzip')[Configurations.COLUMNS]\n",
    "    )\n",
    "    \n",
    "    logger.log_info(f\"  Shape: {df.shape}\")\n",
    "    logger.log_info(f\"  Users: {df['user_id'].n_unique():,}\")\n",
    "    logger.log_info(f\"  Items: {df['parent_asin'].n_unique():,}\")\n",
    "    \n",
    "    # Save to Parquet\n",
    "    df.to_pandas().to_parquet(out_parquet_path, engine='pyarrow', index=False)\n",
    "    logger.log_info(f\"Saved: {out_parquet_path.name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom dataset helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION 1: Inspect Data\n",
    "# ============================================================================\n",
    "\n",
    "def inspect_data(df: pl.DataFrame, category: str):\n",
    "    \"\"\"Inspect and calculate statistics\"\"\"\n",
    "    \n",
    "    logger.log_info(f\"\\n{'='*70}\")\n",
    "    logger.log_info(f\"INSPECTING DATA: {category}\")\n",
    "    logger.log_info(f\"{'='*70}\")\n",
    "    \n",
    "    n_users = df['user_id'].n_unique()\n",
    "    n_items = df['parent_asin'].n_unique()\n",
    "    n_ratings = len(df)\n",
    "    \n",
    "    logger.log_info(f\"Total ratings: {n_ratings:,}\")\n",
    "    logger.log_info(f\"Total users:   {n_users:,}\")\n",
    "    logger.log_info(f\"Total items:   {n_items:,}\")\n",
    "    \n",
    "    # Per-user statistics\n",
    "    user_stats = df.group_by('user_id').agg([\n",
    "        pl.len().alias('n_ratings'),\n",
    "        pl.n_unique('parent_asin').alias('n_items')\n",
    "    ])\n",
    "    \n",
    "    avg_ratings_per_user = user_stats['n_ratings'].mean()\n",
    "    avg_items_per_user = user_stats['n_items'].mean()\n",
    "    \n",
    "    logger.log_info(f\"\\nPer-User Statistics:\")\n",
    "    logger.log_info(f\"  Avg ratings per user: {avg_ratings_per_user:.2f}\")\n",
    "    logger.log_info(f\"  Avg items per user:   {avg_items_per_user:.2f}\")\n",
    "    \n",
    "    logger.log_info(f\"\\nDistribution:\")\n",
    "    for pct in [50, 75, 90, 95]:\n",
    "        r_val = user_stats['n_ratings'].quantile(pct / 100)\n",
    "        i_val = user_stats['n_items'].quantile(pct / 100)\n",
    "        logger.log_info(f\"  {pct}th percentile: {r_val:.0f} ratings, {i_val:.0f} items\")\n",
    "    \n",
    "    logger.log_info(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'avg_ratings_per_user': avg_ratings_per_user,\n",
    "        'avg_items_per_user': avg_items_per_user\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "def create_custom_filtered_datasets(category: str, rating_mult: float = 2.0, item_mult: float = 2.0):\n",
    "    \"\"\"\n",
    "    Create custom filtered datasets\n",
    "    Only saves .filter.parquet files (no JSON)\n",
    "    \"\"\"\n",
    "    \n",
    "    safe_cat = category.replace('/', '-')\n",
    "    \n",
    "    logger.log_info(f\"\\nProcessing: {category}\")\n",
    "    logger.log_info(f\"Multipliers: {rating_mult}x ratings, {item_mult}x items\")\n",
    "    \n",
    "    # Paths\n",
    "    train_input = PROCESSED_DIR / f\"{safe_cat}.5core.train.parquet\"\n",
    "    valid_input = PROCESSED_DIR / f\"{safe_cat}.5core.valid.parquet\"\n",
    "    test_input = PROCESSED_DIR / f\"{safe_cat}.5core.test.parquet\"\n",
    "    \n",
    "    train_output = PROCESSED_DIR / f\"{safe_cat}.5core.train.filter.parquet\"\n",
    "    valid_output = PROCESSED_DIR / f\"{safe_cat}.5core.valid.filter.parquet\"\n",
    "    test_output = PROCESSED_DIR / f\"{safe_cat}.5core.test.filter.parquet\"\n",
    "    \n",
    "    # Check if all exist\n",
    "    if train_output.exists() and valid_output.exists() and test_output.exists():\n",
    "        logger.log_info(f\"Skip: All filter files exist\")\n",
    "        return\n",
    "    \n",
    "    # ========================================\n",
    "    # TRAIN\n",
    "    # ========================================\n",
    "    \n",
    "    if not train_output.exists():\n",
    "        logger.log_info(f\"\\nTRAIN:\")\n",
    "        \n",
    "        # Read\n",
    "        logger.log_info(f\"  Reading: {train_input.name}\")\n",
    "        df = pl.read_parquet(train_input)\n",
    "        logger.log_info(f\"  Shape: {df.shape}\")\n",
    "        \n",
    "        # Stats\n",
    "        user_stats = df.group_by('user_id').agg([\n",
    "            pl.len().alias('n_ratings'),\n",
    "            pl.n_unique('parent_asin').alias('n_items')\n",
    "        ])\n",
    "        \n",
    "        avg_r = user_stats['n_ratings'].mean()\n",
    "        avg_i = user_stats['n_items'].mean()\n",
    "        \n",
    "        logger.log_info(f\"  Avg ratings: {avg_r:.2f}\")\n",
    "        logger.log_info(f\"  Avg items: {avg_i:.2f}\")\n",
    "        \n",
    "        # Thresholds\n",
    "        min_r = avg_r * rating_mult\n",
    "        min_i = avg_i * item_mult\n",
    "        \n",
    "        logger.log_info(f\"  Min ratings: {min_r:.2f}\")\n",
    "        logger.log_info(f\"  Min items: {min_i:.2f}\")\n",
    "        \n",
    "        # Filter\n",
    "        active = (user_stats\n",
    "            .filter(pl.col('n_ratings') > min_r)\n",
    "            .filter(pl.col('n_items') > min_i)['user_id']\n",
    "            .to_list()\n",
    "        )\n",
    "        \n",
    "        logger.log_info(f\"  Active users: {len(active):,} of {user_stats.height:,} ({len(active)/user_stats.height*100:.1f}%)\")\n",
    "        \n",
    "        df_filtered = df.filter(pl.col('user_id').is_in(active))\n",
    "        \n",
    "        logger.log_info(f\"  Filtered: {len(df_filtered):,} ratings, {df_filtered['user_id'].n_unique():,} users, {df_filtered['parent_asin'].n_unique():,} items\")\n",
    "        \n",
    "        # Save\n",
    "        df_filtered.to_pandas().to_parquet(train_output, engine='pyarrow', index=False)\n",
    "        logger.log_info(f\"  Saved: {train_output.name}\")\n",
    "        logger.log_info(f\"  Shape: {df_filtered.shape}\")\n",
    "    \n",
    "    else:\n",
    "        logger.log_info(f\"\\nSkip: {train_output.name} exists\")\n",
    "    \n",
    "    # ========================================\n",
    "    # VALID\n",
    "    # ========================================\n",
    "    \n",
    "    if not valid_output.exists():\n",
    "        logger.log_info(f\"\\nVALID:\")\n",
    "        \n",
    "        # Read train users directly from train.filter.parquet\n",
    "        logger.log_info(f\"  Loading train users from: {train_output.name}\")\n",
    "        df_train_filtered = pl.read_parquet(train_output)\n",
    "        train_users = df_train_filtered['user_id'].unique().to_list()\n",
    "        train_items = df_train_filtered['parent_asin'].unique().to_list()\n",
    "        \n",
    "        logger.log_info(f\"  Train users: {len(train_users):,}\")\n",
    "        logger.log_info(f\"  Train items: {len(train_items):,}\") \n",
    "        \n",
    "        # Read valid\n",
    "        logger.log_info(f\"  Reading: {valid_input.name}\")\n",
    "        df = pl.read_parquet(valid_input)\n",
    "        logger.log_info(f\"  Shape: {df.shape}\")\n",
    "        \n",
    "        # Filter\n",
    "        df_filtered = df.filter(\n",
    "            pl.col('user_id').is_in(train_users) &\n",
    "            pl.col('parent_asin').is_in(train_items)\n",
    "        )\n",
    "        logger.log_info(f\"  Filtered: {len(df_filtered):,} ratings, {df_filtered['user_id'].n_unique():,} users\")\n",
    "        \n",
    "        # Save\n",
    "        df_filtered.to_pandas().to_parquet(valid_output, engine='pyarrow', index=False)\n",
    "        logger.log_info(f\"  Saved: {valid_output.name}\")\n",
    "        logger.log_info(f\"  Shape: {df_filtered.shape}\")\n",
    "    \n",
    "    else:\n",
    "        logger.log_info(f\"\\nSkip: {valid_output.name} exists\")\n",
    "    \n",
    "    # ========================================\n",
    "    # TEST\n",
    "    # ========================================\n",
    "    \n",
    "    if not test_output.exists():\n",
    "        logger.log_info(f\"\\nTEST:\")\n",
    "        \n",
    "        # Read train users directly from train.filter.parquet\n",
    "        logger.log_info(f\"  Loading train users from: {train_output.name}\")\n",
    "        df_train_filtered = pl.read_parquet(train_output)\n",
    "        train_users = df_train_filtered['user_id'].unique().to_list()\n",
    "        \n",
    "        logger.log_info(f\"  Train users: {len(train_users):,}\")\n",
    "        \n",
    "        # Read test\n",
    "        logger.log_info(f\"  Reading: {test_input.name}\")\n",
    "        df = pl.read_parquet(test_input)\n",
    "        logger.log_info(f\"  Shape: {df.shape}\")\n",
    "        \n",
    "        # Filter\n",
    "        df_test_filtered = df.filter(\n",
    "            pl.col('user_id').is_in(train_users) &\n",
    "            pl.col('parent_asin').is_in(train_items)\n",
    "        )\n",
    "        logger.log_info(f\"  Filtered: {len(df_filtered):,} ratings, {df_filtered['user_id'].n_unique():,} users\")\n",
    "        \n",
    "        # Save\n",
    "        df_filtered.to_pandas().to_parquet(test_output, engine='pyarrow', index=False)\n",
    "        logger.log_info(f\"  Saved: {test_output.name}\")\n",
    "        logger.log_info(f\"  Shape: {df_filtered.shape}\")\n",
    "    \n",
    "    else:\n",
    "        logger.log_info(f\"\\nSkip: {test_output.name} exists\")\n",
    "    \n",
    "    logger.log_info(f\"\\nCompleted: {category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_active_users(df: pl.DataFrame, stats: dict, rating_multiplier: float = 2.0, item_multiplier: float = 2.0):\n",
    "    \"\"\"\n",
    "    Filter users: n_ratings > 2×avg AND n_items > 2×avg\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.log_info(f\"{'='*70}\")\n",
    "    logger.log_info(f\"FILTERING ACTIVE USERS\")\n",
    "    logger.log_info(f\"{'='*70}\")\n",
    "    \n",
    "    min_ratings = stats['avg_ratings_per_user'] * rating_multiplier\n",
    "    min_items = stats['avg_items_per_user'] * item_multiplier\n",
    "    \n",
    "    logger.log_info(f\"Thresholds:\")\n",
    "    logger.log_info(f\"  Min ratings: {min_ratings:.2f} (> {rating_multiplier}× avg)\")\n",
    "    logger.log_info(f\"  Min items:   {min_items:.2f} (> {item_multiplier}× avg)\")\n",
    "    \n",
    "    # Calculate per-user stats\n",
    "    user_stats = df.group_by('user_id').agg([\n",
    "        pl.len().alias('n_ratings'),\n",
    "        pl.n_unique('parent_asin').alias('n_items')\n",
    "    ])\n",
    "    \n",
    "    original_users = user_stats.height\n",
    "    \n",
    "    # Filter by ratings\n",
    "    users_by_ratings = user_stats.filter(pl.col('n_ratings') > min_ratings)\n",
    "    logger.log_info(f\"\\nStep 1: Filter by ratings > {min_ratings:.2f}\")\n",
    "    logger.log_info(f\"  {original_users:,} → {users_by_ratings.height:,} users ({users_by_ratings.height/original_users*100:.1f}%)\")\n",
    "    \n",
    "    # Filter by items\n",
    "    active_users = users_by_ratings.filter(pl.col('n_items') > min_items)\n",
    "    active_user_ids = active_users['user_id'].to_list()\n",
    "    \n",
    "    logger.log_info(f\"\\nStep 2: Filter by items > {min_items:.2f}\")\n",
    "    logger.log_info(f\"  {users_by_ratings.height:,} → {len(active_user_ids):,} users ({len(active_user_ids)/users_by_ratings.height*100:.1f}%)\")\n",
    "    \n",
    "    # Apply filter to dataset\n",
    "    df_filtered = df.filter(pl.col('user_id').is_in(active_user_ids))\n",
    "    \n",
    "    logger.log_info(f\"\\n{'='*70}\")\n",
    "    logger.log_info(f\"RESULT:\")\n",
    "    logger.log_info(f\"  Users:   {df_filtered['user_id'].n_unique():,}\")\n",
    "    logger.log_info(f\"  Items:   {df_filtered['parent_asin'].n_unique():,}\")\n",
    "    logger.log_info(f\"  Ratings: {len(df_filtered):,}\")\n",
    "    logger.log_info(f\"  Shape:   {df_filtered.shape}\")\n",
    "    logger.log_info(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return df_filtered, active_user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_category_custom(category: str, rating_mult: float = 2.0, item_mult: float = 2.0):   \n",
    "    safe_cat = category.replace('/', '-')\n",
    "    \n",
    "    logger.log_info(f\"\\n{'='*70}\")\n",
    "    logger.log_info(f\"PROCESSING CATEGORY: {category}\")\n",
    "    logger.log_info(f\"{'='*70}\")\n",
    "    \n",
    "    # File paths\n",
    "    train_input = PROCESSED_DIR / f\"{safe_cat}.5core.train.parquet\"\n",
    "    valid_input = PROCESSED_DIR / f\"{safe_cat}.5core.valid.parquet\"\n",
    "    test_input = PROCESSED_DIR / f\"{safe_cat}.5core.test.parquet\"\n",
    "    \n",
    "    train_output = PROCESSED_DIR / f\"{safe_cat}.5core.train.filter.parquet\"\n",
    "    valid_output = PROCESSED_DIR / f\"{safe_cat}.5core.valid.filter.parquet\"\n",
    "    test_output = PROCESSED_DIR / f\"{safe_cat}.5core.test.filter.parquet\"\n",
    "    users_output = PROCESSED_DIR / f\"{safe_cat}.5core.train.filter.parquet\"\n",
    "    \n",
    "    # Check if all outputs exist\n",
    "    if all([train_output.exists(), valid_output.exists(), test_output.exists()]):\n",
    "        logger.log_info(f\"Skip: All custom files exist for {category}\\n\")\n",
    "        return\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 1: Process TRAIN\n",
    "    # ========================================\n",
    "    \n",
    "    if not train_output.exists():\n",
    "        logger.log_info(f\"\\n{'='*70}\")\n",
    "        logger.log_info(f\"STEP 1: CREATE TRAIN.CUSTOM\")\n",
    "        logger.log_info(f\"{'='*70}\")\n",
    "        \n",
    "        # Read original train.parquet\n",
    "        logger.log_info(f\"Reading: {train_input.name}\")\n",
    "        df_train = pl.read_parquet(train_input)\n",
    "        logger.log_info(f\"  Loaded: {df_train.shape}\")\n",
    "        \n",
    "        # Inspect\n",
    "        stats = inspect_data(df_train, category)\n",
    "        \n",
    "        # Filter\n",
    "        df_train_filtered, train_users = filter_active_users(\n",
    "            df_train, \n",
    "            stats, \n",
    "            rating_mult, \n",
    "            item_mult\n",
    "        )\n",
    "        \n",
    "        # Save\n",
    "        df_train_filtered.to_pandas().to_parquet(train_output, index=False)\n",
    "        logger.log_info(f\"  Saved: {train_output.name}\")\n",
    "        logger.log_info(f\"  Shape: {df_train_filtered.shape}\\n\")\n",
    "        \n",
    "        # Save users list\n",
    "        with open(users_output, 'w') as f:\n",
    "            json.dump({'train_users': train_users}, f)\n",
    "        logger.log_info(f\"  Saved: {users_output.name} ({len(train_users):,} users)\\n\")\n",
    "    \n",
    "    else:\n",
    "        logger.log_info(f\"\\nSkip: {train_output.name} exists\")\n",
    "        # Load users for valid/test filtering\n",
    "        with open(users_output, 'r') as f:\n",
    "            train_users = json.load(f)['train_users']\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 2: Process VALID\n",
    "    # ========================================\n",
    "    \n",
    "    if not valid_output.exists():\n",
    "        logger.log_info(f\"\\n{'='*70}\")\n",
    "        logger.log_info(f\"STEP 2: CREATE VALID.CUSTOM\")\n",
    "        logger.log_info(f\"{'='*70}\")\n",
    "        \n",
    "        # Load users if not already loaded\n",
    "        if not train_output.exists():\n",
    "            with open(users_output, 'r') as f:\n",
    "                train_users = json.load(f)['train_users']\n",
    "        \n",
    "        # Read original valid.parquet\n",
    "        logger.log_info(f\"Reading: {valid_input.name}\")\n",
    "        df_valid = pl.read_parquet(valid_input)\n",
    "        logger.log_info(f\"  Loaded: {df_valid.shape}\")\n",
    "        logger.log_info(f\"  Users: {df_valid['user_id'].n_unique():,}\")\n",
    "        \n",
    "        # Filter by train users\n",
    "        logger.log_info(f\"\\nFiltering by train users ({len(train_users):,})...\")\n",
    "        df_valid_filtered = df_valid.filter(pl.col('user_id').is_in(train_users))\n",
    "        \n",
    "        logger.log_info(f\"  Filtered: {df_valid_filtered['user_id'].n_unique():,} users\")\n",
    "        logger.log_info(f\"  Ratings:  {len(df_valid_filtered):,}\")\n",
    "        \n",
    "        # Save\n",
    "        # df_valid_filtered.write_parquet(valid_output) #df_save.to_pandas().to_parquet(out_path, index=False)\n",
    "        df_valid_filtered.to_pandas().to_parquet(valid_output, index=False)\n",
    "        logger.log_info(f\"  Saved: {valid_output.name}\")\n",
    "        logger.log_info(f\"  Shape: {df_valid_filtered.shape}\\n\")\n",
    "    \n",
    "    else:\n",
    "        logger.log_info(f\"\\nSkip: {valid_output.name} exists\")\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 3: Process TEST\n",
    "    # ========================================\n",
    "    \n",
    "    if not test_output.exists():\n",
    "        logger.log_info(f\"\\n{'='*70}\")\n",
    "        logger.log_info(f\"STEP 3: CREATE TEST.CUSTOM\")\n",
    "        logger.log_info(f\"{'='*70}\")\n",
    "        \n",
    "        # Load users if not already loaded\n",
    "        if not train_output.exists() and not valid_output.exists():\n",
    "            with open(users_output, 'r') as f:\n",
    "                train_users = json.load(f)['train_users']\n",
    "        \n",
    "        # Read original test.parquet\n",
    "        logger.log_info(f\"Reading: {test_input.name}\")\n",
    "        df_test = pl.read_parquet(test_input)\n",
    "        logger.log_info(f\"  Loaded: {df_test.shape}\")\n",
    "        logger.log_info(f\"  Users: {df_test['user_id'].n_unique():,}\")\n",
    "        \n",
    "        # Filter by train users\n",
    "        logger.log_info(f\"\\nFiltering by train users ({len(train_users):,})...\")\n",
    "        df_test_filtered = df_test.filter(pl.col('user_id').is_in(train_users))\n",
    "        \n",
    "        logger.log_info(f\"  Filtered: {df_test_filtered['user_id'].n_unique():,} users\")\n",
    "        logger.log_info(f\"  Ratings:  {len(df_test_filtered):,}\")\n",
    "        \n",
    "        # Save\n",
    "        # df_test_filtered.write_parquet(test_output)\n",
    "        df_test_filtered.to_pandas().to_parquet(test_output, index=False)\n",
    "        logger.log_info(f\"  Saved: {test_output.name}\")\n",
    "        logger.log_info(f\"  Shape: {df_test_filtered.shape}\\n\")\n",
    "    \n",
    "    else:\n",
    "        logger.log_info(f\"\\nSkip: {test_output.name} exists\")\n",
    "    \n",
    "    logger.log_info(f\"{'='*70}\")\n",
    "    logger.log_info(f\"  COMPLETED: {category}\")\n",
    "    logger.log_info(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_custom_filtering(rating_mult=2.0, item_mult=2.0):\n",
    "    \"\"\"\n",
    "    Run custom filtering for all categories\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.log_info(\"=\"*70)\n",
    "    logger.log_info(\"CREATE CUSTOM FILTERED DATASETS\")\n",
    "    logger.log_info(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    for category in Configurations.CATEGORIES:\n",
    "        try:\n",
    "            create_custom_filtered_datasets(category, rating_mult, item_mult)\n",
    "        except Exception as e:\n",
    "            logger.log_exception(f\"Error processing {category}: {e}\")\n",
    "    \n",
    "    logger.log_info(\"\\n\" + \"=\"*70)\n",
    "    logger.log_info(\"COMPLETED\")\n",
    "    logger.log_info(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter dataset helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_sample(input_path: Path, n: int, item_mult: float = 1.0, n_name_out: str = None):\n",
    "    # Parse input filename\n",
    "    stem = input_path.stem  # e.g., \"Electronics.5core.train.filter\"\n",
    "    parts = stem.split('.')\n",
    "    \n",
    "    # Extract components\n",
    "    category = parts[0]\n",
    "    core = parts[1] if len(parts) > 1 else '5core'\n",
    "    split = parts[2] if len(parts) > 2 else 'train'\n",
    "    \n",
    "    # Create output path (replace last suffix with n)\n",
    "    output_path = input_path.parent / f\"{category}.{core}.{split}.{n_name_out}.parquet\"\n",
    "    \n",
    "    # Check exists\n",
    "    if output_path.exists():\n",
    "        logger.log_info(f\"Skip: {output_path.name}\")\n",
    "        return\n",
    "    \n",
    "    logger.log_info(f\"\\nSampling {n} users\")\n",
    "    logger.log_info(f\"  Input:  {input_path.name}\")\n",
    "    logger.log_info(f\"  Output: {output_path.name}\")\n",
    "    \n",
    "    # Read input\n",
    "    df = pl.read_parquet(input_path)\n",
    "    total = df['user_id'].n_unique()\n",
    "    \n",
    "    logger.log_info(f\"  Total users: {total:,}\")\n",
    "    \n",
    "    # Handle train vs valid/test\n",
    "    if split == 'train':\n",
    "        # TRAIN: Sample top N active users\n",
    "        if n >= total:\n",
    "            df_sampled = df\n",
    "            users_n = df['user_id'].unique().to_list()\n",
    "        else:\n",
    "            activity = df.group_by('user_id').agg(pl.len().alias('n')).sort('n', descending=True)\n",
    "            users_n = activity.head(n)['user_id'].to_list()\n",
    "            df_sampled = df.filter(pl.col('user_id').is_in(users_n))\n",
    "        \n",
    "        # Add filter rating/item counts\n",
    "        if item_mult > 1.0:\n",
    "            logger.log_info(f\"  Before item filter: {df_sampled.shape} shape, {len(df_sampled):,} ratings, {df_sampled['user_id'].n_unique():,}  users, {df_sampled['parent_asin'].n_unique():,} items\")\n",
    "\n",
    "            item_counts = df_sampled.group_by('parent_asin').agg(pl.len().alias('n'))\n",
    "            avg_item = item_counts['n'].mean()\n",
    "            min_item = avg_item * item_mult\n",
    "            \n",
    "            logger.log_info(f\"  Avg ratings/item: {avg_item:.2f}\")\n",
    "            logger.log_info(f\"  Min threshold: {min_item:.2f} ({item_mult}x avg)\")\n",
    "            \n",
    "            popular = item_counts.filter(pl.col('n') > min_item)['parent_asin'].to_list()\n",
    "            df_sampled = df_sampled.filter(pl.col('parent_asin').is_in(popular))\n",
    "            \n",
    "            logger.log_info(f\"  After item filter: {df_sampled.shape} shape, {len(df_sampled):,} ratings, {df_sampled['user_id'].n_unique():,}  users, {df_sampled['parent_asin'].n_unique():,} items\")\n",
    "            \n",
    "        logger.log_info(f\"  Sampled: {df_sampled.shape} shape, {len(df_sampled):,} ratings, {len(users_n):,} users\")       \n",
    "    else:\n",
    "        # VALID/TEST: Filter by train users\n",
    "        train_path = input_path.parent / f\"{category}.{core}.train.{n_name_out}.parquet\"\n",
    "        \n",
    "        if not train_path.exists():\n",
    "            logger.log_warning(f\"  Train sample not found: {train_path.name}\")\n",
    "            logger.log_warning(f\"  Create train sample first!\")\n",
    "            return\n",
    "        \n",
    "        # Get train users and items\n",
    "        df_train = pl.read_parquet(train_path)\n",
    "        users_n = df_train['user_id'].unique().to_list()\n",
    "        items_n = df_train['parent_asin'].unique().to_list()\n",
    "        \n",
    "        # Filter\n",
    "        df_sampled = df.filter(\n",
    "            pl.col('user_id').is_in(users_n) &\n",
    "            pl.col('parent_asin').is_in(items_n)\n",
    "        )\n",
    "        \n",
    "        logger.log_info(f\"  Filtered: {len(df_sampled):,} ratings, {df_sampled['user_id'].n_unique():,} users\")\n",
    "    \n",
    "    # Save\n",
    "    df_sampled.to_pandas().to_parquet(output_path, engine='pyarrow', index=False)\n",
    "    logger.log_info(f\"  Saved: {output_path.name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta data helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _meta_path(category: str):\n",
    "    \"\"\"Get path to raw metadata file\"\"\"\n",
    "    return RAW_DIR / f\"{category}.meta.jsonl.gz\"\n",
    "\n",
    "def download_meta(category: str, url: str = None):\n",
    "    \"\"\"Download raw metadata file\"\"\"\n",
    "    url = url or meta_urls.get(category)\n",
    "    if not url:\n",
    "        logger.log_warning(f\"[META] No URL for {category}\")\n",
    "        return\n",
    "    \n",
    "    dst = _meta_path(category)\n",
    "    if dst.exists() and dst.stat().st_size > 0:\n",
    "        logger.log_info(f\"[META] Skip: {dst.name}\")\n",
    "        return\n",
    "    \n",
    "    logger.log_info(f\"[META] Downloading: {category}\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, str(dst))\n",
    "        logger.log_info(f\"[META] {dst.name}\")\n",
    "    except Exception as e:\n",
    "        logger.log_exception(f\"[META] Failed: {e}\")\n",
    "\n",
    "def _process_images(img_list):\n",
    "    \"\"\"Keep first 3 images, hi_res and thumb only\"\"\"\n",
    "    if not img_list or not isinstance(img_list, list):\n",
    "        return []\n",
    "    return [{\"hi_res\": img.get(\"hi_res\"), \"thumb\": img.get(\"thumb\")} \n",
    "            for img in img_list[:3] if isinstance(img, dict)]\n",
    "\n",
    "\n",
    "def _process_description(desc, max_len=2000):\n",
    "    \"\"\"Merge list to string and truncate\"\"\"\n",
    "    if isinstance(desc, list):\n",
    "        desc = \" \".join(str(d) for d in desc if d)\n",
    "    elif not desc:\n",
    "        return \"\"\n",
    "    else:\n",
    "        desc = str(desc)\n",
    "    return desc[:max_len] + (\"...\" if len(desc) > max_len else \"\")\n",
    "\n",
    "\n",
    "def _process_list_field(field, max_items):\n",
    "    \"\"\"Flatten and limit list fields\"\"\"\n",
    "    if not isinstance(field, list):\n",
    "        return []\n",
    "    if field and isinstance(field[0], list):\n",
    "        field = field[0]\n",
    "    return field[:max_items]\n",
    "\n",
    "\n",
    "def _extract_item_metadata(obj):\n",
    "    \"\"\"Extract essential metadata from raw JSON\"\"\"\n",
    "    return {\n",
    "        \"parent_asin\": obj.get(\"parent_asin\"),\n",
    "        \"title\": obj.get(\"title\", \"\"),\n",
    "        \"price\": obj.get(\"price\"),\n",
    "        \"average_rating\": obj.get(\"average_rating\"),\n",
    "        \"rating_number\": obj.get(\"rating_number\"),\n",
    "        \"features\": _process_list_field(obj.get(\"features\"), 10),\n",
    "        \"description\": _process_description(obj.get(\"description\")),\n",
    "        \"categories\": _process_list_field(obj.get(\"categories\"), 5),\n",
    "        \"images\": _process_images(obj.get(\"images\")),\n",
    "        \"store\": obj.get(\"store\", \"\")\n",
    "    }\n",
    "\n",
    "\n",
    "def save_meta_for_training_ui(category: str):\n",
    "    \"\"\"\n",
    "    Save FULL metadata.\n",
    "    Hybrid recommendation need full metadata for cold-start.\n",
    "    \"\"\"\n",
    "    safe_cat = category.replace('/', '-')\n",
    "    out_path = PROCESSED_DIR / f\"{safe_cat}.meta.parquet\"\n",
    "    \n",
    "    if out_path.exists():\n",
    "        logger.log_info(f\"[META] Skip as exist for metadata: {out_path.name}\")\n",
    "        return out_path\n",
    "    \n",
    "    # Load raw metadata\n",
    "    fp = _meta_path(category)\n",
    "    if not fp.exists():\n",
    "        logger.log_warning(f\"[META] Not found: {fp}\")\n",
    "        return None\n",
    "    \n",
    "    logger.log_info(f\"[META] Reading: {fp.name}\")\n",
    "    \n",
    "    rows = []\n",
    "    with gzip.open(fp, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                if obj.get(\"parent_asin\"):\n",
    "                    rows.append(_extract_item_metadata(obj))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    if not rows:\n",
    "        logger.log_warning(f\"[META] No data for {category}\")\n",
    "        return None\n",
    "    \n",
    "    # Save\n",
    "    df = pd.DataFrame(rows)\n",
    "    for col in ['price', 'average_rating', 'rating_number']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    for col in ['title', 'description', 'store']:\n",
    "        df[col] = df[col].fillna('')\n",
    "    \n",
    "    df.to_parquet(out_path, index=False)\n",
    "    logger.log_info(f\"[META] {out_path.name}: {len(df):,} items\")\n",
    "    \n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Main execution logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    logger.log_info(\"=\"*70)\n",
    "    logger.log_info(\"DATASET COLLECTION\")\n",
    "    logger.log_info(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    for core in CORES:\n",
    "        for cat in CATEGORIES:\n",
    "            logger.log_info(f\"\\n{'='*70}\")\n",
    "            logger.log_info(f\"{cat}\")\n",
    "            logger.log_info(f\"{'='*70}\")\n",
    "            \n",
    "            # Download, create processed parquet files for raw data\n",
    "            # IMPORTANT: Process TRAIN first!\n",
    "            for split in Configurations.SPLITS:                \n",
    "                logger.log_info(f\"\\nProcessing {split.upper()}...\")\n",
    "                \n",
    "                url = build_url(core, cat, split)\n",
    "                in_path = local_path_for_parquet(core, cat, split, raw_dir=RAW_DIR)\n",
    "                out_path = local_path_for_parquet(core, cat, split, raw_dir=PROCESSED_DIR)\n",
    "                try:\n",
    "                    download_file(url, in_path)\n",
    "                    save_dataset_to_parquet(in_path, out_path)\n",
    "                except Exception as e:\n",
    "                    logger.log_exception(f\"{split} error: {e}\")\n",
    "\n",
    "    logger.log_info(\"\\n\" + \"=\"*70)\n",
    "    logger.log_info(\"METADATA\")\n",
    "    logger.log_info(\"=\"*70)\n",
    "    \n",
    "    for cat in CATEGORIES:\n",
    "        download_meta(cat)\n",
    "        save_meta_for_training_ui(cat)\n",
    "\n",
    "    logger.log_info(\"\\n\" + \"=\"*70)\n",
    "    logger.log_info(\"CREATING SAMPLES FROM PARQUET DATA\")\n",
    "    logger.log_info(\"=\"*70)\n",
    "    for core in CORES:\n",
    "        for cat in CATEGORIES:\n",
    "            for split in Configurations.SPLITS:\n",
    "                for sample in Configurations.SAMPLE_SIZES:\n",
    "                    if sample != \"full\":\n",
    "                        n = Configurations.SAMPLE_SIZES[sample]\n",
    "                        logger.log_info(f\"\\ncat={cat} - split={split} - sample={sample} - n={n} sampling...\")\n",
    "                        in_path = local_path_for_parquet(core, cat, split,raw_dir=PROCESSED_DIR)\n",
    "                        out_path = local_path_for_parquet(core, cat, split, sample, raw_dir=PROCESSED_DIR)\n",
    "                        logger.log_info(f\"Input: {in_path.name} \\n → Output: {out_path.name}\")\n",
    "                        create_n_sample(in_path, n, Configurations.ITEM_MULTI, sample)\n",
    "\n",
    "    logger.log_info(\"\\n COMPLETED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Unit test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 21:39:20,648 - INFO - ======================================================================\n",
      "2025-10-10 21:39:20,649 - INFO - DATASET COLLECTION\n",
      "2025-10-10 21:39:20,650 - INFO - ======================================================================\n",
      "\n",
      "2025-10-10 21:39:20,651 - INFO - \n",
      "======================================================================\n",
      "2025-10-10 21:39:20,651 - INFO - Electronics\n",
      "2025-10-10 21:39:20,652 - INFO - ======================================================================\n",
      "2025-10-10 21:39:20,652 - INFO - \n",
      "Processing TRAIN...\n",
      "2025-10-10 21:39:20,653 - INFO - Exists, skip: Electronics.5core.train.csv.gz\n",
      "2025-10-10 21:39:20,653 - INFO - Skip: Electronics.5core.train.parquet\n",
      "2025-10-10 21:39:20,653 - INFO - \n",
      "Processing VALID...\n",
      "2025-10-10 21:39:20,653 - INFO - Exists, skip: Electronics.5core.valid.csv.gz\n",
      "2025-10-10 21:39:20,655 - INFO - Skip: Electronics.5core.valid.parquet\n",
      "2025-10-10 21:39:20,655 - INFO - \n",
      "Processing TEST...\n",
      "2025-10-10 21:39:20,655 - INFO - Exists, skip: Electronics.5core.test.csv.gz\n",
      "2025-10-10 21:39:20,656 - INFO - Skip: Electronics.5core.test.parquet\n",
      "2025-10-10 21:39:20,656 - INFO - \n",
      "======================================================================\n",
      "2025-10-10 21:39:20,656 - INFO - Beauty_and_Personal_Care\n",
      "2025-10-10 21:39:20,657 - INFO - ======================================================================\n",
      "2025-10-10 21:39:20,657 - INFO - \n",
      "Processing TRAIN...\n",
      "2025-10-10 21:39:20,657 - INFO - Exists, skip: Beauty_and_Personal_Care.5core.train.csv.gz\n",
      "2025-10-10 21:39:20,658 - INFO - Skip: Beauty_and_Personal_Care.5core.train.parquet\n",
      "2025-10-10 21:39:20,658 - INFO - \n",
      "Processing VALID...\n",
      "2025-10-10 21:39:20,658 - INFO - Exists, skip: Beauty_and_Personal_Care.5core.valid.csv.gz\n",
      "2025-10-10 21:39:20,659 - INFO - Skip: Beauty_and_Personal_Care.5core.valid.parquet\n",
      "2025-10-10 21:39:20,659 - INFO - \n",
      "Processing TEST...\n",
      "2025-10-10 21:39:20,659 - INFO - Exists, skip: Beauty_and_Personal_Care.5core.test.csv.gz\n",
      "2025-10-10 21:39:20,660 - INFO - Skip: Beauty_and_Personal_Care.5core.test.parquet\n",
      "2025-10-10 21:39:20,660 - INFO - \n",
      "======================================================================\n",
      "2025-10-10 21:39:20,660 - INFO - Sports_and_Outdoors\n",
      "2025-10-10 21:39:20,660 - INFO - ======================================================================\n",
      "2025-10-10 21:39:20,660 - INFO - \n",
      "Processing TRAIN...\n",
      "2025-10-10 21:39:20,661 - INFO - Downloading (attempt 1/3): https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/5core/last_out_w_his/Sports_and_Outdoors.train.csv.gz\n",
      "2025-10-10 21:39:23,654 - INFO - Saved: Sports_and_Outdoors.5core.train.csv.gz\n",
      "2025-10-10 21:39:23,656 - INFO - Reading: Sports_and_Outdoors.5core.train.csv.gz\n",
      "2025-10-10 21:39:27,493 - INFO -   Shape: (2652476, 4)\n",
      "2025-10-10 21:39:27,561 - INFO -   Users: 409,772\n",
      "2025-10-10 21:39:27,686 - INFO -   Items: 155,870\n",
      "2025-10-10 21:39:28,417 - INFO - Saved: Sports_and_Outdoors.5core.train.parquet\n",
      "\n",
      "2025-10-10 21:39:28,420 - INFO - \n",
      "Processing VALID...\n",
      "2025-10-10 21:39:28,420 - INFO - Downloading (attempt 1/3): https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/5core/last_out_w_his/Sports_and_Outdoors.valid.csv.gz\n",
      "2025-10-10 21:39:30,422 - INFO - Saved: Sports_and_Outdoors.5core.valid.csv.gz\n",
      "2025-10-10 21:39:30,423 - INFO - Reading: Sports_and_Outdoors.5core.valid.csv.gz\n",
      "2025-10-10 21:39:31,125 - INFO -   Shape: (409772, 4)\n",
      "2025-10-10 21:39:31,132 - INFO -   Users: 409,772\n",
      "2025-10-10 21:39:31,140 - INFO -   Items: 107,369\n",
      "2025-10-10 21:39:31,285 - INFO - Saved: Sports_and_Outdoors.5core.valid.parquet\n",
      "\n",
      "2025-10-10 21:39:31,286 - INFO - \n",
      "Processing TEST...\n",
      "2025-10-10 21:39:31,286 - INFO - Downloading (attempt 1/3): https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/5core/last_out_w_his/Sports_and_Outdoors.test.csv.gz\n",
      "2025-10-10 21:39:33,413 - INFO - Saved: Sports_and_Outdoors.5core.test.csv.gz\n",
      "2025-10-10 21:39:33,414 - INFO - Reading: Sports_and_Outdoors.5core.test.csv.gz\n",
      "2025-10-10 21:39:34,175 - INFO -   Shape: (409772, 4)\n",
      "2025-10-10 21:39:34,182 - INFO -   Users: 409,772\n",
      "2025-10-10 21:39:34,191 - INFO -   Items: 101,579\n",
      "2025-10-10 21:39:34,331 - INFO - Saved: Sports_and_Outdoors.5core.test.parquet\n",
      "\n",
      "2025-10-10 21:39:34,332 - INFO - \n",
      "======================================================================\n",
      "2025-10-10 21:39:34,332 - INFO - METADATA\n",
      "2025-10-10 21:39:34,332 - INFO - ======================================================================\n",
      "2025-10-10 21:39:34,333 - INFO - [META] Skip: Electronics.meta.jsonl.gz\n",
      "2025-10-10 21:39:34,333 - INFO - [META] Skip as exist for metadata: Electronics.meta.parquet\n",
      "2025-10-10 21:39:34,333 - INFO - [META] Skip: Beauty_and_Personal_Care.meta.jsonl.gz\n",
      "2025-10-10 21:39:34,333 - INFO - [META] Skip as exist for metadata: Beauty_and_Personal_Care.meta.parquet\n",
      "2025-10-10 21:39:34,334 - INFO - [META] Downloading: Sports_and_Outdoors\n",
      "2025-10-10 21:40:27,895 - INFO - [META] Sports_and_Outdoors.meta.jsonl.gz\n",
      "2025-10-10 21:40:27,896 - INFO - [META] Reading: Sports_and_Outdoors.meta.jsonl.gz\n",
      "2025-10-10 21:41:13,673 - INFO - [META] Sports_and_Outdoors.meta.parquet: 1,587,421 items\n",
      "2025-10-10 21:41:17,214 - INFO - \n",
      "======================================================================\n",
      "2025-10-10 21:41:17,216 - INFO - CREATING SAMPLES FROM PARQUET DATA\n",
      "2025-10-10 21:41:17,216 - INFO - ======================================================================\n",
      "2025-10-10 21:41:17,216 - INFO - \n",
      "cat=Electronics - split=train - sample=small - n=2000 sampling...\n",
      "2025-10-10 21:41:17,217 - INFO - Input: Electronics.5core.train.parquet \n",
      " → Output: Electronics.5core.train.small.parquet\n",
      "2025-10-10 21:41:17,217 - INFO - Skip: Electronics.5core.train.small.parquet\n",
      "2025-10-10 21:41:17,217 - INFO - \n",
      "cat=Electronics - split=train - sample=medium - n=20000 sampling...\n",
      "2025-10-10 21:41:17,218 - INFO - Input: Electronics.5core.train.parquet \n",
      " → Output: Electronics.5core.train.medium.parquet\n",
      "2025-10-10 21:41:17,218 - INFO - Skip: Electronics.5core.train.medium.parquet\n",
      "2025-10-10 21:41:17,218 - INFO - \n",
      "cat=Electronics - split=train - sample=large - n=50000 sampling...\n",
      "2025-10-10 21:41:17,219 - INFO - Input: Electronics.5core.train.parquet \n",
      " → Output: Electronics.5core.train.large.parquet\n",
      "2025-10-10 21:41:17,219 - INFO - Skip: Electronics.5core.train.large.parquet\n",
      "2025-10-10 21:41:17,219 - INFO - \n",
      "cat=Electronics - split=train - sample=big - n=50000 sampling...\n",
      "2025-10-10 21:41:17,219 - INFO - Input: Electronics.5core.train.parquet \n",
      " → Output: Electronics.5core.train.big.parquet\n",
      "2025-10-10 21:41:17,220 - INFO - Skip: Electronics.5core.train.big.parquet\n",
      "2025-10-10 21:41:17,220 - INFO - \n",
      "cat=Electronics - split=valid - sample=small - n=2000 sampling...\n",
      "2025-10-10 21:41:17,220 - INFO - Input: Electronics.5core.valid.parquet \n",
      " → Output: Electronics.5core.valid.small.parquet\n",
      "2025-10-10 21:41:17,220 - INFO - Skip: Electronics.5core.valid.small.parquet\n",
      "2025-10-10 21:41:17,221 - INFO - \n",
      "cat=Electronics - split=valid - sample=medium - n=20000 sampling...\n",
      "2025-10-10 21:41:17,221 - INFO - Input: Electronics.5core.valid.parquet \n",
      " → Output: Electronics.5core.valid.medium.parquet\n",
      "2025-10-10 21:41:17,221 - INFO - Skip: Electronics.5core.valid.medium.parquet\n",
      "2025-10-10 21:41:17,222 - INFO - \n",
      "cat=Electronics - split=valid - sample=large - n=50000 sampling...\n",
      "2025-10-10 21:41:17,222 - INFO - Input: Electronics.5core.valid.parquet \n",
      " → Output: Electronics.5core.valid.large.parquet\n",
      "2025-10-10 21:41:17,222 - INFO - Skip: Electronics.5core.valid.large.parquet\n",
      "2025-10-10 21:41:17,222 - INFO - \n",
      "cat=Electronics - split=valid - sample=big - n=50000 sampling...\n",
      "2025-10-10 21:41:17,223 - INFO - Input: Electronics.5core.valid.parquet \n",
      " → Output: Electronics.5core.valid.big.parquet\n",
      "2025-10-10 21:41:17,223 - INFO - Skip: Electronics.5core.valid.big.parquet\n",
      "2025-10-10 21:41:17,223 - INFO - \n",
      "cat=Electronics - split=test - sample=small - n=2000 sampling...\n",
      "2025-10-10 21:41:17,223 - INFO - Input: Electronics.5core.test.parquet \n",
      " → Output: Electronics.5core.test.small.parquet\n",
      "2025-10-10 21:41:17,223 - INFO - Skip: Electronics.5core.test.small.parquet\n",
      "2025-10-10 21:41:17,224 - INFO - \n",
      "cat=Electronics - split=test - sample=medium - n=20000 sampling...\n",
      "2025-10-10 21:41:17,224 - INFO - Input: Electronics.5core.test.parquet \n",
      " → Output: Electronics.5core.test.medium.parquet\n",
      "2025-10-10 21:41:17,224 - INFO - Skip: Electronics.5core.test.medium.parquet\n",
      "2025-10-10 21:41:17,224 - INFO - \n",
      "cat=Electronics - split=test - sample=large - n=50000 sampling...\n",
      "2025-10-10 21:41:17,225 - INFO - Input: Electronics.5core.test.parquet \n",
      " → Output: Electronics.5core.test.large.parquet\n",
      "2025-10-10 21:41:17,225 - INFO - Skip: Electronics.5core.test.large.parquet\n",
      "2025-10-10 21:41:17,225 - INFO - \n",
      "cat=Electronics - split=test - sample=big - n=50000 sampling...\n",
      "2025-10-10 21:41:17,225 - INFO - Input: Electronics.5core.test.parquet \n",
      " → Output: Electronics.5core.test.big.parquet\n",
      "2025-10-10 21:41:17,225 - INFO - Skip: Electronics.5core.test.big.parquet\n",
      "2025-10-10 21:41:17,226 - INFO - \n",
      "cat=Beauty_and_Personal_Care - split=train - sample=small - n=2000 sampling...\n",
      "2025-10-10 21:41:17,226 - INFO - Input: Beauty_and_Personal_Care.5core.train.parquet \n",
      " → Output: Beauty_and_Personal_Care.5core.train.small.parquet\n",
      "2025-10-10 21:41:17,226 - INFO - \n",
      "Sampling 2000 users\n",
      "2025-10-10 21:41:17,226 - INFO -   Input:  Beauty_and_Personal_Care.5core.train.parquet\n",
      "2025-10-10 21:41:17,226 - INFO -   Output: Beauty_and_Personal_Care.5core.train.small.parquet\n",
      "2025-10-10 21:41:17,511 - INFO -   Total users: 729,576\n",
      "2025-10-10 21:41:17,701 - INFO -   Before item filter: (402895, 4) shape, 402,895 ratings, 2,000  users, 70,982 items\n",
      "2025-10-10 21:41:17,709 - INFO -   Avg ratings/item: 5.68\n",
      "2025-10-10 21:41:17,710 - INFO -   Min threshold: 510.84 (90x avg)\n",
      "2025-10-10 21:41:17,715 - INFO -   After item filter: (0, 4) shape, 0 ratings, 0  users, 0 items\n",
      "2025-10-10 21:41:17,715 - INFO -   Sampled: (0, 4) shape, 0 ratings, 2,000 users\n",
      "2025-10-10 21:41:17,726 - INFO -   Saved: Beauty_and_Personal_Care.5core.train.small.parquet\n",
      "\n",
      "2025-10-10 21:41:17,730 - INFO - \n",
      "cat=Beauty_and_Personal_Care - split=train - sample=medium - n=20000 sampling...\n",
      "2025-10-10 21:41:17,731 - INFO - Input: Beauty_and_Personal_Care.5core.train.parquet \n",
      " → Output: Beauty_and_Personal_Care.5core.train.medium.parquet\n",
      "2025-10-10 21:41:17,731 - INFO - \n",
      "Sampling 20000 users\n",
      "2025-10-10 21:41:17,731 - INFO -   Input:  Beauty_and_Personal_Care.5core.train.parquet\n",
      "2025-10-10 21:41:17,732 - INFO -   Output: Beauty_and_Personal_Care.5core.train.medium.parquet\n",
      "2025-10-10 21:41:17,878 - INFO -   Total users: 729,576\n",
      "2025-10-10 21:41:18,068 - INFO -   Before item filter: (1054001, 4) shape, 1,054,001 ratings, 20,000  users, 153,179 items\n",
      "2025-10-10 21:41:18,091 - INFO -   Avg ratings/item: 6.88\n",
      "2025-10-10 21:41:18,091 - INFO -   Min threshold: 619.28 (90x avg)\n",
      "2025-10-10 21:41:18,094 - INFO -   After item filter: (754, 4) shape, 754 ratings, 754  users, 1 items\n",
      "2025-10-10 21:41:18,094 - INFO -   Sampled: (754, 4) shape, 754 ratings, 20,000 users\n",
      "2025-10-10 21:41:18,138 - INFO -   Saved: Beauty_and_Personal_Care.5core.train.medium.parquet\n",
      "\n",
      "2025-10-10 21:41:18,142 - INFO - \n",
      "cat=Beauty_and_Personal_Care - split=train - sample=large - n=50000 sampling...\n",
      "2025-10-10 21:41:18,142 - INFO - Input: Beauty_and_Personal_Care.5core.train.parquet \n",
      " → Output: Beauty_and_Personal_Care.5core.train.large.parquet\n",
      "2025-10-10 21:41:18,142 - INFO - \n",
      "Sampling 50000 users\n",
      "2025-10-10 21:41:18,143 - INFO -   Input:  Beauty_and_Personal_Care.5core.train.parquet\n",
      "2025-10-10 21:41:18,143 - INFO -   Output: Beauty_and_Personal_Care.5core.train.large.parquet\n",
      "2025-10-10 21:41:18,292 - INFO -   Total users: 729,576\n",
      "2025-10-10 21:41:18,523 - INFO -   Before item filter: (1588687, 4) shape, 1,588,687 ratings, 50,000  users, 180,522 items\n",
      "2025-10-10 21:41:18,553 - INFO -   Avg ratings/item: 8.80\n",
      "2025-10-10 21:41:18,553 - INFO -   Min threshold: 792.05 (90x avg)\n",
      "2025-10-10 21:41:18,556 - INFO -   After item filter: (7744, 4) shape, 7,744 ratings, 6,635  users, 8 items\n",
      "2025-10-10 21:41:18,556 - INFO -   Sampled: (7744, 4) shape, 7,744 ratings, 50,000 users\n",
      "2025-10-10 21:41:18,562 - INFO -   Saved: Beauty_and_Personal_Care.5core.train.large.parquet\n",
      "\n",
      "2025-10-10 21:41:18,566 - INFO - \n",
      "cat=Beauty_and_Personal_Care - split=train - sample=big - n=50000 sampling...\n",
      "2025-10-10 21:41:18,567 - INFO - Input: Beauty_and_Personal_Care.5core.train.parquet \n",
      " → Output: Beauty_and_Personal_Care.5core.train.big.parquet\n",
      "2025-10-10 21:41:18,567 - INFO - \n",
      "Sampling 50000 users\n",
      "2025-10-10 21:41:18,568 - INFO -   Input:  Beauty_and_Personal_Care.5core.train.parquet\n",
      "2025-10-10 21:41:18,568 - INFO -   Output: Beauty_and_Personal_Care.5core.train.big.parquet\n",
      "2025-10-10 21:41:18,705 - INFO -   Total users: 729,576\n",
      "2025-10-10 21:41:18,932 - INFO -   Before item filter: (1588687, 4) shape, 1,588,687 ratings, 50,000  users, 180,549 items\n",
      "2025-10-10 21:41:18,963 - INFO -   Avg ratings/item: 8.80\n",
      "2025-10-10 21:41:18,963 - INFO -   Min threshold: 791.93 (90x avg)\n",
      "2025-10-10 21:41:18,966 - INFO -   After item filter: (7718, 4) shape, 7,718 ratings, 6,613  users, 8 items\n",
      "2025-10-10 21:41:18,967 - INFO -   Sampled: (7718, 4) shape, 7,718 ratings, 50,000 users\n",
      "2025-10-10 21:41:18,972 - INFO -   Saved: Beauty_and_Personal_Care.5core.train.big.parquet\n",
      "\n",
      "2025-10-10 21:41:18,975 - INFO - \n",
      "cat=Beauty_and_Personal_Care - split=valid - sample=small - n=2000 sampling...\n",
      "2025-10-10 21:41:18,976 - INFO - Input: Beauty_and_Personal_Care.5core.valid.parquet \n",
      " → Output: Beauty_and_Personal_Care.5core.valid.small.parquet\n",
      "2025-10-10 21:41:18,976 - INFO - \n",
      "Sampling 2000 users\n",
      "2025-10-10 21:41:18,976 - INFO -   Input:  Beauty_and_Personal_Care.5core.valid.parquet\n",
      "2025-10-10 21:41:18,977 - INFO -   Output: Beauty_and_Personal_Care.5core.valid.small.parquet\n",
      "2025-10-10 21:41:19,053 - INFO -   Total users: 729,576\n",
      "2025-10-10 21:41:19,056 - INFO -   Filtered: 0 ratings, 0 users\n",
      "2025-10-10 21:41:19,058 - INFO -   Saved: Beauty_and_Personal_Care.5core.valid.small.parquet\n",
      "\n",
      "2025-10-10 21:41:19,059 - INFO - \n",
      "cat=Beauty_and_Personal_Care - split=valid - sample=medium - n=20000 sampling...\n",
      "2025-10-10 21:41:19,059 - INFO - Input: Beauty_and_Personal_Care.5core.valid.parquet \n",
      " → Output: Beauty_and_Personal_Care.5core.valid.medium.parquet\n",
      "2025-10-10 21:41:19,059 - INFO - \n",
      "Sampling 20000 users\n",
      "2025-10-10 21:41:19,059 - INFO -   Input:  Beauty_and_Personal_Care.5core.valid.parquet\n",
      "2025-10-10 21:41:19,059 - INFO -   Output: Beauty_and_Personal_Care.5core.valid.medium.parquet\n",
      "2025-10-10 21:41:19,113 - INFO -   Total users: 729,576\n",
      "2025-10-10 21:41:19,118 - INFO -   Filtered: 0 ratings, 0 users\n",
      "2025-10-10 21:41:19,119 - INFO -   Saved: Beauty_and_Personal_Care.5core.valid.medium.parquet\n",
      "\n",
      "2025-10-10 21:41:19,120 - INFO - \n",
      "cat=Beauty_and_Personal_Care - split=valid - sample=large - n=50000 sampling...\n",
      "2025-10-10 21:41:19,120 - INFO - Input: Beauty_and_Personal_Care.5core.valid.parquet \n",
      " → Output: Beauty_and_Personal_Care.5core.valid.large.parquet\n",
      "2025-10-10 21:41:19,120 - INFO - \n",
      "Sampling 50000 users\n",
      "2025-10-10 21:41:19,121 - INFO -   Input:  Beauty_and_Personal_Care.5core.valid.parquet\n",
      "2025-10-10 21:41:19,121 - INFO -   Output: Beauty_and_Personal_Care.5core.valid.large.parquet\n",
      "2025-10-10 21:41:19,175 - INFO -   Total users: 729,576\n",
      "2025-10-10 21:41:19,183 - INFO -   Filtered: 66 ratings, 66 users\n",
      "2025-10-10 21:41:19,186 - INFO -   Saved: Beauty_and_Personal_Care.5core.valid.large.parquet\n",
      "\n",
      "2025-10-10 21:41:19,187 - INFO - \n",
      "cat=Beauty_and_Personal_Care - split=valid - sample=big - n=50000 sampling...\n",
      "2025-10-10 21:41:19,187 - INFO - Input: Beauty_and_Personal_Care.5core.valid.parquet \n",
      " → Output: Beauty_and_Personal_Care.5core.valid.big.parquet\n",
      "2025-10-10 21:41:19,188 - INFO - \n",
      "Sampling 50000 users\n",
      "2025-10-10 21:41:19,188 - INFO -   Input:  Beauty_and_Personal_Care.5core.valid.parquet\n",
      "2025-10-10 21:41:19,188 - INFO -   Output: Beauty_and_Personal_Care.5core.valid.big.parquet\n",
      "2025-10-10 21:41:19,243 - INFO -   Total users: 729,576\n",
      "2025-10-10 21:41:19,250 - INFO -   Filtered: 66 ratings, 66 users\n",
      "2025-10-10 21:41:19,253 - INFO -   Saved: Beauty_and_Personal_Care.5core.valid.big.parquet\n",
      "\n",
      "2025-10-10 21:41:19,253 - INFO - \n",
      "cat=Beauty_and_Personal_Care - split=test - sample=small - n=2000 sampling...\n",
      "2025-10-10 21:41:19,253 - INFO - Input: Beauty_and_Personal_Care.5core.test.parquet \n",
      " → Output: Beauty_and_Personal_Care.5core.test.small.parquet\n",
      "2025-10-10 21:41:19,254 - INFO - \n",
      "Sampling 2000 users\n",
      "2025-10-10 21:41:19,254 - INFO -   Input:  Beauty_and_Personal_Care.5core.test.parquet\n",
      "2025-10-10 21:41:19,254 - INFO -   Output: Beauty_and_Personal_Care.5core.test.small.parquet\n",
      "2025-10-10 21:41:19,331 - INFO -   Total users: 729,576\n",
      "2025-10-10 21:41:19,333 - INFO -   Filtered: 0 ratings, 0 users\n",
      "2025-10-10 21:41:19,335 - INFO -   Saved: Beauty_and_Personal_Care.5core.test.small.parquet\n",
      "\n",
      "2025-10-10 21:41:19,336 - INFO - \n",
      "cat=Beauty_and_Personal_Care - split=test - sample=medium - n=20000 sampling...\n",
      "2025-10-10 21:41:19,336 - INFO - Input: Beauty_and_Personal_Care.5core.test.parquet \n",
      " → Output: Beauty_and_Personal_Care.5core.test.medium.parquet\n",
      "2025-10-10 21:41:19,337 - INFO - \n",
      "Sampling 20000 users\n",
      "2025-10-10 21:41:19,337 - INFO -   Input:  Beauty_and_Personal_Care.5core.test.parquet\n",
      "2025-10-10 21:41:19,337 - INFO -   Output: Beauty_and_Personal_Care.5core.test.medium.parquet\n",
      "2025-10-10 21:41:19,393 - INFO -   Total users: 729,576\n",
      "2025-10-10 21:41:19,398 - INFO -   Filtered: 0 ratings, 0 users\n",
      "2025-10-10 21:41:19,400 - INFO -   Saved: Beauty_and_Personal_Care.5core.test.medium.parquet\n",
      "\n",
      "2025-10-10 21:41:19,400 - INFO - \n",
      "cat=Beauty_and_Personal_Care - split=test - sample=large - n=50000 sampling...\n",
      "2025-10-10 21:41:19,400 - INFO - Input: Beauty_and_Personal_Care.5core.test.parquet \n",
      " → Output: Beauty_and_Personal_Care.5core.test.large.parquet\n",
      "2025-10-10 21:41:19,401 - INFO - \n",
      "Sampling 50000 users\n",
      "2025-10-10 21:41:19,401 - INFO -   Input:  Beauty_and_Personal_Care.5core.test.parquet\n",
      "2025-10-10 21:41:19,401 - INFO -   Output: Beauty_and_Personal_Care.5core.test.large.parquet\n",
      "2025-10-10 21:41:19,455 - INFO -   Total users: 729,576\n",
      "2025-10-10 21:41:19,462 - INFO -   Filtered: 65 ratings, 65 users\n",
      "2025-10-10 21:41:19,464 - INFO -   Saved: Beauty_and_Personal_Care.5core.test.large.parquet\n",
      "\n",
      "2025-10-10 21:41:19,465 - INFO - \n",
      "cat=Beauty_and_Personal_Care - split=test - sample=big - n=50000 sampling...\n",
      "2025-10-10 21:41:19,465 - INFO - Input: Beauty_and_Personal_Care.5core.test.parquet \n",
      " → Output: Beauty_and_Personal_Care.5core.test.big.parquet\n",
      "2025-10-10 21:41:19,465 - INFO - \n",
      "Sampling 50000 users\n",
      "2025-10-10 21:41:19,466 - INFO -   Input:  Beauty_and_Personal_Care.5core.test.parquet\n",
      "2025-10-10 21:41:19,466 - INFO -   Output: Beauty_and_Personal_Care.5core.test.big.parquet\n",
      "2025-10-10 21:41:19,522 - INFO -   Total users: 729,576\n",
      "2025-10-10 21:41:19,530 - INFO -   Filtered: 65 ratings, 65 users\n",
      "2025-10-10 21:41:19,532 - INFO -   Saved: Beauty_and_Personal_Care.5core.test.big.parquet\n",
      "\n",
      "2025-10-10 21:41:19,533 - INFO - \n",
      "cat=Sports_and_Outdoors - split=train - sample=small - n=2000 sampling...\n",
      "2025-10-10 21:41:19,533 - INFO - Input: Sports_and_Outdoors.5core.train.parquet \n",
      " → Output: Sports_and_Outdoors.5core.train.small.parquet\n",
      "2025-10-10 21:41:19,534 - INFO - \n",
      "Sampling 2000 users\n",
      "2025-10-10 21:41:19,534 - INFO -   Input:  Sports_and_Outdoors.5core.train.parquet\n",
      "2025-10-10 21:41:19,534 - INFO -   Output: Sports_and_Outdoors.5core.train.small.parquet\n",
      "2025-10-10 21:41:19,637 - INFO -   Total users: 409,772\n",
      "2025-10-10 21:41:19,712 - INFO -   Before item filter: (138761, 4) shape, 138,761 ratings, 2,000  users, 56,390 items\n",
      "2025-10-10 21:41:19,716 - INFO -   Avg ratings/item: 2.46\n",
      "2025-10-10 21:41:19,716 - INFO -   Min threshold: 221.47 (90x avg)\n",
      "2025-10-10 21:41:19,717 - INFO -   After item filter: (0, 4) shape, 0 ratings, 0  users, 0 items\n",
      "2025-10-10 21:41:19,718 - INFO -   Sampled: (0, 4) shape, 0 ratings, 2,000 users\n",
      "2025-10-10 21:41:19,719 - INFO -   Saved: Sports_and_Outdoors.5core.train.small.parquet\n",
      "\n",
      "2025-10-10 21:41:19,721 - INFO - \n",
      "cat=Sports_and_Outdoors - split=train - sample=medium - n=20000 sampling...\n",
      "2025-10-10 21:41:19,721 - INFO - Input: Sports_and_Outdoors.5core.train.parquet \n",
      " → Output: Sports_and_Outdoors.5core.train.medium.parquet\n",
      "2025-10-10 21:41:19,722 - INFO - \n",
      "Sampling 20000 users\n",
      "2025-10-10 21:41:19,722 - INFO -   Input:  Sports_and_Outdoors.5core.train.parquet\n",
      "2025-10-10 21:41:19,722 - INFO -   Output: Sports_and_Outdoors.5core.train.medium.parquet\n",
      "2025-10-10 21:41:19,803 - INFO -   Total users: 409,772\n",
      "2025-10-10 21:41:19,933 - INFO -   Before item filter: (571322, 4) shape, 571,322 ratings, 20,000  users, 118,182 items\n",
      "2025-10-10 21:41:19,952 - INFO -   Avg ratings/item: 4.83\n",
      "2025-10-10 21:41:19,953 - INFO -   Min threshold: 435.08 (90x avg)\n",
      "2025-10-10 21:41:19,955 - INFO -   After item filter: (1724, 4) shape, 1,724 ratings, 1,603  users, 3 items\n",
      "2025-10-10 21:41:19,955 - INFO -   Sampled: (1724, 4) shape, 1,724 ratings, 20,000 users\n",
      "2025-10-10 21:41:19,959 - INFO -   Saved: Sports_and_Outdoors.5core.train.medium.parquet\n",
      "\n",
      "2025-10-10 21:41:19,961 - INFO - \n",
      "cat=Sports_and_Outdoors - split=train - sample=large - n=50000 sampling...\n",
      "2025-10-10 21:41:19,962 - INFO - Input: Sports_and_Outdoors.5core.train.parquet \n",
      " → Output: Sports_and_Outdoors.5core.train.large.parquet\n",
      "2025-10-10 21:41:19,962 - INFO - \n",
      "Sampling 50000 users\n",
      "2025-10-10 21:41:19,962 - INFO -   Input:  Sports_and_Outdoors.5core.train.parquet\n",
      "2025-10-10 21:41:19,962 - INFO -   Output: Sports_and_Outdoors.5core.train.large.parquet\n",
      "2025-10-10 21:41:20,045 - INFO -   Total users: 409,772\n",
      "2025-10-10 21:41:20,181 - INFO -   Before item filter: (959332, 4) shape, 959,332 ratings, 50,000  users, 139,234 items\n",
      "2025-10-10 21:41:20,209 - INFO -   Avg ratings/item: 6.89\n",
      "2025-10-10 21:41:20,209 - INFO -   Min threshold: 620.11 (90x avg)\n",
      "2025-10-10 21:41:20,212 - INFO -   After item filter: (3249, 4) shape, 3,249 ratings, 3,079  users, 3 items\n",
      "2025-10-10 21:41:20,212 - INFO -   Sampled: (3249, 4) shape, 3,249 ratings, 50,000 users\n",
      "2025-10-10 21:41:20,216 - INFO -   Saved: Sports_and_Outdoors.5core.train.large.parquet\n",
      "\n",
      "2025-10-10 21:41:20,219 - INFO - \n",
      "cat=Sports_and_Outdoors - split=train - sample=big - n=50000 sampling...\n",
      "2025-10-10 21:41:20,219 - INFO - Input: Sports_and_Outdoors.5core.train.parquet \n",
      " → Output: Sports_and_Outdoors.5core.train.big.parquet\n",
      "2025-10-10 21:41:20,219 - INFO - \n",
      "Sampling 50000 users\n",
      "2025-10-10 21:41:20,220 - INFO -   Input:  Sports_and_Outdoors.5core.train.parquet\n",
      "2025-10-10 21:41:20,220 - INFO -   Output: Sports_and_Outdoors.5core.train.big.parquet\n",
      "2025-10-10 21:41:20,301 - INFO -   Total users: 409,772\n",
      "2025-10-10 21:41:20,494 - INFO -   Before item filter: (959332, 4) shape, 959,332 ratings, 50,000  users, 139,224 items\n",
      "2025-10-10 21:41:20,514 - INFO -   Avg ratings/item: 6.89\n",
      "2025-10-10 21:41:20,515 - INFO -   Min threshold: 620.15 (90x avg)\n",
      "2025-10-10 21:41:20,517 - INFO -   After item filter: (3250, 4) shape, 3,250 ratings, 3,079  users, 3 items\n",
      "2025-10-10 21:41:20,517 - INFO -   Sampled: (3250, 4) shape, 3,250 ratings, 50,000 users\n",
      "2025-10-10 21:41:20,521 - INFO -   Saved: Sports_and_Outdoors.5core.train.big.parquet\n",
      "\n",
      "2025-10-10 21:41:20,525 - INFO - \n",
      "cat=Sports_and_Outdoors - split=valid - sample=small - n=2000 sampling...\n",
      "2025-10-10 21:41:20,525 - INFO - Input: Sports_and_Outdoors.5core.valid.parquet \n",
      " → Output: Sports_and_Outdoors.5core.valid.small.parquet\n",
      "2025-10-10 21:41:20,526 - INFO - \n",
      "Sampling 2000 users\n",
      "2025-10-10 21:41:20,526 - INFO -   Input:  Sports_and_Outdoors.5core.valid.parquet\n",
      "2025-10-10 21:41:20,526 - INFO -   Output: Sports_and_Outdoors.5core.valid.small.parquet\n",
      "2025-10-10 21:41:20,566 - INFO -   Total users: 409,772\n",
      "2025-10-10 21:41:20,568 - INFO -   Filtered: 0 ratings, 0 users\n",
      "2025-10-10 21:41:20,570 - INFO -   Saved: Sports_and_Outdoors.5core.valid.small.parquet\n",
      "\n",
      "2025-10-10 21:41:20,570 - INFO - \n",
      "cat=Sports_and_Outdoors - split=valid - sample=medium - n=20000 sampling...\n",
      "2025-10-10 21:41:20,570 - INFO - Input: Sports_and_Outdoors.5core.valid.parquet \n",
      " → Output: Sports_and_Outdoors.5core.valid.medium.parquet\n",
      "2025-10-10 21:41:20,571 - INFO - \n",
      "Sampling 20000 users\n",
      "2025-10-10 21:41:20,571 - INFO -   Input:  Sports_and_Outdoors.5core.valid.parquet\n",
      "2025-10-10 21:41:20,571 - INFO -   Output: Sports_and_Outdoors.5core.valid.medium.parquet\n",
      "2025-10-10 21:41:20,599 - INFO -   Total users: 409,772\n",
      "2025-10-10 21:41:20,603 - INFO -   Filtered: 4 ratings, 4 users\n",
      "2025-10-10 21:41:20,606 - INFO -   Saved: Sports_and_Outdoors.5core.valid.medium.parquet\n",
      "\n",
      "2025-10-10 21:41:20,606 - INFO - \n",
      "cat=Sports_and_Outdoors - split=valid - sample=large - n=50000 sampling...\n",
      "2025-10-10 21:41:20,606 - INFO - Input: Sports_and_Outdoors.5core.valid.parquet \n",
      " → Output: Sports_and_Outdoors.5core.valid.large.parquet\n",
      "2025-10-10 21:41:20,606 - INFO - \n",
      "Sampling 50000 users\n",
      "2025-10-10 21:41:20,607 - INFO -   Input:  Sports_and_Outdoors.5core.valid.parquet\n",
      "2025-10-10 21:41:20,607 - INFO -   Output: Sports_and_Outdoors.5core.valid.large.parquet\n",
      "2025-10-10 21:41:20,634 - INFO -   Total users: 409,772\n",
      "2025-10-10 21:41:20,638 - INFO -   Filtered: 14 ratings, 14 users\n",
      "2025-10-10 21:41:20,640 - INFO -   Saved: Sports_and_Outdoors.5core.valid.large.parquet\n",
      "\n",
      "2025-10-10 21:41:20,641 - INFO - \n",
      "cat=Sports_and_Outdoors - split=valid - sample=big - n=50000 sampling...\n",
      "2025-10-10 21:41:20,641 - INFO - Input: Sports_and_Outdoors.5core.valid.parquet \n",
      " → Output: Sports_and_Outdoors.5core.valid.big.parquet\n",
      "2025-10-10 21:41:20,641 - INFO - \n",
      "Sampling 50000 users\n",
      "2025-10-10 21:41:20,642 - INFO -   Input:  Sports_and_Outdoors.5core.valid.parquet\n",
      "2025-10-10 21:41:20,642 - INFO -   Output: Sports_and_Outdoors.5core.valid.big.parquet\n",
      "2025-10-10 21:41:20,669 - INFO -   Total users: 409,772\n",
      "2025-10-10 21:41:20,674 - INFO -   Filtered: 15 ratings, 15 users\n",
      "2025-10-10 21:41:20,676 - INFO -   Saved: Sports_and_Outdoors.5core.valid.big.parquet\n",
      "\n",
      "2025-10-10 21:41:20,676 - INFO - \n",
      "cat=Sports_and_Outdoors - split=test - sample=small - n=2000 sampling...\n",
      "2025-10-10 21:41:20,677 - INFO - Input: Sports_and_Outdoors.5core.test.parquet \n",
      " → Output: Sports_and_Outdoors.5core.test.small.parquet\n",
      "2025-10-10 21:41:20,677 - INFO - \n",
      "Sampling 2000 users\n",
      "2025-10-10 21:41:20,677 - INFO -   Input:  Sports_and_Outdoors.5core.test.parquet\n",
      "2025-10-10 21:41:20,677 - INFO -   Output: Sports_and_Outdoors.5core.test.small.parquet\n",
      "2025-10-10 21:41:20,712 - INFO -   Total users: 409,772\n",
      "2025-10-10 21:41:20,714 - INFO -   Filtered: 0 ratings, 0 users\n",
      "2025-10-10 21:41:20,716 - INFO -   Saved: Sports_and_Outdoors.5core.test.small.parquet\n",
      "\n",
      "2025-10-10 21:41:20,716 - INFO - \n",
      "cat=Sports_and_Outdoors - split=test - sample=medium - n=20000 sampling...\n",
      "2025-10-10 21:41:20,716 - INFO - Input: Sports_and_Outdoors.5core.test.parquet \n",
      " → Output: Sports_and_Outdoors.5core.test.medium.parquet\n",
      "2025-10-10 21:41:20,716 - INFO - \n",
      "Sampling 20000 users\n",
      "2025-10-10 21:41:20,717 - INFO -   Input:  Sports_and_Outdoors.5core.test.parquet\n",
      "2025-10-10 21:41:20,717 - INFO -   Output: Sports_and_Outdoors.5core.test.medium.parquet\n",
      "2025-10-10 21:41:20,744 - INFO -   Total users: 409,772\n",
      "2025-10-10 21:41:20,748 - INFO -   Filtered: 3 ratings, 3 users\n",
      "2025-10-10 21:41:20,750 - INFO -   Saved: Sports_and_Outdoors.5core.test.medium.parquet\n",
      "\n",
      "2025-10-10 21:41:20,750 - INFO - \n",
      "cat=Sports_and_Outdoors - split=test - sample=large - n=50000 sampling...\n",
      "2025-10-10 21:41:20,750 - INFO - Input: Sports_and_Outdoors.5core.test.parquet \n",
      " → Output: Sports_and_Outdoors.5core.test.large.parquet\n",
      "2025-10-10 21:41:20,751 - INFO - \n",
      "Sampling 50000 users\n",
      "2025-10-10 21:41:20,751 - INFO -   Input:  Sports_and_Outdoors.5core.test.parquet\n",
      "2025-10-10 21:41:20,751 - INFO -   Output: Sports_and_Outdoors.5core.test.large.parquet\n",
      "2025-10-10 21:41:20,778 - INFO -   Total users: 409,772\n",
      "2025-10-10 21:41:20,783 - INFO -   Filtered: 11 ratings, 11 users\n",
      "2025-10-10 21:41:20,786 - INFO -   Saved: Sports_and_Outdoors.5core.test.large.parquet\n",
      "\n",
      "2025-10-10 21:41:20,786 - INFO - \n",
      "cat=Sports_and_Outdoors - split=test - sample=big - n=50000 sampling...\n",
      "2025-10-10 21:41:20,787 - INFO - Input: Sports_and_Outdoors.5core.test.parquet \n",
      " → Output: Sports_and_Outdoors.5core.test.big.parquet\n",
      "2025-10-10 21:41:20,787 - INFO - \n",
      "Sampling 50000 users\n",
      "2025-10-10 21:41:20,787 - INFO -   Input:  Sports_and_Outdoors.5core.test.parquet\n",
      "2025-10-10 21:41:20,787 - INFO -   Output: Sports_and_Outdoors.5core.test.big.parquet\n",
      "2025-10-10 21:41:20,814 - INFO -   Total users: 409,772\n",
      "2025-10-10 21:41:20,818 - INFO -   Filtered: 11 ratings, 11 users\n",
      "2025-10-10 21:41:20,820 - INFO -   Saved: Sports_and_Outdoors.5core.test.big.parquet\n",
      "\n",
      "2025-10-10 21:41:20,820 - INFO - \n",
      " COMPLETED\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_dataset(category: str, suffix: str = 'small'):\n",
    "    safe_cat = category.replace('/', '-')\n",
    "    \n",
    "    logger.log_info(\"=\"*70)\n",
    "    logger.log_info(f\"DIAGNOSTIC: {category} (suffix={suffix})\")\n",
    "    logger.log_info(\"=\"*70)\n",
    "    \n",
    "    # Load files\n",
    "    train = PROCESSED_DIR / f\"{safe_cat}.5core.train.{suffix}.parquet\"\n",
    "    valid = PROCESSED_DIR / f\"{safe_cat}.5core.valid.{suffix}.parquet\"\n",
    "    test = PROCESSED_DIR / f\"{safe_cat}.5core.test.{suffix}.parquet\"\n",
    "    \n",
    "    if not train.exists():\n",
    "        logger.log_error(f\"File not found: {train.name}\")\n",
    "        return\n",
    "    \n",
    "    df_train = pl.read_parquet(train)\n",
    "    df_valid = pl.read_parquet(valid) if valid.exists() else None\n",
    "    df_test = pl.read_parquet(test) if test.exists() else None\n",
    "    \n",
    "    # Stats\n",
    "    def stats(df, name):\n",
    "        u, i, r = df['user_id'].n_unique(), df['parent_asin'].n_unique(), len(df)\n",
    "        s = 1 - (r / (u * i))\n",
    "        logger.log_info(f\"{name}: {r:,} ratings, {u:,} users, {i:,} items, sparsity {s:.2%}\")\n",
    "        return u, i, r\n",
    "    \n",
    "    train_u, train_i, train_r = stats(df_train, \"TRAIN\")\n",
    "    \n",
    "    if df_valid is not None:\n",
    "        valid_u, valid_i, valid_r = stats(df_valid, \"VALID\")\n",
    "        \n",
    "        # Check overlap\n",
    "        train_users = set(df_train['user_id'].unique())\n",
    "        valid_users = set(df_valid['user_id'].unique())\n",
    "        train_items = set(df_train['parent_asin'].unique())\n",
    "        valid_items = set(df_valid['parent_asin'].unique())\n",
    "        \n",
    "        user_overlap = len(train_users & valid_users)\n",
    "        item_overlap = len(train_items & valid_items)\n",
    "        \n",
    "        logger.log_info(f\"\\nOVERLAP:\")\n",
    "        logger.log_info(f\"  Users: {user_overlap:,} / {valid_u:,} ({user_overlap/valid_u*100:.1f}%)\")\n",
    "        logger.log_info(f\"  Items: {item_overlap:,} / {valid_i:,} ({item_overlap/valid_i*100:.1f}%)\")\n",
    "        \n",
    "        if user_overlap < valid_u:\n",
    "            logger.log_warning(f\"  {valid_u - user_overlap:,} valid users NOT in train!\")\n",
    "        if item_overlap < valid_i:\n",
    "            logger.log_warning(f\"  {valid_i - item_overlap:,} valid items NOT in train!\")\n",
    "    \n",
    "    if df_test is not None:\n",
    "        test_u, test_i, test_r = stats(df_test, \"TEST\")\n",
    "    \n",
    "    logger.log_info(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 21:41:20,843 - INFO - ======================================================================\n",
      "2025-10-10 21:41:20,843 - INFO - DIAGNOSTIC: Electronics (suffix=big)\n",
      "2025-10-10 21:41:20,844 - INFO - ======================================================================\n",
      "2025-10-10 21:41:20,911 - INFO - TRAIN: 85,890 ratings, 35,494 users, 82 items, sparsity 97.05%\n",
      "2025-10-10 21:41:20,913 - INFO - VALID: 858 ratings, 858 users, 81 items, sparsity 98.77%\n",
      "2025-10-10 21:41:20,926 - INFO - \n",
      "OVERLAP:\n",
      "2025-10-10 21:41:20,926 - INFO -   Users: 858 / 858 (100.0%)\n",
      "2025-10-10 21:41:20,926 - INFO -   Items: 81 / 81 (100.0%)\n",
      "2025-10-10 21:41:20,927 - INFO - TEST: 675 ratings, 675 users, 76 items, sparsity 98.68%\n",
      "2025-10-10 21:41:20,927 - INFO - ======================================================================\n",
      "\n",
      "2025-10-10 21:41:20,928 - INFO - ======================================================================\n",
      "2025-10-10 21:41:20,928 - INFO - DIAGNOSTIC: Beauty_and_Personal_Care (suffix=big)\n",
      "2025-10-10 21:41:20,928 - INFO - ======================================================================\n",
      "2025-10-10 21:41:20,931 - INFO - TRAIN: 7,718 ratings, 6,613 users, 8 items, sparsity 85.41%\n",
      "2025-10-10 21:41:20,931 - INFO - VALID: 66 ratings, 66 users, 8 items, sparsity 87.50%\n",
      "2025-10-10 21:41:20,933 - INFO - \n",
      "OVERLAP:\n",
      "2025-10-10 21:41:20,933 - INFO -   Users: 66 / 66 (100.0%)\n",
      "2025-10-10 21:41:20,933 - INFO -   Items: 8 / 8 (100.0%)\n",
      "2025-10-10 21:41:20,933 - INFO - TEST: 65 ratings, 65 users, 8 items, sparsity 87.50%\n",
      "2025-10-10 21:41:20,933 - INFO - ======================================================================\n",
      "\n",
      "2025-10-10 21:41:20,934 - INFO - ======================================================================\n",
      "2025-10-10 21:41:20,934 - INFO - DIAGNOSTIC: Sports_and_Outdoors (suffix=big)\n",
      "2025-10-10 21:41:20,934 - INFO - ======================================================================\n",
      "2025-10-10 21:41:20,936 - INFO - TRAIN: 3,250 ratings, 3,079 users, 3 items, sparsity 64.82%\n",
      "2025-10-10 21:41:20,936 - INFO - VALID: 15 ratings, 15 users, 3 items, sparsity 66.67%\n",
      "2025-10-10 21:41:20,937 - INFO - \n",
      "OVERLAP:\n",
      "2025-10-10 21:41:20,937 - INFO -   Users: 15 / 15 (100.0%)\n",
      "2025-10-10 21:41:20,938 - INFO -   Items: 3 / 3 (100.0%)\n",
      "2025-10-10 21:41:20,938 - INFO - TEST: 11 ratings, 11 users, 3 items, sparsity 66.67%\n",
      "2025-10-10 21:41:20,938 - INFO - ======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cat in Configurations.CATEGORIES:\n",
    "    diagnose_dataset(cat, \"big\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Collection & Preprocessing\n",
        "\n",
        "## Overview\n",
        "This notebook downloads and preprocesses Amazon 2023 product review data from McAuley Lab. It handles downloading raw CSV files, converting to Parquet format, creating stratified samples, and processing metadata. The pipeline ensures data consistency across train/validation/test splits while maintaining the 5-core property (each user and item has at least 5 interactions).\n",
        "\n",
        "## Notebook Structure\n",
        "1. **Setup**: Import libraries, configure paths, and define constants\n",
        "2. **Download Functions**: Fetch raw data from URLs with retry logic\n",
        "3. **Conversion**: Transform CSV.GZ to Parquet for efficient storage\n",
        "4. **Sampling**: Create stratified samples (small/medium/large/big) maintaining user-item distributions\n",
        "5. **Metadata Processing**: Download and extract essential product information (title, features, description, etc.)\n",
        "6. **Pipeline**: Automated execution for all categories\n",
        "\n",
        "## Process Flow\n",
        "**Data Collection:**\n",
        "- For each category: Download train/valid/test CSV.GZ from McAuley Lab → Convert to Parquet\n",
        "- Download metadata JSONL.GZ → Extract essential fields → Save as Parquet\n",
        "\n",
        "**Sampling:**\n",
        "- Create samples from full dataset: small (2k users), medium (20k), large (50k), big (50k with item filtering)\n",
        "- For train: Sample top N active users → Filter items by popularity threshold\n",
        "- For valid/test: Filter to match train users and items for consistency\n",
        "\n",
        "**Output:**\n",
        "- Full datasets: `category.5core.{split}.parquet`\n",
        "- Samples: `category.5core.{split}.{size}.parquet`\n",
        "- Metadata: `category.meta.parquet`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os, json, gzip, urllib.request\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "\n",
        "module_path = str((Path(\"..\") / \"utilities\").resolve())\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "from logger import Logger\n",
        "from configurations import Configurations\n",
        "\n",
        "logger = Logger(process_name=\"data_collection\", log_file=Configurations.LOG_PATH)\n",
        "\n",
        "RAW_DIR = Path(Configurations.DATA_RAW_PATH)\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PROCESSED_DIR = Path(Configurations.DATA_PROCESSED_PATH)\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CATEGORIES = Configurations.CATEGORIES\n",
        "CORES = Configurations.CORES\n",
        "SPLITS = Configurations.SPLITS\n",
        "BASE_URL = Configurations.BASE_URL\n",
        "meta_base_url = \"https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_{category}.jsonl.gz\"\n",
        "meta_urls = {cat: meta_base_url.format(category=cat) for cat in CATEGORIES}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_url(core: str, category: str, split: str) -> str:\n",
        "    return f\"{BASE_URL}/{core}/last_out_w_his/{category}.{split}.csv.gz\"\n",
        "\n",
        "def local_path_for_parquet(core: str, category: str, split: str, sample: str = None, raw_dir=RAW_DIR) -> Path:\n",
        "    safe_cat = category.replace(\"/\", \"-\")\n",
        "    if raw_dir == RAW_DIR:\n",
        "        return RAW_DIR / f\"{safe_cat}.{core}.{split}.csv.gz\"\n",
        "    elif raw_dir == PROCESSED_DIR:\n",
        "        if sample is None:\n",
        "            return PROCESSED_DIR / f\"{safe_cat}.{core}.{split}.parquet\"\n",
        "        else:\n",
        "            return PROCESSED_DIR / f\"{safe_cat}.{core}.{split}.{sample}.parquet\"\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid directory: {raw_dir}\")\n",
        "\n",
        "def download_file(url: str, out_path: Path, max_retries: int = 3) -> None:\n",
        "    if out_path.exists() and out_path.stat().st_size > 0:\n",
        "        logger.log_info(f\"Exists, skip: {out_path.name}\")\n",
        "        return\n",
        "    attempt = 0\n",
        "    while attempt < max_retries:\n",
        "        try:\n",
        "            attempt += 1\n",
        "            logger.log_info(f\"Downloading (attempt {attempt}/{max_retries}): {url}\")\n",
        "            tmp = str(out_path) + \".part\"\n",
        "            urllib.request.urlretrieve(url, tmp)\n",
        "            os.replace(tmp, out_path)\n",
        "            logger.log_info(f\"Saved: {out_path.name}\")\n",
        "            return\n",
        "        except Exception as e:\n",
        "            logger.log_warning(f\"Failed attempt {attempt} for {url}: {e}\")\n",
        "    raise RuntimeError(f\"Exceeded retries: {url}\")\n",
        "\n",
        "def save_dataset_to_parquet(csv_gz_path: Path, out_parquet_path: Path):\n",
        "    if out_parquet_path.exists():\n",
        "        logger.log_info(f\"Skip: {out_parquet_path.name}\")\n",
        "        return\n",
        "    logger.log_info(f\"Reading: {csv_gz_path.name}\")\n",
        "    df = pl.from_pandas(pd.read_csv(csv_gz_path, compression='gzip')[Configurations.COLUMNS])\n",
        "    logger.log_info(f\"  Shape: {df.shape}\")\n",
        "    logger.log_info(f\"  Users: {df['user_id'].n_unique():,}\")\n",
        "    logger.log_info(f\"  Items: {df['parent_asin'].n_unique():,}\")\n",
        "    df.to_pandas().to_parquet(out_parquet_path, engine='pyarrow', index=False)\n",
        "    logger.log_info(f\"Saved: {out_parquet_path.name}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_n_sample(input_path: Path, n: int, item_mult: float = 1.0, n_name_out: str = None):\n",
        "    stem = input_path.stem\n",
        "    parts = stem.split('.')\n",
        "    category, core, split = parts[0], parts[1] if len(parts) > 1 else '5core', parts[2] if len(parts) > 2 else 'train'\n",
        "    output_path = input_path.parent / f\"{category}.{core}.{split}.{n_name_out}.parquet\"\n",
        "    \n",
        "    if output_path.exists():\n",
        "        logger.log_info(f\"Skip: {output_path.name}\")\n",
        "        return\n",
        "    \n",
        "    if category == \"Electronics\":\n",
        "        item_mult = item_mult * 1.82\n",
        "        logger.log_info(f\"Category Electronics detected: item_mult adjusted to {item_mult}\")\n",
        "\n",
        "    if category == \"Sports_and_Outdoors\":\n",
        "        item_mult = item_mult * 0.7\n",
        "        logger.log_info(f\"Category Sports_and_Outdoors detected: item_mult adjusted to {item_mult}\")\n",
        "\n",
        "    logger.log_info(f\"\\nSampling {n} users\")\n",
        "    logger.log_info(f\"  Input:  {input_path.name}\")\n",
        "    logger.log_info(f\"  Output: {output_path.name}\")\n",
        "    \n",
        "    df = pl.read_parquet(input_path)\n",
        "    total = df['user_id'].n_unique()\n",
        "    logger.log_info(f\"  Total users: {total:,}\")\n",
        "    \n",
        "    if split == 'train':\n",
        "        if n >= total:\n",
        "            df_sampled = df\n",
        "            users_n = df['user_id'].unique().to_list()\n",
        "        else:\n",
        "            activity = df.group_by('user_id').agg(pl.len().alias('n')).sort('n', descending=True)\n",
        "            users_n = activity.head(n)['user_id'].to_list()\n",
        "            df_sampled = df.filter(pl.col('user_id').is_in(users_n))\n",
        "        \n",
        "        if item_mult > 1.0:\n",
        "            logger.log_info(f\"  Before item filter: {df_sampled.shape} shape, {len(df_sampled):,} ratings, {df_sampled['user_id'].n_unique():,}  users, {df_sampled['parent_asin'].n_unique():,} items\")\n",
        "            item_counts = df_sampled.group_by('parent_asin').agg(pl.len().alias('n'))\n",
        "            avg_item = item_counts['n'].mean()\n",
        "            min_item = avg_item * item_mult\n",
        "            logger.log_info(f\"  Avg ratings/item: {avg_item:.2f}\")\n",
        "            logger.log_info(f\"  Min threshold: {min_item:.2f} ({item_mult}x avg)\")\n",
        "            popular = item_counts.filter(pl.col('n') > min_item)['parent_asin'].to_list()\n",
        "            df_sampled = df_sampled.filter(pl.col('parent_asin').is_in(popular))\n",
        "            logger.log_info(f\"  After item filter: {df_sampled.shape} shape, {len(df_sampled):,} ratings, {df_sampled['user_id'].n_unique():,}  users, {df_sampled['parent_asin'].n_unique():,} items\")\n",
        "        \n",
        "        logger.log_info(f\"  Sampled: {df_sampled.shape} shape, {len(df_sampled):,} ratings, {len(users_n):,} users\")\n",
        "    else:\n",
        "        train_path = input_path.parent / f\"{category}.{core}.train.{n_name_out}.parquet\"\n",
        "        if not train_path.exists():\n",
        "            logger.log_warning(f\"  Train sample not found: {train_path.name}\")\n",
        "            return\n",
        "        df_train = pl.read_parquet(train_path)\n",
        "        users_n = df_train['user_id'].unique().to_list()\n",
        "        items_n = df_train['parent_asin'].unique().to_list()\n",
        "        df_sampled = df.filter(pl.col('user_id').is_in(users_n) & pl.col('parent_asin').is_in(items_n))\n",
        "        logger.log_info(f\"  Filtered: {len(df_sampled):,} ratings, {df_sampled['user_id'].n_unique():,} users\")\n",
        "    \n",
        "    df_sampled.to_pandas().to_parquet(output_path, engine='pyarrow', index=False)\n",
        "    logger.log_info(f\"  Saved: {output_path.name}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_meta(category: str, url: str = None):\n",
        "    url = url or meta_urls.get(category)\n",
        "    if not url:\n",
        "        return\n",
        "    dst = RAW_DIR / f\"{category}.meta.jsonl.gz\"\n",
        "    if dst.exists() and dst.stat().st_size > 0:\n",
        "        logger.log_info(f\"[META] Skip: {dst.name}\")\n",
        "        return\n",
        "    logger.log_info(f\"[META] Downloading: {category}\")\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, str(dst))\n",
        "        logger.log_info(f\"[META] {dst.name}\")\n",
        "    except Exception as e:\n",
        "        logger.log_exception(f\"[META] Failed: {e}\")\n",
        "\n",
        "def save_meta_for_training_ui(category: str):\n",
        "    safe_cat = category.replace('/', '-')\n",
        "    out_path = PROCESSED_DIR / f\"{safe_cat}.meta.parquet\"\n",
        "    \n",
        "    if out_path.exists():\n",
        "        logger.log_info(f\"[META] Skip: {out_path.name}\")\n",
        "        return out_path\n",
        "    \n",
        "    fp = RAW_DIR / f\"{category}.meta.jsonl.gz\"\n",
        "    if not fp.exists():\n",
        "        logger.log_warning(f\"[META] Not found: {fp}\")\n",
        "        return None\n",
        "    \n",
        "    logger.log_info(f\"[META] Reading: {fp.name}\")\n",
        "    \n",
        "    def extract_item(obj):\n",
        "        def process_list(field, max_items):\n",
        "            if not isinstance(field, list):\n",
        "                return []\n",
        "            if field and isinstance(field[0], list):\n",
        "                field = field[0]\n",
        "            return field[:max_items]\n",
        "        \n",
        "        def process_desc(desc, max_len=2000):\n",
        "            if isinstance(desc, list):\n",
        "                desc = \" \".join(str(d) for d in desc if d)\n",
        "            elif not desc:\n",
        "                return \"\"\n",
        "            else:\n",
        "                desc = str(desc)\n",
        "            return desc[:max_len] + (\"...\" if len(desc) > max_len else \"\")\n",
        "        \n",
        "        def process_images(img_list):\n",
        "            if not img_list or not isinstance(img_list, list):\n",
        "                return []\n",
        "            return [{\"hi_res\": img.get(\"hi_res\"), \"thumb\": img.get(\"thumb\")} \n",
        "                    for img in img_list[:3] if isinstance(img, dict)]\n",
        "        \n",
        "        return {\n",
        "            \"parent_asin\": obj.get(\"parent_asin\"),\n",
        "            \"title\": obj.get(\"title\", \"\"),\n",
        "            \"price\": obj.get(\"price\"),\n",
        "            \"average_rating\": obj.get(\"average_rating\"),\n",
        "            \"rating_number\": obj.get(\"rating_number\"),\n",
        "            \"features\": process_list(obj.get(\"features\"), 10),\n",
        "            \"description\": process_desc(obj.get(\"description\")),\n",
        "            \"categories\": process_list(obj.get(\"categories\"), 5),\n",
        "            \"images\": process_images(obj.get(\"images\")),\n",
        "            \"store\": obj.get(\"store\", \"\")\n",
        "        }\n",
        "    \n",
        "    rows = []\n",
        "    with gzip.open(fp, \"rt\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                obj = json.loads(line)\n",
        "                if obj.get(\"parent_asin\"):\n",
        "                    rows.append(extract_item(obj))\n",
        "            except:\n",
        "                continue\n",
        "    \n",
        "    if not rows:\n",
        "        logger.log_warning(f\"[META] No data for {category}\")\n",
        "        return None\n",
        "    \n",
        "    df = pd.DataFrame(rows)\n",
        "    for col in ['price', 'average_rating', 'rating_number']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    for col in ['title', 'description', 'store']:\n",
        "        df[col] = df[col].fillna('')\n",
        "    \n",
        "    df.to_parquet(out_path, index=False)\n",
        "    logger.log_info(f\"[META] {out_path.name}: {len(df):,} items\")\n",
        "    return out_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run():\n",
        "    logger.log_info(\"=\"*70)\n",
        "    logger.log_info(\"DATASET COLLECTION\")\n",
        "    logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    for core in CORES:\n",
        "        for cat in CATEGORIES:\n",
        "            logger.log_info(f\"\\n{'='*70}\")\n",
        "            logger.log_info(f\"{cat}\")\n",
        "            logger.log_info(f\"{'='*70}\")\n",
        "            \n",
        "            for split in SPLITS:\n",
        "                logger.log_info(f\"\\nProcessing {split.upper()}...\")\n",
        "                url = build_url(core, cat, split)\n",
        "                in_path = local_path_for_parquet(core, cat, split, raw_dir=RAW_DIR)\n",
        "                out_path = local_path_for_parquet(core, cat, split, raw_dir=PROCESSED_DIR)\n",
        "                try:\n",
        "                    download_file(url, in_path)\n",
        "                    save_dataset_to_parquet(in_path, out_path)\n",
        "                except Exception as e:\n",
        "                    logger.log_exception(f\"{split} error: {e}\")\n",
        "\n",
        "    logger.log_info(\"\\n\" + \"=\"*70)\n",
        "    logger.log_info(\"METADATA\")\n",
        "    logger.log_info(\"=\"*70)\n",
        "    \n",
        "    for cat in CATEGORIES:\n",
        "        download_meta(cat)\n",
        "        save_meta_for_training_ui(cat)\n",
        "\n",
        "    logger.log_info(\"\\n\" + \"=\"*70)\n",
        "    logger.log_info(\"CREATING SAMPLES\")\n",
        "    logger.log_info(\"=\"*70)\n",
        "    \n",
        "    for core in CORES:\n",
        "        for cat in CATEGORIES:\n",
        "            for split in SPLITS:\n",
        "                for sample in Configurations.SAMPLE_SIZES:\n",
        "                    if sample != \"full\":\n",
        "                        n = Configurations.SAMPLE_SIZES[sample]\n",
        "                        logger.log_info(f\"\\ncat={cat} - split={split} - sample={sample} - n={n} sampling...\")\n",
        "                        in_path = local_path_for_parquet(core, cat, split, raw_dir=PROCESSED_DIR)\n",
        "                        out_path = local_path_for_parquet(core, cat, split, sample, raw_dir=PROCESSED_DIR)\n",
        "                        logger.log_info(f\"Input: {in_path.name} \\n → Output: {out_path.name}\")\n",
        "                        create_n_sample(in_path, n, Configurations.ITEM_MULTI, sample)\n",
        "\n",
        "    logger.log_info(\"\\n COMPLETED\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-03 15:04:00,747 - INFO - ======================================================================\n",
            "2025-11-03 15:04:00,748 - INFO - DATASET COLLECTION\n",
            "2025-11-03 15:04:00,749 - INFO - ======================================================================\n",
            "\n",
            "2025-11-03 15:04:00,750 - INFO - \n",
            "======================================================================\n",
            "2025-11-03 15:04:00,750 - INFO - Electronics\n",
            "2025-11-03 15:04:00,751 - INFO - ======================================================================\n",
            "2025-11-03 15:04:00,751 - INFO - \n",
            "Processing TRAIN...\n",
            "2025-11-03 15:04:00,752 - INFO - Exists, skip: Electronics.5core.train.csv.gz\n",
            "2025-11-03 15:04:00,757 - INFO - Skip: Electronics.5core.train.parquet\n",
            "2025-11-03 15:04:00,759 - INFO - \n",
            "Processing VALID...\n",
            "2025-11-03 15:04:00,792 - INFO - Exists, skip: Electronics.5core.valid.csv.gz\n",
            "2025-11-03 15:04:00,793 - INFO - Skip: Electronics.5core.valid.parquet\n",
            "2025-11-03 15:04:00,793 - INFO - \n",
            "Processing TEST...\n",
            "2025-11-03 15:04:00,794 - INFO - Exists, skip: Electronics.5core.test.csv.gz\n",
            "2025-11-03 15:04:00,794 - INFO - Skip: Electronics.5core.test.parquet\n",
            "2025-11-03 15:04:00,794 - INFO - \n",
            "======================================================================\n",
            "2025-11-03 15:04:00,794 - INFO - Beauty_and_Personal_Care\n",
            "2025-11-03 15:04:00,795 - INFO - ======================================================================\n",
            "2025-11-03 15:04:00,795 - INFO - \n",
            "Processing TRAIN...\n",
            "2025-11-03 15:04:00,795 - INFO - Exists, skip: Beauty_and_Personal_Care.5core.train.csv.gz\n",
            "2025-11-03 15:04:00,796 - INFO - Skip: Beauty_and_Personal_Care.5core.train.parquet\n",
            "2025-11-03 15:04:00,796 - INFO - \n",
            "Processing VALID...\n",
            "2025-11-03 15:04:00,798 - INFO - Exists, skip: Beauty_and_Personal_Care.5core.valid.csv.gz\n",
            "2025-11-03 15:04:00,801 - INFO - Skip: Beauty_and_Personal_Care.5core.valid.parquet\n",
            "2025-11-03 15:04:00,802 - INFO - \n",
            "Processing TEST...\n",
            "2025-11-03 15:04:00,804 - INFO - Exists, skip: Beauty_and_Personal_Care.5core.test.csv.gz\n",
            "2025-11-03 15:04:00,806 - INFO - Skip: Beauty_and_Personal_Care.5core.test.parquet\n",
            "2025-11-03 15:04:00,806 - INFO - \n",
            "======================================================================\n",
            "2025-11-03 15:04:00,806 - INFO - Sports_and_Outdoors\n",
            "2025-11-03 15:04:00,807 - INFO - ======================================================================\n",
            "2025-11-03 15:04:00,807 - INFO - \n",
            "Processing TRAIN...\n",
            "2025-11-03 15:04:00,807 - INFO - Exists, skip: Sports_and_Outdoors.5core.train.csv.gz\n",
            "2025-11-03 15:04:00,807 - INFO - Skip: Sports_and_Outdoors.5core.train.parquet\n",
            "2025-11-03 15:04:00,808 - INFO - \n",
            "Processing VALID...\n",
            "2025-11-03 15:04:00,808 - INFO - Exists, skip: Sports_and_Outdoors.5core.valid.csv.gz\n",
            "2025-11-03 15:04:00,808 - INFO - Skip: Sports_and_Outdoors.5core.valid.parquet\n",
            "2025-11-03 15:04:00,809 - INFO - \n",
            "Processing TEST...\n",
            "2025-11-03 15:04:00,809 - INFO - Exists, skip: Sports_and_Outdoors.5core.test.csv.gz\n",
            "2025-11-03 15:04:00,809 - INFO - Skip: Sports_and_Outdoors.5core.test.parquet\n",
            "2025-11-03 15:04:00,809 - INFO - \n",
            "======================================================================\n",
            "2025-11-03 15:04:00,810 - INFO - METADATA\n",
            "2025-11-03 15:04:00,810 - INFO - ======================================================================\n",
            "2025-11-03 15:04:00,810 - INFO - [META] Skip: Electronics.meta.jsonl.gz\n",
            "2025-11-03 15:04:00,810 - INFO - [META] Skip: Electronics.meta.parquet\n",
            "2025-11-03 15:04:00,811 - INFO - [META] Skip: Beauty_and_Personal_Care.meta.jsonl.gz\n",
            "2025-11-03 15:04:00,811 - INFO - [META] Skip: Beauty_and_Personal_Care.meta.parquet\n",
            "2025-11-03 15:04:00,811 - INFO - [META] Skip: Sports_and_Outdoors.meta.jsonl.gz\n",
            "2025-11-03 15:04:00,812 - INFO - [META] Skip: Sports_and_Outdoors.meta.parquet\n",
            "2025-11-03 15:04:00,812 - INFO - \n",
            "======================================================================\n",
            "2025-11-03 15:04:00,812 - INFO - CREATING SAMPLES\n",
            "2025-11-03 15:04:00,812 - INFO - ======================================================================\n",
            "2025-11-03 15:04:00,813 - INFO - \n",
            "cat=Electronics - split=train - sample=big - n=68000 sampling...\n",
            "2025-11-03 15:04:00,813 - INFO - Input: Electronics.5core.train.parquet \n",
            " → Output: Electronics.5core.train.big.parquet\n",
            "2025-11-03 15:04:00,813 - INFO - Skip: Electronics.5core.train.big.parquet\n",
            "2025-11-03 15:04:00,813 - INFO - \n",
            "cat=Electronics - split=valid - sample=big - n=68000 sampling...\n",
            "2025-11-03 15:04:00,813 - INFO - Input: Electronics.5core.valid.parquet \n",
            " → Output: Electronics.5core.valid.big.parquet\n",
            "2025-11-03 15:04:00,814 - INFO - Skip: Electronics.5core.valid.big.parquet\n",
            "2025-11-03 15:04:00,814 - INFO - \n",
            "cat=Electronics - split=test - sample=big - n=68000 sampling...\n",
            "2025-11-03 15:04:00,814 - INFO - Input: Electronics.5core.test.parquet \n",
            " → Output: Electronics.5core.test.big.parquet\n",
            "2025-11-03 15:04:00,814 - INFO - Skip: Electronics.5core.test.big.parquet\n",
            "2025-11-03 15:04:00,815 - INFO - \n",
            "cat=Beauty_and_Personal_Care - split=train - sample=big - n=68000 sampling...\n",
            "2025-11-03 15:04:00,815 - INFO - Input: Beauty_and_Personal_Care.5core.train.parquet \n",
            " → Output: Beauty_and_Personal_Care.5core.train.big.parquet\n",
            "2025-11-03 15:04:00,815 - INFO - Skip: Beauty_and_Personal_Care.5core.train.big.parquet\n",
            "2025-11-03 15:04:00,816 - INFO - \n",
            "cat=Beauty_and_Personal_Care - split=valid - sample=big - n=68000 sampling...\n",
            "2025-11-03 15:04:00,816 - INFO - Input: Beauty_and_Personal_Care.5core.valid.parquet \n",
            " → Output: Beauty_and_Personal_Care.5core.valid.big.parquet\n",
            "2025-11-03 15:04:00,816 - INFO - Skip: Beauty_and_Personal_Care.5core.valid.big.parquet\n",
            "2025-11-03 15:04:00,816 - INFO - \n",
            "cat=Beauty_and_Personal_Care - split=test - sample=big - n=68000 sampling...\n",
            "2025-11-03 15:04:00,816 - INFO - Input: Beauty_and_Personal_Care.5core.test.parquet \n",
            " → Output: Beauty_and_Personal_Care.5core.test.big.parquet\n",
            "2025-11-03 15:04:00,817 - INFO - Skip: Beauty_and_Personal_Care.5core.test.big.parquet\n",
            "2025-11-03 15:04:00,817 - INFO - \n",
            "cat=Sports_and_Outdoors - split=train - sample=big - n=68000 sampling...\n",
            "2025-11-03 15:04:00,817 - INFO - Input: Sports_and_Outdoors.5core.train.parquet \n",
            " → Output: Sports_and_Outdoors.5core.train.big.parquet\n",
            "2025-11-03 15:04:00,817 - INFO - Skip: Sports_and_Outdoors.5core.train.big.parquet\n",
            "2025-11-03 15:04:00,818 - INFO - \n",
            "cat=Sports_and_Outdoors - split=valid - sample=big - n=68000 sampling...\n",
            "2025-11-03 15:04:00,818 - INFO - Input: Sports_and_Outdoors.5core.valid.parquet \n",
            " → Output: Sports_and_Outdoors.5core.valid.big.parquet\n",
            "2025-11-03 15:04:00,818 - INFO - Skip: Sports_and_Outdoors.5core.valid.big.parquet\n",
            "2025-11-03 15:04:00,819 - INFO - \n",
            "cat=Sports_and_Outdoors - split=test - sample=big - n=68000 sampling...\n",
            "2025-11-03 15:04:00,819 - INFO - Input: Sports_and_Outdoors.5core.test.parquet \n",
            " → Output: Sports_and_Outdoors.5core.test.big.parquet\n",
            "2025-11-03 15:04:00,819 - INFO - Skip: Sports_and_Outdoors.5core.test.big.parquet\n",
            "2025-11-03 15:04:00,819 - INFO - \n",
            " COMPLETED\n"
          ]
        }
      ],
      "source": [
        "run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def diagnose_dataset(category: str, suffix: str = 'small'):\n",
        "    safe_cat = category.replace('/', '-')\n",
        "    logger.log_info(\"=\"*70)\n",
        "    logger.log_info(f\"DIAGNOSTIC: {category} (suffix={suffix})\")\n",
        "    logger.log_info(\"=\"*70)\n",
        "    \n",
        "    train = PROCESSED_DIR / f\"{safe_cat}.5core.train.{suffix}.parquet\"\n",
        "    valid = PROCESSED_DIR / f\"{safe_cat}.5core.valid.{suffix}.parquet\"\n",
        "    test = PROCESSED_DIR / f\"{safe_cat}.5core.test.{suffix}.parquet\"\n",
        "    \n",
        "    if not train.exists():\n",
        "        logger.log_error(f\"File not found: {train.name}\")\n",
        "        return\n",
        "    \n",
        "    df_train = pl.read_parquet(train)\n",
        "    df_valid = pl.read_parquet(valid) if valid.exists() else None\n",
        "    df_test = pl.read_parquet(test) if test.exists() else None\n",
        "    \n",
        "    def stats(df, name):\n",
        "        u, i, r = df['user_id'].n_unique(), df['parent_asin'].n_unique(), len(df)\n",
        "        s = 1 - (r / (u * i))\n",
        "        logger.log_info(f\"{name}: {r:,} ratings, {u:,} users, sparsity {s:.2%}\")\n",
        "        return u, i, r\n",
        "    \n",
        "    train_u, train_i, train_r = stats(df_train, \"TRAIN\")\n",
        "    \n",
        "    if df_valid is not None:\n",
        "        valid_u, valid_i, valid_r = stats(df_valid, \"VALID\")\n",
        "        train_users = set(df_train['user_id'].unique())\n",
        "        valid_users = set(df_valid['user_id'].unique())\n",
        "        train_items = set(df_train['parent_asin'].unique())\n",
        "        valid_items = set(df_valid['parent_asin'].unique())\n",
        "        user_overlap = len(train_users & valid_users)\n",
        "        item_overlap = len(train_items & valid_items)\n",
        "        logger.log_info(f\"\\nOVERLAP:\")\n",
        "        logger.log_info(f\"  Users: {user_overlap:,} / {valid_u:,} ({user_overlap/valid_u*100:.1f}%)\")\n",
        "        logger.log_info(f\"  Items: {item_overlap:,} / {valid_i:,} ({item_overlap/valid_i*100:.1f}%)\")\n",
        "        if user_overlap < valid_u:\n",
        "            logger.log_warning(f\"  {valid_u - user_overlap:,} valid users NOT in train!\")\n",
        "        if item_overlap < valid_i:\n",
        "            logger.log_warning(f\"  {valid_i - item_overlap:,} valid items NOT in train!\")\n",
        "    \n",
        "    if df_test is not None:\n",
        "        stats(df_test, \"TEST\")\n",
        "    \n",
        "    logger.log_info(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-03 15:04:00,837 - INFO - ======================================================================\n",
            "2025-11-03 15:04:00,838 - INFO - DIAGNOSTIC: Electronics (suffix=big)\n",
            "2025-11-03 15:04:00,839 - INFO - ======================================================================\n",
            "2025-11-03 15:04:00,897 - INFO - TRAIN: 51,914 ratings, 32,556 users, sparsity 94.30%\n",
            "2025-11-03 15:04:00,898 - INFO - VALID: 456 ratings, 456 users, sparsity 96.15%\n",
            "2025-11-03 15:04:00,914 - INFO - \n",
            "OVERLAP:\n",
            "2025-11-03 15:04:00,915 - INFO -   Users: 456 / 456 (100.0%)\n",
            "2025-11-03 15:04:00,915 - INFO -   Items: 26 / 26 (100.0%)\n",
            "2025-11-03 15:04:00,916 - INFO - TEST: 349 ratings, 349 users, sparsity 96.30%\n",
            "2025-11-03 15:04:00,916 - INFO - ======================================================================\n",
            "\n",
            "2025-11-03 15:04:00,918 - INFO - ======================================================================\n",
            "2025-11-03 15:04:00,918 - INFO - DIAGNOSTIC: Beauty_and_Personal_Care (suffix=big)\n",
            "2025-11-03 15:04:00,918 - INFO - ======================================================================\n",
            "2025-11-03 15:04:00,924 - INFO - TRAIN: 22,143 ratings, 16,821 users, sparsity 94.28%\n",
            "2025-11-03 15:04:00,925 - INFO - VALID: 336 ratings, 336 users, sparsity 95.65%\n",
            "2025-11-03 15:04:00,928 - INFO - \n",
            "OVERLAP:\n",
            "2025-11-03 15:04:00,929 - INFO -   Users: 336 / 336 (100.0%)\n",
            "2025-11-03 15:04:00,929 - INFO -   Items: 23 / 23 (100.0%)\n",
            "2025-11-03 15:04:00,929 - INFO - TEST: 304 ratings, 304 users, sparsity 95.65%\n",
            "2025-11-03 15:04:00,930 - INFO - ======================================================================\n",
            "\n",
            "2025-11-03 15:04:00,932 - INFO - ======================================================================\n",
            "2025-11-03 15:04:00,932 - INFO - DIAGNOSTIC: Sports_and_Outdoors (suffix=big)\n",
            "2025-11-03 15:04:00,933 - INFO - ======================================================================\n",
            "2025-11-03 15:04:00,939 - INFO - TRAIN: 14,534 ratings, 11,776 users, sparsity 94.63%\n",
            "2025-11-03 15:04:00,939 - INFO - VALID: 157 ratings, 157 users, sparsity 95.65%\n",
            "2025-11-03 15:04:00,941 - INFO - \n",
            "OVERLAP:\n",
            "2025-11-03 15:04:00,941 - INFO -   Users: 157 / 157 (100.0%)\n",
            "2025-11-03 15:04:00,942 - INFO -   Items: 23 / 23 (100.0%)\n",
            "2025-11-03 15:04:00,942 - INFO - TEST: 147 ratings, 147 users, sparsity 95.45%\n",
            "2025-11-03 15:04:00,942 - INFO - ======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for cat in Configurations.CATEGORIES:\n",
        "    diagnose_dataset(cat, \"big\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

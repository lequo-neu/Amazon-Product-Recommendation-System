{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Recommendation System\n",
    "**Goal**\n",
    "- Combine User-CF, Item-CF, Content-CF, Model-CF, Trending models\n",
    "- Adaptive weighting based on user scenario (new/cold/warm)\n",
    "- Evaluate on warm-warm, warm-cold, cold-warm scenarios\n",
    "- Save artifacts for API integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, pickle, time\n",
    "import numpy as np, polars as pl\n",
    "from pathlib import Path\n",
    "from scipy.sparse import load_npz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..', 'utilities'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "from logger import Logger\n",
    "from configurations import Configurations\n",
    "from visualization_helpers import visualize_final_results\n",
    "from evaluation_metrics import (\n",
    "       recall_at_k,\n",
    "       ndcg_at_k,\n",
    "       map_at_k\n",
    ")\n",
    "\n",
    "m_log_file = Configurations.LOG_PATH\n",
    "logger = Logger(process_name=\"hybrid\", log_file=m_log_file)\n",
    "\n",
    "PROCESSED_DIR = Path(Configurations.DATA_PROCESSED_PATH)\n",
    "MODELS_DIR = Path(Configurations.MODELS_PATH)\n",
    "CATEGORY = Configurations.CATEGORIES\n",
    "\n",
    "logger.log_info(\"=\"*70)\n",
    "logger.log_info(\"HYBRID RECOMMENDATION SYSTEM\")\n",
    "logger.log_info(\"=\"*70)\n",
    "logger.log_info(f\"Categories: {CATEGORY}\")\n",
    "logger.log_info(f\"Sample size: {Configurations.DEV_SAMPLE_SIZE}\")\n",
    "logger.log_info(\"=\"*70 + \"\\n\")\n",
    "\n",
    "N_RECS = 10\n",
    "COLD_USER_THRESHOLD = 5  # Users with < 5 ratings = cold\n",
    "\n",
    "# %% [markdown]\n",
    "## Helper Functions\n",
    "\n",
    "# %%\n",
    "def _candidate_files(category: str, split: str = \"train\"):\n",
    "    dev_sample_size = Configurations.DEV_SAMPLE_SIZE\n",
    "    safe_cat = category.replace('/', '-')\n",
    "    \n",
    "    if dev_sample_size != 'full':\n",
    "        for size_name in Configurations.SAMPLE_SIZES.keys():\n",
    "            if size_name == dev_sample_size:\n",
    "                return PROCESSED_DIR / f\"{safe_cat}.5core.{split}.{size_name}.parquet\"\n",
    "    else:\n",
    "        return PROCESSED_DIR / f\"{safe_cat}.5core.{split}.parquet\"\n",
    "\n",
    "def load_5core_data(category: str, split: str = \"train\") -> pl.DataFrame:\n",
    "    p = _candidate_files(category, split)\n",
    "    logger.log_info(f\"[Load-{split.upper()}] {p}\")\n",
    "    df = pl.read_parquet(p, low_memory=False)\n",
    "    logger.log_info(f\"[Load-{split.upper()}] shape={df.shape} | \"\n",
    "                   f\"users={df['user_id'].n_unique()} | \"\n",
    "                   f\"items={df['parent_asin'].n_unique()}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Load All Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_models(category: str):\n",
    "    \"\"\"Load all 5 base models for hybrid\"\"\"\n",
    "    logger.log_info(f\"[Hybrid] Loading all models for {category}...\")\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # Load Model-CF (SVD)\n",
    "    model_dir = MODELS_DIR / 'model' / category\n",
    "    if model_dir.exists():\n",
    "        R = load_npz(model_dir / \"R.npz\")\n",
    "        U = np.load(model_dir / \"U.npy\")\n",
    "        V = np.load(model_dir / \"V.npy\")\n",
    "        \n",
    "        if V.shape[0] != U.shape[1]:\n",
    "            logger.log_warning(f\"V shape mismatch: {V.shape} (expected first dim = {U.shape[1]})\")\n",
    "            logger.log_info(f\"Transposing V to correct shape...\")\n",
    "            V = V.T\n",
    "            logger.log_info(f\"V shape after transpose: {V.shape}\")\n",
    "        \n",
    "        # Verify dimensions now match\n",
    "        assert V.shape[0] == U.shape[1], f\"U and V dimension mismatch: U{U.shape}, V{V.shape}\"\n",
    "        \n",
    "        with open(model_dir / \"user_rev.pkl\", \"rb\") as f:\n",
    "            user_rev = pickle.load(f)\n",
    "        with open(model_dir / \"item_rev.pkl\", \"rb\") as f:\n",
    "            item_rev = pickle.load(f)\n",
    "        \n",
    "        user_idx = json.loads((model_dir / \"user_idx.json\").read_text())\n",
    "        item_idx = json.loads((model_dir / \"item_idx.json\").read_text())\n",
    "        \n",
    "        best_factors = Configurations.load_best_factors(category)\n",
    "        \n",
    "        models['model'] = {\n",
    "            'R': R, 'U': U, 'V': V,\n",
    "            'user_rev': user_rev, 'item_rev': item_rev,\n",
    "            'user_idx': user_idx, 'item_idx': item_idx,\n",
    "            'n_factors': best_factors\n",
    "        }\n",
    "        logger.log_info(f\"  [4/5] Model-CF loaded (factors={best_factors}, U={U.shape}, V={V.shape})\")\n",
    "        \n",
    "        # Load Item-CF\n",
    "        item_dir = MODELS_DIR / 'item' / category\n",
    "        if item_dir.exists():\n",
    "            R = load_npz(item_dir / \"R.npz\")\n",
    "            Rc = load_npz(item_dir / \"Rc.npz\") if (item_dir / \"Rc.npz\").exists() else None\n",
    "            user_means = np.load(item_dir / \"user_means.npy\")\n",
    "            item_similarity = load_npz(item_dir / \"item_similarity.npz\")\n",
    "            \n",
    "            with open(item_dir / \"user_rev.pkl\", \"rb\") as f:\n",
    "                user_rev = pickle.load(f)\n",
    "            with open(item_dir / \"item_rev.pkl\", \"rb\") as f:\n",
    "                item_rev = pickle.load(f)\n",
    "            \n",
    "            user_idx = json.loads((item_dir / \"user_idx.json\").read_text())\n",
    "            item_idx = json.loads((item_dir / \"item_idx.json\").read_text())\n",
    "            \n",
    "            best_k = Configurations.load_best_k_item(category)\n",
    "            \n",
    "            models['item'] = {\n",
    "                'R': R, 'Rc': Rc, 'user_means': user_means,\n",
    "                'item_similarity': item_similarity,\n",
    "                'user_rev': user_rev, 'item_rev': item_rev,\n",
    "                'user_idx': user_idx, 'item_idx': item_idx,\n",
    "                'top_k_similar': best_k\n",
    "            }\n",
    "            logger.log_info(f\"  [2/5] Item-CF loaded (K={best_k})\")\n",
    "        \n",
    "        # Load Content-CF\n",
    "        content_dir = MODELS_DIR / 'content' / category\n",
    "        if content_dir.exists():\n",
    "            R = load_npz(content_dir / \"R.npz\")\n",
    "            item_similarity = load_npz(content_dir / \"item_similarity.npz\")\n",
    "            \n",
    "            with open(content_dir / \"user_rev.pkl\", \"rb\") as f:\n",
    "                user_rev = pickle.load(f)\n",
    "            with open(content_dir / \"item_rev.pkl\", \"rb\") as f:\n",
    "                item_rev = pickle.load(f)\n",
    "            \n",
    "            user_idx = json.loads((content_dir / \"user_idx.json\").read_text())\n",
    "            item_idx = json.loads((content_dir / \"item_idx.json\").read_text())\n",
    "            \n",
    "            best_k = Configurations.load_best_k_content(category)\n",
    "            \n",
    "            models['content'] = {\n",
    "                'R': R, 'item_similarity': item_similarity,\n",
    "                'user_rev': user_rev, 'item_rev': item_rev,\n",
    "                'user_idx': user_idx, 'item_idx': item_idx,\n",
    "                'top_k_similar': best_k\n",
    "            }\n",
    "            logger.log_info(f\"  [3/5] Content-CF loaded (K={best_k})\")\n",
    "        \n",
    "        # Load Model-CF (SVD)\n",
    "        model_dir = MODELS_DIR / 'model' / category\n",
    "        if model_dir.exists():\n",
    "            R = load_npz(model_dir / \"R.npz\")\n",
    "            U = np.load(model_dir / \"U.npy\")\n",
    "            V = np.load(model_dir / \"V.npy\")\n",
    "            \n",
    "            with open(model_dir / \"user_rev.pkl\", \"rb\") as f:\n",
    "                user_rev = pickle.load(f)\n",
    "            with open(model_dir / \"item_rev.pkl\", \"rb\") as f:\n",
    "                item_rev = pickle.load(f)\n",
    "            \n",
    "            user_idx = json.loads((model_dir / \"user_idx.json\").read_text())\n",
    "            item_idx = json.loads((model_dir / \"item_idx.json\").read_text())\n",
    "            \n",
    "            best_factors = Configurations.load_best_factors(category)\n",
    "            \n",
    "            models['model'] = {\n",
    "                'R': R, 'U': U, 'V': V,\n",
    "                'user_rev': user_rev, 'item_rev': item_rev,\n",
    "                'user_idx': user_idx, 'item_idx': item_idx,\n",
    "                'n_factors': best_factors\n",
    "            }\n",
    "            logger.log_info(f\"  [4/5] Model-CF loaded (factors={best_factors})\")\n",
    "        \n",
    "        # Load Trending\n",
    "        trending_dir = MODELS_DIR / 'trending' / category\n",
    "        if trending_dir.exists():\n",
    "            item_stats = pl.read_parquet(trending_dir / \"item_stats.parquet\")\n",
    "            R = load_npz(trending_dir / \"R.npz\")\n",
    "            \n",
    "            with open(trending_dir / \"user_rev.pkl\", \"rb\") as f:\n",
    "                user_rev = pickle.load(f)\n",
    "            with open(trending_dir / \"item_rev.pkl\", \"rb\") as f:\n",
    "                item_rev = pickle.load(f)\n",
    "            \n",
    "            user_idx = json.loads((trending_dir / \"user_idx.json\").read_text())\n",
    "            item_idx = json.loads((trending_dir / \"item_idx.json\").read_text())\n",
    "            \n",
    "            models['trending'] = {\n",
    "                'item_stats': item_stats,\n",
    "                'R': R,\n",
    "                'user_rev': user_rev, 'item_rev': item_rev,\n",
    "                'user_idx': user_idx, 'item_idx': item_idx\n",
    "            }\n",
    "            logger.log_info(f\"  [5/5] Trending loaded\")\n",
    "        \n",
    "        logger.log_info(f\"[Hybrid] Loaded {len(models)} models\")\n",
    "        \n",
    "        return models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Scenario Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_user_scenario(user_id: str, models: dict, threshold: int = 5):\n",
    "    \"\"\"\n",
    "    Detect user scenario: new-user, cold-user, warm-user\n",
    "    \n",
    "    Returns: (scenario, n_ratings)\n",
    "    \"\"\"\n",
    "    # Check in each model\n",
    "    for model_name in ['user', 'item', 'content', 'model']:\n",
    "        if model_name not in models:\n",
    "            continue\n",
    "        \n",
    "        user_idx = models[model_name].get('user_idx', {})\n",
    "        if user_id not in user_idx:\n",
    "            continue\n",
    "        \n",
    "        R = models[model_name].get('R')\n",
    "        if R is None:\n",
    "            continue\n",
    "        \n",
    "        u = int(user_idx[user_id])\n",
    "        n_ratings = R.getrow(u).nnz\n",
    "        \n",
    "        if n_ratings == 0:\n",
    "            return 'new-user', 0\n",
    "        elif n_ratings < threshold:\n",
    "            return 'cold-user', n_ratings\n",
    "        else:\n",
    "            return 'warm-user', n_ratings\n",
    "    \n",
    "    return 'new-user', 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_user_cf(user_id: str, models: dict):\n",
    "    \"\"\"User-CF prediction\"\"\"\n",
    "    if 'user' not in models:\n",
    "        return None\n",
    "    \n",
    "    art = models['user']\n",
    "    user_idx = art['user_idx']\n",
    "    \n",
    "    if user_id not in user_idx:\n",
    "        return None\n",
    "    \n",
    "    u = int(user_idx[user_id])\n",
    "    X = art['Rc'] if art.get('Rc') is not None else art['R']\n",
    "    nn_model = art['nn_model']\n",
    "    k = art.get('k_neighbors', 30)\n",
    "    \n",
    "    # Get similarities\n",
    "    if isinstance(nn_model, dict) and 'similarity' in nn_model:\n",
    "        sim_matrix = nn_model['similarity']\n",
    "        user_sims = sim_matrix[u]\n",
    "        top_k_idx = np.argsort(-user_sims)[:k]\n",
    "        sims = user_sims[top_k_idx]\n",
    "    else:\n",
    "        distances, indices = nn_model.kneighbors(X.getrow(u), return_distance=True)\n",
    "        d, idx = distances.ravel(), indices.ravel()\n",
    "        mask = idx != u\n",
    "        idx = idx[mask][:k]\n",
    "        sims = np.clip(1.0 - d[mask][:k], 0.0, 1.0)\n",
    "        top_k_idx = idx\n",
    "    \n",
    "    if len(top_k_idx) == 0:\n",
    "        return None\n",
    "    \n",
    "    sims = np.clip(sims, 0, None)\n",
    "    denom = np.sum(np.abs(sims)) + 1e-8\n",
    "    scores = X[top_k_idx, :].T.dot(sims) / denom\n",
    "    \n",
    "    if art.get('Rc') is not None:\n",
    "        scores = scores + art['user_means'][u]\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def predict_item_cf(user_id: str, models: dict):\n",
    "    \"\"\"Item-CF prediction\"\"\"\n",
    "    if 'item' not in models:\n",
    "        return None\n",
    "    \n",
    "    art = models['item']\n",
    "    user_idx = art['user_idx']\n",
    "    \n",
    "    if user_id not in user_idx:\n",
    "        return None\n",
    "    \n",
    "    u = int(user_idx[user_id])\n",
    "    R = art['R']\n",
    "    Rc = art.get('Rc')\n",
    "    X = Rc if Rc is not None else R\n",
    "    \n",
    "    user_ratings = X.getrow(u).toarray().ravel()\n",
    "    rated_items = np.nonzero(R.getrow(u).toarray().ravel())[0]\n",
    "    \n",
    "    if len(rated_items) == 0:\n",
    "        return None\n",
    "    \n",
    "    item_sim = art['item_similarity']\n",
    "    top_k = art.get('top_k_similar', 30)\n",
    "    scores = np.zeros(R.shape[1], dtype=np.float32)\n",
    "    \n",
    "    sim_matrix = item_sim[:, rated_items].toarray()\n",
    "    \n",
    "    for i in range(R.shape[1]):\n",
    "        sims = sim_matrix[i, :]\n",
    "        positive_mask = sims > 0\n",
    "        \n",
    "        if not positive_mask.any():\n",
    "            continue\n",
    "        \n",
    "        sims_pos = sims[positive_mask]\n",
    "        ratings_pos = user_ratings[rated_items[positive_mask]]\n",
    "        \n",
    "        k_use = min(top_k, len(sims_pos))\n",
    "        \n",
    "        if k_use > 0:\n",
    "            if k_use < len(sims_pos):\n",
    "                top_idx = np.argpartition(-sims_pos, k_use-1)[:k_use]\n",
    "            else:\n",
    "                top_idx = np.arange(len(sims_pos))\n",
    "            \n",
    "            final_sims = sims_pos[top_idx]\n",
    "            final_ratings = ratings_pos[top_idx]\n",
    "            \n",
    "            sim_sum = np.sum(final_sims)\n",
    "            if sim_sum > 1e-8:\n",
    "                scores[i] = np.dot(final_sims, final_ratings) / sim_sum\n",
    "    \n",
    "    if Rc is not None:\n",
    "        scores = scores + art['user_means'][u]\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def predict_content_cf(user_id: str, models: dict):\n",
    "    \"\"\"Content-CF prediction\"\"\"\n",
    "    if 'content' not in models:\n",
    "        return None\n",
    "    \n",
    "    art = models['content']\n",
    "    user_idx = art['user_idx']\n",
    "    \n",
    "    if user_id not in user_idx:\n",
    "        return None\n",
    "    \n",
    "    u = int(user_idx[user_id])\n",
    "    R = art['R']\n",
    "    \n",
    "    user_ratings = R.getrow(u).toarray().ravel()\n",
    "    rated_items = np.nonzero(user_ratings)[0]\n",
    "    \n",
    "    if len(rated_items) == 0:\n",
    "        return None\n",
    "    \n",
    "    item_sim = art['item_similarity']\n",
    "    top_k = art.get('top_k_similar', 30)\n",
    "    scores = np.zeros(R.shape[1], dtype=np.float32)\n",
    "    \n",
    "    sim_matrix = item_sim[:, rated_items].toarray()\n",
    "    \n",
    "    for i in range(R.shape[1]):\n",
    "        sims = sim_matrix[i, :]\n",
    "        positive_mask = sims > 0\n",
    "        \n",
    "        if not positive_mask.any():\n",
    "            continue\n",
    "        \n",
    "        sims_pos = sims[positive_mask]\n",
    "        ratings_pos = user_ratings[rated_items[positive_mask]]\n",
    "        \n",
    "        k_use = min(top_k, len(sims_pos))\n",
    "        \n",
    "        if k_use > 0:\n",
    "            if k_use < len(sims_pos):\n",
    "                top_idx = np.argpartition(-sims_pos, k_use-1)[:k_use]\n",
    "            else:\n",
    "                top_idx = np.arange(len(sims_pos))\n",
    "            \n",
    "            final_sims = sims_pos[top_idx]\n",
    "            final_ratings = ratings_pos[top_idx]\n",
    "            \n",
    "            sim_sum = np.sum(final_sims)\n",
    "            if sim_sum > 1e-8:\n",
    "                scores[i] = np.dot(final_sims, final_ratings) / sim_sum\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def predict_model_cf(user_id: str, models: dict):\n",
    "    if 'model' not in models:\n",
    "        return None\n",
    "    \n",
    "    art = models['model']\n",
    "    user_idx = art['user_idx']\n",
    "    \n",
    "    if user_id not in user_idx:\n",
    "        return None\n",
    "    \n",
    "    u = int(user_idx[user_id])\n",
    "    U = art['U']\n",
    "    V = art['V']\n",
    "    \n",
    "    # Safety check: Ensure dimensions match\n",
    "    # U[u] should be (n_factors,)\n",
    "    # V should be (n_factors, n_items)\n",
    "    if V.shape[0] == U.shape[1]:\n",
    "        # V is correct shape (n_factors, n_items)\n",
    "        scores = U[u] @ V\n",
    "    else:\n",
    "        # V is wrong shape, need transpose\n",
    "        logger.log_warning(f\"V shape mismatch in predict: U{U.shape}, V{V.shape}, transposing...\")\n",
    "        scores = U[u] @ V.T\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_trending_items(models: dict, n_items: int = 100):\n",
    "    \"\"\"Get trending items as array\"\"\"\n",
    "    if 'trending' not in models:\n",
    "        return None\n",
    "    \n",
    "    art = models['trending']\n",
    "    item_stats = art['item_stats']\n",
    "    \n",
    "    # Get top trending\n",
    "    top_items = item_stats['parent_asin'].to_list()[:n_items]\n",
    "    \n",
    "    # Convert to scores array matching matrix size\n",
    "    item_idx = art['item_idx']\n",
    "    scores = np.zeros(len(item_idx), dtype=np.float32)\n",
    "    \n",
    "    for rank, item in enumerate(top_items):\n",
    "        if item in item_idx:\n",
    "            idx = int(item_idx[item])\n",
    "            # Reciprocal rank scoring\n",
    "            scores[idx] = 1.0 / (rank + 1)\n",
    "    \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Adaptive Weight Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveWeights:    \n",
    "    def __init__(self):\n",
    "        self.configs = {\n",
    "            'new-user': {\n",
    "                'user': 0.0, 'item': 0.0, 'content': 0.0, 'model': 0.0, 'trending': 1.0\n",
    "            },\n",
    "            'cold-user': {\n",
    "                'user': 0.1, 'item': 0.1, 'content': 0.3, 'model': 0.1, 'trending': 0.4\n",
    "            },\n",
    "            'warm-user': {\n",
    "                'user': 0.25, 'item': 0.35, 'content': 0.20, 'model': 0.20, 'trending': 0.0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_weights(self, scenario: str, available_models: list, n_ratings: int = 0):\n",
    "        \"\"\"Get adaptive weights\"\"\"\n",
    "        base = self.configs.get(scenario, self.configs['warm-user'])\n",
    "        \n",
    "        # Filter to available\n",
    "        weights = {m: w for m, w in base.items() if m in available_models}\n",
    "        \n",
    "        # Adjust by activity level\n",
    "        if scenario == 'cold-user' and n_ratings > 0:\n",
    "            # More ratings = more weight on CF\n",
    "            if n_ratings >= 3:\n",
    "                if 'user' in weights:\n",
    "                    weights['user'] *= 1.5\n",
    "                if 'item' in weights:\n",
    "                    weights['item'] *= 1.5\n",
    "                if 'trending' in weights:\n",
    "                    weights['trending'] *= 0.7\n",
    "        \n",
    "        elif scenario == 'warm-user' and n_ratings > 20:\n",
    "            # Very active user = more weight on CF\n",
    "            if 'user' in weights:\n",
    "                weights['user'] *= 1.2\n",
    "            if 'item' in weights:\n",
    "                weights['item'] *= 1.2\n",
    "        \n",
    "        # Normalize\n",
    "        total = sum(weights.values())\n",
    "        if total > 0:\n",
    "            weights = {k: v / total for k, v in weights.items()}\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Hybrid Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hybrid(user_id: str, models: dict, weight_calc: AdaptiveWeights):\n",
    "    # Detect scenario\n",
    "    scenario, n_ratings = detect_user_scenario(user_id, models, COLD_USER_THRESHOLD)\n",
    "    \n",
    "    logger.log_info(f\"[Hybrid] User {user_id[:12]}... | Scenario: {scenario} | Ratings: {n_ratings}\")\n",
    "    \n",
    "    # Get predictions from each model\n",
    "    predictions = {}\n",
    "    \n",
    "    user_scores = predict_user_cf(user_id, models)\n",
    "    if user_scores is not None:\n",
    "        predictions['user'] = user_scores\n",
    "    \n",
    "    item_scores = predict_item_cf(user_id, models)\n",
    "    if item_scores is not None:\n",
    "        predictions['item'] = item_scores\n",
    "    \n",
    "    content_scores = predict_content_cf(user_id, models)\n",
    "    if content_scores is not None:\n",
    "        predictions['content'] = content_scores\n",
    "    \n",
    "    model_scores = predict_model_cf(user_id, models)\n",
    "    if model_scores is not None:\n",
    "        predictions['model'] = model_scores\n",
    "    \n",
    "    trending_scores = get_trending_items(models, n_items=100)\n",
    "    if trending_scores is not None:\n",
    "        predictions['trending'] = trending_scores\n",
    "    \n",
    "    if not predictions:\n",
    "        logger.log_warning(f\"[Hybrid] No predictions available for {user_id}\")\n",
    "        return None, \"no_models\", {}\n",
    "    \n",
    "    # Get weights\n",
    "    available = list(predictions.keys())\n",
    "    weights = weight_calc.get_weights(scenario, available, n_ratings)\n",
    "    \n",
    "    logger.log_info(f\"[Hybrid] Models: {available}\")\n",
    "    logger.log_info(f\"[Hybrid] Weights: {format_weights(weights)}\")\n",
    "    \n",
    "    # Combine predictions\n",
    "    ref_model = available[0]\n",
    "    n_items = len(predictions[ref_model])\n",
    "    combined = np.zeros(n_items, dtype=np.float32)\n",
    "    \n",
    "    for model_name, scores in predictions.items():\n",
    "        weight = weights.get(model_name, 0.0)\n",
    "        if weight > 0:\n",
    "            combined += weight * scores\n",
    "    \n",
    "    # Build strategy string\n",
    "    strategy_parts = [f\"{m}({w*100:.0f}%)\" for m, w in sorted(weights.items(), key=lambda x: -x[1]) if w > 0.05]\n",
    "    strategy = f\"hybrid-{scenario}-\" + \"+\".join(strategy_parts)\n",
    "    \n",
    "    metadata = {\n",
    "        'scenario': scenario,\n",
    "        'n_ratings': n_ratings,\n",
    "        'weights': weights,\n",
    "        'models': available\n",
    "    }\n",
    "    \n",
    "    return combined, strategy, metadata\n",
    "\n",
    "\n",
    "def format_weights(weights: dict) -> str:\n",
    "    \"\"\"Format weights for logging\"\"\"\n",
    "    return \"{\" + \", \".join(f\"{k}:{v:.2f}\" for k, v in weights.items()) + \"}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_hybrid(user_id: str, n_recs: int, models: dict, weight_calc: AdaptiveWeights):    \n",
    "    # Predict\n",
    "    scores, strategy, metadata = predict_hybrid(user_id, models, weight_calc)\n",
    "    \n",
    "    if scores is None:\n",
    "        logger.log_warning(f\"[Recommend] No scores for {user_id}\")\n",
    "        return pl.DataFrame({\"parent_asin\": [], \"score\": []}), strategy\n",
    "    \n",
    "    # Get reference R and item_rev\n",
    "    ref_model = metadata['models'][0]\n",
    "    R = models[ref_model]['R']\n",
    "    item_rev = models[ref_model]['item_rev']\n",
    "    user_idx = models[ref_model]['user_idx']\n",
    "    \n",
    "    # Exclude rated items\n",
    "    rated = set()\n",
    "    if user_id in user_idx:\n",
    "        u = int(user_idx[user_id])\n",
    "        rated = set(R.getrow(u).indices.tolist())\n",
    "    \n",
    "    cand_mask = np.ones(len(scores), dtype=bool)\n",
    "    if rated:\n",
    "        cand_mask[list(rated)] = False\n",
    "    \n",
    "    cand_scores = scores[cand_mask]\n",
    "    \n",
    "    if cand_scores.size == 0:\n",
    "        return pl.DataFrame({\"parent_asin\": [], \"score\": []}), strategy\n",
    "    \n",
    "    # Select top-N\n",
    "    n_top = min(n_recs, cand_scores.size)\n",
    "    cand_indices = np.nonzero(cand_mask)[0]\n",
    "    top_pos = np.argpartition(-cand_scores, n_top - 1)[:n_top]\n",
    "    top_pos = top_pos[np.argsort(-cand_scores[top_pos])]\n",
    "    \n",
    "    rec_asins = [item_rev[cand_indices[i]] for i in top_pos]\n",
    "    rec_scores = [float(cand_scores[i]) for i in top_pos]\n",
    "    \n",
    "    return pl.DataFrame({\"parent_asin\": rec_asins, \"score\": rec_scores}), strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hybrid(category: str, models: dict, weight_calc: AdaptiveWeights,\n",
    "                   k_values: list = [10, 20, 50], split: str = \"test\", \n",
    "                   sample_users: int = 1000):\n",
    "    \n",
    "    logger.log_info(f\"[Eval-Hybrid] {category} on {split.upper()}\")\n",
    "    \n",
    "    df_eval = load_5core_data(category, split=split)\n",
    "    \n",
    "    # Get train users from any model\n",
    "    train_users = None\n",
    "    for model_name in ['user', 'item', 'content', 'model']:\n",
    "        if model_name in models:\n",
    "            train_users = list(models[model_name]['user_idx'].keys())\n",
    "            break\n",
    "    \n",
    "    if train_users is None:\n",
    "        logger.log_warning(\"[Eval-Hybrid] No models available\")\n",
    "        return None\n",
    "    \n",
    "    # Filter to train users\n",
    "    df_eval = df_eval.filter(pl.col('user_id').is_in(train_users))\n",
    "    \n",
    "    if len(df_eval) == 0:\n",
    "        logger.log_warning(\"[Eval-Hybrid] No data after filtering\")\n",
    "        return None\n",
    "    \n",
    "    logger.log_info(f\"[Eval-Hybrid] {len(df_eval):,} ratings, {df_eval['user_id'].n_unique():,} users\")\n",
    "    \n",
    "    # Sample users\n",
    "    eval_users = df_eval['user_id'].unique().to_list()\n",
    "    if len(eval_users) > sample_users:\n",
    "        np.random.seed(42)\n",
    "        eval_users = np.random.choice(eval_users, sample_users, replace=False).tolist()\n",
    "    \n",
    "    logger.log_info(f\"[Eval-Hybrid] Evaluating {len(eval_users)} users...\")\n",
    "    \n",
    "    # Initialize metrics\n",
    "    metrics_acc = {\n",
    "        **{f'recall@{k}': [] for k in k_values},\n",
    "        **{f'ndcg@{k}': [] for k in k_values},\n",
    "        **{f'map@{k}': [] for k in k_values}\n",
    "    }\n",
    "    \n",
    "    scenario_counts = {'new-user': 0, 'cold-user': 0, 'warm-user': 0}\n",
    "    evaluated = 0\n",
    "    \n",
    "    for user_id in eval_users:\n",
    "        # Get relevant items\n",
    "        user_eval = df_eval.filter(pl.col('user_id') == user_id)\n",
    "        relevant = set(user_eval.filter(pl.col('rating') >= 4)['parent_asin'].to_list())\n",
    "        \n",
    "        if len(relevant) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get recommendations\n",
    "        recs_df, strategy = recommend_hybrid(user_id, n_recs=50, models=models, weight_calc=weight_calc)\n",
    "        \n",
    "        if len(recs_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        recommended = recs_df['parent_asin'].to_list()\n",
    "        \n",
    "        # Track scenario\n",
    "        scenario = strategy.split('-')[1] if len(strategy.split('-')) > 1 else 'unknown'\n",
    "        scenario_counts[scenario] = scenario_counts.get(scenario, 0) + 1\n",
    "        \n",
    "        evaluated += 1\n",
    "        \n",
    "        # Calculate metrics\n",
    "        for k in k_values:\n",
    "            metrics_acc[f'recall@{k}'].append(recall_at_k(recommended, relevant, k))\n",
    "            metrics_acc[f'ndcg@{k}'].append(ndcg_at_k(recommended, relevant, k))\n",
    "            metrics_acc[f'map@{k}'].append(map_at_k(recommended, relevant, k))\n",
    "    \n",
    "    logger.log_info(f\"[Eval-Hybrid] Evaluated: {evaluated} users\")\n",
    "    logger.log_info(f\"[Eval-Hybrid] Scenarios: {scenario_counts}\")\n",
    "    \n",
    "    # Aggregate\n",
    "    results = {\n",
    "        'category': category,\n",
    "        'split': split,\n",
    "        'n_users': evaluated,\n",
    "        'rmse': np.nan,  # Hybrid uses ranking, not rating prediction\n",
    "        'accuracy': np.nan\n",
    "    }\n",
    "    \n",
    "    for k in k_values:\n",
    "        for metric in ['recall', 'ndcg', 'map']:\n",
    "            key = f'{metric}@{k}'\n",
    "            results[key] = np.mean(metrics_acc[key]) if metrics_acc[key] else 0.0\n",
    "    \n",
    "    logger.log_info(f\"[Eval-Hybrid] NDCG@10={results['ndcg@10']:.4f}, \"\n",
    "                   f\"Recall@10={results['recall@10']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Save/Load Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hybrid_config(out_dir: Path, weight_calc: AdaptiveWeights, metadata: dict):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    config = {\n",
    "        'weight_configs': weight_calc.configs,\n",
    "        'cold_threshold': COLD_USER_THRESHOLD,\n",
    "        'metadata': metadata\n",
    "    }\n",
    "    \n",
    "    with open(out_dir / 'config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    logger.log_info(f\"[Saved-Hybrid] {out_dir}\")\n",
    "\n",
    "\n",
    "def load_hybrid_config(model_dir: Path):\n",
    "    \"\"\"Load hybrid configuration\"\"\"\n",
    "    with open(model_dir / 'config.json', 'r') as f:\n",
    "        return json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hybrid_pipeline(category: str):\n",
    "    \"\"\"Main pipeline for hybrid system\"\"\"\n",
    "    \n",
    "    logger.log_info(f\"\\n{'='*70}\\nCATEGORY: {category}\\n{'='*70}\\n\")\n",
    "    \n",
    "    model_dir = MODELS_DIR / 'hybrid' / category\n",
    "    \n",
    "    # Load all base models\n",
    "    logger.log_info(\"[STEP 1] Loading base models...\")\n",
    "    models = load_all_models(category)\n",
    "    \n",
    "    if len(models) < 2:\n",
    "        logger.log_warning(f\"[Skip] Not enough models for {category} (need >= 2)\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize weight calculator\n",
    "    logger.log_info(\"\\n[STEP 2] Initializing adaptive weights...\")\n",
    "    weight_calc = AdaptiveWeights()\n",
    "    \n",
    "    # Evaluate on validation\n",
    "    logger.log_info(\"\\n[STEP 3] Evaluating on VALIDATION...\")\n",
    "    val_results = evaluate_hybrid(\n",
    "        category, models, weight_calc,\n",
    "        k_values=[10, 20, 50],\n",
    "        split=\"valid\",\n",
    "        sample_users=Configurations.get_eval_samples_tuning()\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test\n",
    "    logger.log_info(\"\\n[STEP 4] Evaluating on TEST...\")\n",
    "    test_results = evaluate_hybrid(\n",
    "        category, models, weight_calc,\n",
    "        k_values=[10, 20, 50],\n",
    "        split=\"test\",\n",
    "        sample_users=Configurations.get_eval_samples_final()\n",
    "    )\n",
    "    \n",
    "    # Save config\n",
    "    logger.log_info(\"\\n[STEP 5] Saving configuration...\")\n",
    "    save_hybrid_config(model_dir, weight_calc, {\n",
    "        'models_used': list(models.keys()),\n",
    "        'category': category\n",
    "    })\n",
    "    \n",
    "    logger.log_info(f\"\\n[Complete] Hybrid system ready for {category}\")\n",
    "    logger.log_info(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return {\n",
    "        'val_results': val_results,\n",
    "        'test_results': test_results,\n",
    "        'models_used': list(models.keys())\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.log_info(\"\\n\" + \"=\"*70)\n",
    "logger.log_info(\"HYBRID SYSTEM - ALL CATEGORIES\")\n",
    "logger.log_info(\"=\"*70 + \"\\n\")\n",
    "\n",
    "hybrid_results = {}\n",
    "\n",
    "for cat in CATEGORY:\n",
    "    try:\n",
    "        result = run_hybrid_pipeline(cat)\n",
    "        if result:\n",
    "            hybrid_results[cat] = result\n",
    "    except Exception as e:\n",
    "        logger.log_exception(f\"[Error] {cat}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.log_info(\"\\n\" + \"=\"*70)\n",
    "logger.log_info(\"SAVING RESULTS\")\n",
    "logger.log_info(\"=\"*70 + \"\\n\")\n",
    "\n",
    "test_results_list = [hybrid_results[cat]['test_results'] \n",
    "                     for cat in CATEGORY \n",
    "                     if cat in hybrid_results and hybrid_results[cat]['test_results']]\n",
    "\n",
    "if test_results_list:\n",
    "    df_final = pl.DataFrame(test_results_list)\n",
    "    \n",
    "    logger.log_info(\"Final Test Results:\")\n",
    "    display(df_final)\n",
    "    \n",
    "    out_csv = MODELS_DIR / 'hybrid' / 'final_test_results.csv'\n",
    "    df_final.write_csv(out_csv)\n",
    "    logger.log_info(f\"\\nSaved: {out_csv}\")\n",
    "    \n",
    "    visualize_final_results(\n",
    "        test_results_list,\n",
    "        save_dir=MODELS_DIR / 'hybrid',\n",
    "        algo_name='Hybrid',\n",
    "        k_values=[10, 20, 50]\n",
    "    )\n",
    "    logger.log_info(\"Saved: evaluation_results.png\\n\")\n",
    "\n",
    "logger.log_info(\"=\"*70)\n",
    "logger.log_info(\"HYBRID SYSTEM COMPLETE\")\n",
    "logger.log_info(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Compare all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_models():\n",
    "    \"\"\"Compare all algorithms on test set\"\"\"\n",
    "    \n",
    "    logger.log_info(\"\\n\" + \"=\"*70)\n",
    "    logger.log_info(\"COMPARISON: ALL ALGORITHMS\")\n",
    "    logger.log_info(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for algo in ['user', 'item', 'content', 'model', 'trending', 'hybrid']:\n",
    "        file = MODELS_DIR / algo / 'final_test_results.csv'\n",
    "        \n",
    "        if file.exists():\n",
    "            df = pl.read_csv(file)\n",
    "            df = df.with_columns(pl.lit(algo).alias('algorithm'))\n",
    "            results.append(df)\n",
    "            logger.log_info(f\"[Load] {algo}: {len(df)} categories\")\n",
    "    \n",
    "    if not results:\n",
    "        logger.log_warning(\"No results found\")\n",
    "        return\n",
    "    \n",
    "    df_all = pl.concat(results)\n",
    "    \n",
    "    # Display comparison\n",
    "    logger.log_info(\"\\nAll Results:\")\n",
    "    display(df_all.select(['algorithm', 'category', 'ndcg@10', 'recall@10', 'map@10']))\n",
    "    \n",
    "    # Save\n",
    "    out_csv = MODELS_DIR / 'comparison_all_models.csv'\n",
    "    df_all.write_csv(out_csv)\n",
    "    logger.log_info(f\"\\nSaved: {out_csv}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    plot_model_comparison(df_all)\n",
    "    \n",
    "    return df_all\n",
    "\n",
    "def plot_model_comparison(df_all: pl.DataFrame):\n",
    "    \"\"\"Plot comparison of all models\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    algorithms = df_all['algorithm'].unique().to_list()\n",
    "    metrics = ['ndcg@10', 'recall@10', 'map@10']\n",
    "    titles = ['NDCG@10', 'Recall@10', 'MAP@10']\n",
    "    \n",
    "    for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "        scores = []\n",
    "        for algo in algorithms:\n",
    "            algo_data = df_all.filter(pl.col('algorithm') == algo)\n",
    "            if len(algo_data) > 0:\n",
    "                scores.append(algo_data[metric][0])\n",
    "            else:\n",
    "                scores.append(0)\n",
    "        \n",
    "        axes[idx].bar(algorithms, scores, alpha=0.8)\n",
    "        axes[idx].set_ylabel('Score')\n",
    "        axes[idx].set_title(title, fontweight='bold')\n",
    "        axes[idx].set_xticks(axes[idx].get_xticks())\n",
    "        axes[idx].set_xticklabels(algorithms, rotation=45)\n",
    "        axes[idx].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add values on bars\n",
    "        for i, v in enumerate(scores):\n",
    "            axes[idx].text(i, v, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.suptitle('Model Comparison - All Algorithms', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    out_path = MODELS_DIR / 'model_comparison.png'\n",
    "    plt.savefig(out_path, dpi=150, bbox_inches='tight')\n",
    "    logger.log_info(f\"Saved comparison plot: {out_path}\")\n",
    "    plt.show()\n",
    "\n",
    "compare_all_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hybrid_recommendation():\n",
    "    \"\"\"Unit test: Verify hybrid works\"\"\"\n",
    "    \n",
    "    logger.log_info(\"\\n\" + \"=\"*70)\n",
    "    logger.log_info(\"[UNIT TEST] Hybrid Recommendation\")\n",
    "    logger.log_info(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    cat = CATEGORY[0]\n",
    "    \n",
    "    # Load models\n",
    "    models = load_all_models(cat)\n",
    "    weight_calc = AdaptiveWeights()\n",
    "    \n",
    "    # Get test users\n",
    "    user_rev = models[list(models.keys())[0]]['user_rev']\n",
    "    \n",
    "    # Test different user types\n",
    "    test_cases = [\n",
    "        ('First user', user_rev[0]),\n",
    "        ('Middle user', user_rev[len(user_rev)//2]),\n",
    "        ('Last user', user_rev[-1])\n",
    "    ]\n",
    "    \n",
    "    for case_name, user_id in test_cases:\n",
    "        logger.log_info(f\"\\n[Test] {case_name}: {user_id[:12]}...\")\n",
    "        logger.log_info(\"-\"*70)\n",
    "        \n",
    "        try:\n",
    "            recs_df, strategy = recommend_hybrid(user_id, N_RECS, models, weight_calc)\n",
    "            \n",
    "            logger.log_info(f\"  Strategy: {strategy}\")\n",
    "            logger.log_info(f\"  Recommendations: {len(recs_df)}\")\n",
    "            \n",
    "            if len(recs_df) > 0:\n",
    "                logger.log_info(f\"  Score range: [{recs_df['score'].min():.2f}, {recs_df['score'].max():.2f}]\")\n",
    "                display(recs_df.head(5))\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.log_exception(f\"  Error: {e}\")\n",
    "    \n",
    "    logger.log_info(\"\\n\" + \"=\"*70)\n",
    "    logger.log_info(\"UNIT TEST COMPLETE\")\n",
    "    logger.log_info(\"=\"*70 + \"\\n\")\n",
    "\n",
    "test_hybrid_recommendation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

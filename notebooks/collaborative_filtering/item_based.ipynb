{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Item-Based Collaborative Filtering (5-core â€¢ TRAIN)\n",
        "\n",
        "**Goal**\n",
        "- Build an item-based CF recommender on **5-core / TRAIN** for a given category.\n",
        "- Use sparse matrices + cosine similarity for scalability.\n",
        "- Produce Top-N recommendations for one user or a batch of users.\n",
        "\n",
        "**What this notebook does**\n",
        "1. Load 5-core **TRAIN** from `PROCESSED_DIR` with schema: `user_id`, `parent_asin`, `rating`, `timestamp`, `history`\n",
        "2. Build a **user-item** sparse matrix (CSR).\n",
        "3. Compute **item-item similarity** matrix using cosine similarity.\n",
        "4. Predict scores for **unseen items** and generate **Top-N** recommendations.\n",
        "5. Evaluate using TEST/VALID sets: Accuracy, RMSE, Recall@K, NDCG@K, MAP@K.\n",
        "6. (Optional) Save recommendations to disk for UI integration.\n",
        "\n",
        "> Notes:\n",
        "> - We compute item-item similarity on the item-user matrix (transpose of R).\n",
        "> - Ratings may be mean-centered per user (optional).\n",
        "> - Predictions based on weighted sum of similar items."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task: Import modules and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, json, pickle, time\n",
        "import numpy as np, polars as pl\n",
        "from pathlib import Path\n",
        "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "module_path = os.path.abspath(os.path.join('..', '../utilities'))\n",
        "sys.path.append(module_path)\n",
        "\n",
        "from logger import Logger\n",
        "from configurations import Configurations\n",
        "from visualization_helpers import (\n",
        "    visualize_hyperparameter_tuning,\n",
        "    visualize_final_results,\n",
        "    visualize_val_test_comparison\n",
        ")\n",
        "from evaluation_metrics import (\n",
        "       compute_rmse_accuracy,\n",
        "       recall_at_k,\n",
        "       ndcg_at_k,\n",
        "       map_at_k\n",
        ")\n",
        "\n",
        "m_log_file = Configurations.LOG_PATH\n",
        "logger = Logger(process_name=\"item_based\", log_file=m_log_file)\n",
        "\n",
        "PROCESSED_DIR = Path(Configurations.DATA_PROCESSED_PATH)\n",
        "RAW_DIR = Path(Configurations.DATA_RAW_PATH)\n",
        "MODELS_DIR = Path(Configurations.MODELS_PATH)\n",
        "\n",
        "# ============================================================================\n",
        "# WORKFLOW CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "CATEGORY = Configurations.CATEGORIES\n",
        "\n",
        "# Auto-detect phase\n",
        "has_tuning = Configurations.has_tuning_results_item(CATEGORY[0]) \n",
        "\n",
        "K_VALUES = Configurations.K_VALUES_ITEM\n",
        "if has_tuning:\n",
        "    PHASE = 'final'\n",
        "    logger.log_info(\"=\"*70)\n",
        "    logger.log_info(\"PHASE: FINAL EVALUATION (ITEM-BASED)\")\n",
        "    logger.log_info(\"=\"*70)\n",
        "    \n",
        "    for cat in CATEGORY:\n",
        "        best_k = Configurations.load_best_k_item(cat)\n",
        "        logger.log_info(f\"  {cat}: Best K = {best_k}\")\n",
        "else:\n",
        "    PHASE = 'train_tune'\n",
        "    logger.log_info(\"=\"*70)\n",
        "    logger.log_info(\"PHASE: TRAINING + TUNING (ITEM-BASED)\")\n",
        "    logger.log_info(\"=\"*70)\n",
        "    logger.log_info(f\"K values: {K_VALUES}\")\n",
        "\n",
        "logger.log_info(\"=\"*70)\n",
        "logger.log_info(f\"Categories: {CATEGORY}\")\n",
        "logger.log_info(f\"Sample size: {Configurations.DEV_SAMPLE_SIZE}\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Settings\n",
        "N_RECS = 10\n",
        "MEAN_CENTER = True\n",
        "MAX_USERS = None\n",
        "MAX_ITEMS = None\n",
        "TOP_K_SIMILAR = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task: Define functions for CF recommendation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _candidate_files(category: str, split: str = \"train\"):\n",
        "    dev_sample_size = Configurations.DEV_SAMPLE_SIZE\n",
        "\n",
        "    if dev_sample_size != 'full':\n",
        "        sample_sizes = Configurations.SAMPLE_SIZES\n",
        "        for size_name in sample_sizes.keys():\n",
        "            if size_name == dev_sample_size:\n",
        "             return PROCESSED_DIR / f\"{category.replace('/', '-')}.5core.{split}.{size_name}.parquet\"\n",
        "    else:\n",
        "        return PROCESSED_DIR / f\"{category.replace('/', '-')}.5core.{split}.parquet\"\n",
        "\n",
        "def load_5core_data(category: str, split: str = \"train\") -> pl.DataFrame:\n",
        "    p = _candidate_files(category, split)\n",
        "    df = pl.read_parquet(p, low_memory=False)\n",
        "    df = df.__copy__()\n",
        "    logger.log_info(f\"[Load-{split.upper()}] shape={df.shape} | users={df['user_id'].n_unique()} | items={df['parent_asin'].n_unique()}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Build Item-Based Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_item_model(df_train: pl.DataFrame, mean_center: bool = True, \n",
        "                    max_users: int | None = None, max_items: int | None = None,\n",
        "                    min_similarity: float = 0.01):\n",
        "    df = df_train.select(['user_id', 'parent_asin', 'rating']).with_columns(pl.col('rating').cast(pl.Float32))\n",
        "    if max_users is not None:\n",
        "        first_users = df['user_id'].unique()[:max_users].to_list()\n",
        "        df = df.filter(pl.col('user_id').is_in(first_users))\n",
        "    if max_items is not None:\n",
        "        first_items = df['parent_asin'].unique()[:max_items].to_list()\n",
        "        df = df.filter(pl.col('parent_asin').is_in(first_items))\n",
        "    user_rev = df['user_id'].unique().to_list()\n",
        "    item_rev = df['parent_asin'].unique().to_list()\n",
        "    user_idx = {u_id: idx for idx, u_id in enumerate(user_rev)}\n",
        "    item_idx = {a_id: idx for idx, a_id in enumerate(item_rev)}\n",
        "    u = np.array([user_idx[x] for x in df['user_id'].to_list()], dtype=np.int32)\n",
        "    i = np.array([item_idx[x] for x in df['parent_asin'].to_list()], dtype=np.int32)\n",
        "    v = np.array(df['rating'].to_list(), dtype=np.float32)\n",
        "    nU = len(user_rev)\n",
        "    nI = len(item_rev)\n",
        "    R = csr_matrix((v, (u, i)), shape=(nU, nI), dtype=np.float32)\n",
        "    user_means = np.zeros(nU, dtype=np.float32)\n",
        "    Rc = None\n",
        "    if mean_center:\n",
        "        Rc = R.copy().astype(np.float32)\n",
        "        row_sums = np.array(R.sum(axis=1)).ravel().astype(np.float32)\n",
        "        row_cnts = np.diff(R.indptr).astype(np.int32)\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            user_means = np.where(row_cnts > 0, row_sums / row_cnts, 0.0).astype(np.float32)\n",
        "        if Rc.nnz:\n",
        "            Rc.data -= np.repeat(user_means, row_cnts)\n",
        "    X = Rc if Rc is not None else R\n",
        "    logger.log_info(f\"[Item-Similarity] Computing cosine similarity for {nI} items...\")\n",
        "    item_similarity = cosine_similarity(X.T, dense_output=False)\n",
        "    if min_similarity > 0:\n",
        "        item_similarity.data[item_similarity.data < min_similarity] = 0\n",
        "        item_similarity.eliminate_zeros()\n",
        "        logger.log_info(f\"[Item-Similarity] Applied threshold {min_similarity}, \"\n",
        "                        f\"kept {item_similarity.nnz:,} similarities\")\n",
        "    user_rev_arr = np.array(user_rev, dtype=object)\n",
        "    item_rev_arr = np.array(item_rev, dtype=object)\n",
        "    logger.log_info(f\"[Item-Model] R{R.shape} nnz={R.nnz} | Similarity{item_similarity.shape}\")\n",
        "    return R, Rc, user_idx, item_idx, user_rev_arr, item_rev_arr, user_means, item_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prediction & Recommendation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_item_based(user_idx_val: int, R: csr_matrix, Rc: csr_matrix | None, \n",
        "                      item_similarity: csr_matrix, user_means: np.ndarray, \n",
        "                      top_k: int = 30) -> np.ndarray:\n",
        "    X = Rc if Rc is not None else R\n",
        "    user_ratings = X.getrow(user_idx_val).toarray().ravel()\n",
        "    rated_items = np.nonzero(R.getrow(user_idx_val).toarray().ravel())[0]\n",
        "    \n",
        "    if len(rated_items) == 0:\n",
        "        return np.zeros(R.shape[1], dtype=np.float32)\n",
        "    \n",
        "    rated_ratings = user_ratings[rated_items]\n",
        "    n_items = R.shape[1]\n",
        "    scores = np.zeros(n_items, dtype=np.float32)\n",
        "    \n",
        "    # Get similarity matrix: all items vs rated items\n",
        "    sim_matrix = item_similarity[:, rated_items].toarray()\n",
        "    \n",
        "    for i in range(n_items):\n",
        "        sims = sim_matrix[i, :]\n",
        "        \n",
        "        # Only use POSITIVE similarities\n",
        "        positive_mask = sims > 0\n",
        "        n_positive = positive_mask.sum()\n",
        "        \n",
        "        # SAFETY CHECK 1: Must have positive similarities\n",
        "        if n_positive == 0:\n",
        "            continue\n",
        "        \n",
        "        sims_positive = sims[positive_mask]\n",
        "        ratings_positive = rated_ratings[positive_mask]\n",
        "        \n",
        "        # SAFETY CHECK 2: Validate arrays match\n",
        "        assert len(sims_positive) == len(ratings_positive), \\\n",
        "            f\"Length mismatch: sims={len(sims_positive)}, ratings={len(ratings_positive)}\"\n",
        "        \n",
        "        # Select top-K\n",
        "        k_use = min(top_k, n_positive)\n",
        "        \n",
        "        # ðŸ”¥ SAFETY CHECK 3: k_use must be valid\n",
        "        if k_use <= 0:\n",
        "            continue\n",
        "        \n",
        "        if k_use < n_positive:\n",
        "            # Need to select top-K\n",
        "            # FIX: Use k_use-1 as kth parameter for argpartition\n",
        "            if k_use == 1:\n",
        "                top_idx = np.array([np.argmax(sims_positive)])\n",
        "            else:\n",
        "                top_idx = np.argpartition(-sims_positive, min(k_use-1, len(sims_positive)-1))[:k_use]\n",
        "        else:\n",
        "            # Use all positive similarities\n",
        "            top_idx = np.arange(n_positive)\n",
        "        \n",
        "        # SAFETY CHECK 4: Validate indices\n",
        "        if len(top_idx) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Get final similarities and ratings\n",
        "        final_sims = sims_positive[top_idx]\n",
        "        final_ratings = ratings_positive[top_idx]\n",
        "        \n",
        "        # SAFETY CHECK 5: Must have valid data\n",
        "        if len(final_sims) == 0 or len(final_ratings) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Weighted average\n",
        "        sim_sum = np.sum(final_sims)\n",
        "        if sim_sum > 1e-8:\n",
        "            scores[i] = np.dot(final_sims, final_ratings) / sim_sum\n",
        "    \n",
        "    # Add back user mean if centered\n",
        "    if Rc is not None:\n",
        "        scores = scores + user_means[user_idx_val]\n",
        "    \n",
        "    return scores\n",
        "\n",
        "\n",
        "def recommend_item_based(user_id: str, n_recs: int, artifacts: dict) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Generate top-N recommendations for a user using item-based CF\n",
        "    \"\"\"\n",
        "    R = artifacts['R']\n",
        "    Rc = artifacts.get('Rc')\n",
        "    user_idx = artifacts['user_idx']\n",
        "    item_rev = artifacts['item_rev']\n",
        "    user_means = artifacts['user_means']\n",
        "    item_similarity = artifacts['item_similarity']\n",
        "    \n",
        "    # Get top_k from artifacts or use default\n",
        "    top_k = artifacts.get('top_k_similar', 30)\n",
        "    \n",
        "    # Check if user exists\n",
        "    if user_id not in user_idx:\n",
        "        logger.log_warning(f\"[Recommend] user_id={user_id} not found.\")\n",
        "        return pl.DataFrame({\"parent_asin\": [], \"score\": []})\n",
        "    \n",
        "    # Get user index\n",
        "    u = user_idx[user_id]\n",
        "    \n",
        "    # Predict scores for all items\n",
        "    scores = predict_item_based(u, R, Rc, item_similarity, user_means, top_k=top_k)\n",
        "    \n",
        "    # Filter out already rated items\n",
        "    rated = set(R.getrow(u).indices.tolist())\n",
        "    cand_mask = np.ones(R.shape[1], dtype=bool)\n",
        "    if rated:\n",
        "        cand_mask[list(rated)] = False\n",
        "    \n",
        "    # Get candidate scores\n",
        "    cand_scores = scores[cand_mask]\n",
        "    \n",
        "    if cand_scores.size == 0:\n",
        "        logger.log_warning(f\"[Recommend] No candidate items for user {user_id}\")\n",
        "        return pl.DataFrame({\"parent_asin\": [], \"score\": []})\n",
        "    \n",
        "    # Select top-N\n",
        "    n_top = min(n_recs, cand_scores.size)\n",
        "    cand_indices = np.nonzero(cand_mask)[0]\n",
        "    \n",
        "    # Use argpartition for efficient top-N selection\n",
        "    top_pos = np.argpartition(-cand_scores, n_top - 1)[:n_top]\n",
        "    \n",
        "    # Sort by score descending\n",
        "    picked = sorted(\n",
        "        [(int(cand_indices[p]), float(cand_scores[p])) for p in top_pos],\n",
        "        key=lambda x: -x[1]\n",
        "    )\n",
        "    \n",
        "    # Extract ASINs and scores\n",
        "    rec_asins = [item_rev[i] for i, _ in picked]\n",
        "    rec_scores = [s for _, s in picked]\n",
        "    \n",
        "    return pl.DataFrame({\n",
        "        \"parent_asin\": rec_asins,\n",
        "        \"score\": rec_scores\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save/Load Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_item_artifacts(out_dir: Path, R, Rc, user_rev, item_rev, user_idx, item_idx, user_means, item_similarity):\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    save_npz(out_dir / \"R.npz\", R)\n",
        "    if Rc is not None:\n",
        "        save_npz(out_dir / \"Rc.npz\", Rc)\n",
        "    save_npz(out_dir / \"item_similarity.npz\", item_similarity)\n",
        "    np.save(out_dir / \"user_means.npy\", user_means)\n",
        "    with open(out_dir / \"user_rev.pkl\", \"wb\") as f: pickle.dump(user_rev, f)\n",
        "    with open(out_dir / \"item_rev.pkl\", \"wb\") as f: pickle.dump(item_rev, f)\n",
        "    (out_dir / \"user_idx.json\").write_text(json.dumps({str(k): int(v) for k, v in user_idx.items()}))\n",
        "    (out_dir / \"item_idx.json\").write_text(json.dumps({str(k): int(v) for k, v in item_idx.items()}))\n",
        "    logger.log_info(f\"[Saved-Item] {out_dir}\")\n",
        "\n",
        "def load_item_artifacts(model_dir: str | Path):\n",
        "    md = Path(model_dir)\n",
        "    R = load_npz(md / \"R.npz\")\n",
        "    Rc = load_npz(md / \"Rc.npz\") if (md / \"Rc.npz\").exists() else None\n",
        "    item_similarity = load_npz(md / \"item_similarity.npz\")\n",
        "    user_means = np.load(md / \"user_means.npy\")\n",
        "    with open(md / \"user_rev.pkl\", \"rb\") as f: user_rev = pickle.load(f)\n",
        "    with open(md / \"item_rev.pkl\", \"rb\") as f: item_rev = pickle.load(f)\n",
        "    user_idx = {k: int(v) for k, v in json.loads((md / \"user_idx.json\").read_text()).items()}\n",
        "    item_idx = {k: int(v) for k, v in json.loads((md / \"item_idx.json\").read_text()).items()}\n",
        "    return dict(R=R, Rc=Rc, item_similarity=item_similarity, user_means=user_means, user_rev=user_rev, item_rev=item_rev, user_idx=user_idx, item_idx=item_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_item_based(category: str, artifacts: dict, k_values: list = [10, 20, 50], \n",
        "                       split: str = \"test\", sample_users: int = 1000):\n",
        "    \n",
        "    logger.log_info(f\"[Eval-Item] {category} on {split.upper()}\")\n",
        "    \n",
        "    df_eval = load_5core_data(category, split=split)\n",
        "    R, Rc = artifacts['R'], artifacts.get('Rc')\n",
        "    user_idx, item_idx = artifacts['user_idx'], artifacts['item_idx']\n",
        "    user_means = artifacts['user_means']\n",
        "    item_similarity = artifacts['item_similarity']\n",
        "    \n",
        "    top_k = artifacts.get('top_k_similar', 30)\n",
        "    \n",
        "    # Filter to train users only\n",
        "    train_user_list = list(user_idx.keys())\n",
        "    df_eval = df_eval.filter(pl.col('user_id').is_in(train_user_list))\n",
        "    \n",
        "    if len(df_eval) == 0:\n",
        "        logger.log_warning(f\"[Eval-Item] No data after filtering\")\n",
        "        return None\n",
        "    \n",
        "    logger.log_info(f\"[Eval-Item] After filtering: {len(df_eval):,} ratings, {df_eval['user_id'].n_unique():,} users\")\n",
        "    \n",
        "    # Sample users for evaluation\n",
        "    eval_users = df_eval['user_id'].unique().to_list()\n",
        "    if len(eval_users) > sample_users:\n",
        "        np.random.seed(42)\n",
        "        eval_users = np.random.choice(eval_users, sample_users, replace=False).tolist()\n",
        "    \n",
        "    logger.log_info(f\"[Eval-Item] Evaluating {len(eval_users)} users with top_k={top_k}...\")\n",
        "    \n",
        "    # Initialize accumulators\n",
        "    metrics_acc = {\n",
        "        'rmse': [], 'accuracy': [],\n",
        "        **{f'recall@{k}': [] for k in k_values},\n",
        "        **{f'ndcg@{k}': [] for k in k_values},\n",
        "        **{f'map@{k}': [] for k in k_values}\n",
        "    }\n",
        "    \n",
        "    evaluated_users = 0\n",
        "    \n",
        "    for user_id in eval_users:\n",
        "        if user_id not in user_idx:\n",
        "            continue\n",
        "        \n",
        "        u = user_idx[user_id]\n",
        "        user_eval = df_eval.filter(pl.col('user_id') == user_id)\n",
        "        actual_items = set(user_eval['parent_asin'].to_list())\n",
        "        actual_ratings = {row['parent_asin']: row['rating'] \n",
        "                         for row in user_eval.iter_rows(named=True)}\n",
        "        \n",
        "        if len(actual_items) == 0:\n",
        "            continue\n",
        "        \n",
        "        known_items = {item for item in actual_items if item in item_idx}\n",
        "        \n",
        "        # Skip user if ALL items are unknown\n",
        "        if len(known_items) == 0:\n",
        "            continue\n",
        "        \n",
        "        evaluated_users += 1\n",
        "        scores = predict_item_based(u, R, Rc, item_similarity, user_means, top_k=top_k)\n",
        "        \n",
        "        # RMSE & Accuracy - only for known items\n",
        "        predictions = np.full(R.shape[1], np.nan)\n",
        "        actuals = np.full(R.shape[1], np.nan)\n",
        "        for asin in known_items:\n",
        "            idx = item_idx[asin]\n",
        "            predictions[idx] = scores[idx]\n",
        "            actuals[idx] = actual_ratings[asin]\n",
        "        \n",
        "        rmse, acc = compute_rmse_accuracy(predictions, actuals)\n",
        "        if not np.isnan(rmse):\n",
        "            metrics_acc['rmse'].append(rmse)\n",
        "            metrics_acc['accuracy'].append(acc)\n",
        "        \n",
        "        # Ranking metrics\n",
        "        rated = set(R.getrow(u).indices.tolist())\n",
        "        cand_mask = np.ones(R.shape[1], dtype=bool)\n",
        "        if rated:\n",
        "            cand_mask[list(rated)] = False\n",
        "        \n",
        "        cand_scores = scores[cand_mask]\n",
        "        if cand_scores.size == 0:\n",
        "            continue\n",
        "        \n",
        "        cand_indices = np.nonzero(cand_mask)[0]\n",
        "        max_k = max(k_values)\n",
        "        n_top = min(max_k, cand_scores.size)\n",
        "        top_pos = np.argpartition(-cand_scores, n_top - 1)[:n_top]\n",
        "        sorted_idx = top_pos[np.argsort(-cand_scores[top_pos])]\n",
        "        \n",
        "        item_rev = artifacts['item_rev']\n",
        "        recommended = [item_rev[cand_indices[i]] for i in sorted_idx]\n",
        "        \n",
        "        # Use known_items for ranking metrics\n",
        "        for k in k_values:\n",
        "            metrics_acc[f'recall@{k}'].append(recall_at_k(recommended, known_items, k))\n",
        "            metrics_acc[f'ndcg@{k}'].append(ndcg_at_k(recommended, known_items, k))\n",
        "            metrics_acc[f'map@{k}'].append(map_at_k(recommended, known_items, k))\n",
        "    \n",
        "    logger.log_info(f\"[Eval-Item] Actually evaluated: {evaluated_users} users\")\n",
        "    \n",
        "    # Aggregate results\n",
        "    results = {\n",
        "        'category': category,\n",
        "        'split': split,\n",
        "        'n_users': evaluated_users,\n",
        "        'rmse': np.mean(metrics_acc['rmse']) if metrics_acc['rmse'] else np.nan,\n",
        "        'accuracy': np.mean(metrics_acc['accuracy']) if metrics_acc['accuracy'] else np.nan\n",
        "    }\n",
        "    \n",
        "    for k in k_values:\n",
        "        for metric in ['recall', 'ndcg', 'map']:\n",
        "            key = f'{metric}@{k}'\n",
        "            results[key] = np.mean(metrics_acc[key]) if metrics_acc[key] else 0.0\n",
        "    \n",
        "    logger.log_info(f\"[Eval-Item] RMSE={results['rmse']:.4f}, Acc={results['accuracy']:.4f}\")\n",
        "    logger.log_info(f\"[Eval-Item] NDCG@10={results['ndcg@10']:.4f}, Recall@10={results['recall@10']:.4f}\")\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hyperparameter Tuning Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tune_k_similar_single_category(category: str, k_values: list, n_eval_users: int):\n",
        "    \"\"\"\n",
        "    Tune top_k_similar for a single category on VALIDATION set\n",
        "    \"\"\"\n",
        "    logger.log_info(f\"\\n{'='*70}\")\n",
        "    logger.log_info(f\"TUNING TOP_K_SIMILAR: {category}\")\n",
        "    logger.log_info(f\"{'='*70}\")\n",
        "    logger.log_info(f\"K values: {k_values}\")\n",
        "    logger.log_info(f\"Validation users: {n_eval_users}\\n\")\n",
        "    \n",
        "    # Load training data ONCE\n",
        "    logger.log_info(\"Loading training data...\")\n",
        "    df_train = load_5core_data(category, split=\"train\")\n",
        "    \n",
        "    # Build base model ONCE (item similarity matrix)\n",
        "    logger.log_info(\"Building item-based model...\")\n",
        "    R, Rc, user_idx, item_idx, user_rev, item_rev, user_means, item_similarity = build_item_model(\n",
        "        df_train, mean_center=MEAN_CENTER, max_users=MAX_USERS, max_items=MAX_ITEMS\n",
        "    )\n",
        "    \n",
        "    # Prepare base artifacts for evaluation\n",
        "    base_artifacts = {\n",
        "        'R': R, 'Rc': Rc,\n",
        "        'user_means': user_means,\n",
        "        'user_rev': user_rev,\n",
        "        'item_rev': item_rev,\n",
        "        'user_idx': user_idx,\n",
        "        'item_idx': item_idx,\n",
        "        'item_similarity': item_similarity\n",
        "    }\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    # Test each K value\n",
        "    for i, k in enumerate(k_values, 1):\n",
        "        logger.log_info(f\"\\n[{i}/{len(k_values)}] Testing K={k}\")\n",
        "        logger.log_info(\"-\"*70)\n",
        "        \n",
        "        # Create artifacts for this K\n",
        "        eval_artifacts = base_artifacts.copy()\n",
        "        eval_artifacts['top_k_similar'] = k\n",
        "        \n",
        "        # Evaluate on validation\n",
        "        start_time = time.time()\n",
        "        metrics = evaluate_item_based(\n",
        "            category, eval_artifacts,\n",
        "            k_values=[10, 20, 50],\n",
        "            split=\"valid\",\n",
        "            sample_users=n_eval_users\n",
        "        )\n",
        "        eval_time = time.time() - start_time\n",
        "        \n",
        "        if metrics:\n",
        "            # Store all results\n",
        "            result = {\n",
        "                'K': k,\n",
        "                'NDCG@10': metrics['ndcg@10'],\n",
        "                'NDCG@20': metrics['ndcg@20'],\n",
        "                'NDCG@50': metrics['ndcg@50'],\n",
        "                'Recall@10': metrics['recall@10'],\n",
        "                'Recall@20': metrics['recall@20'],\n",
        "                'Recall@50': metrics['recall@50'],\n",
        "                'MAP@10': metrics['map@10'],\n",
        "                'MAP@20': metrics['map@20'],\n",
        "                'MAP@50': metrics['map@50'],\n",
        "                'RMSE': metrics['rmse'],\n",
        "                'Accuracy': metrics['accuracy'],\n",
        "                'Eval_Time': eval_time\n",
        "            }\n",
        "            results.append(result)\n",
        "            \n",
        "            # Print summary\n",
        "            logger.log_info(f\"Results:\")\n",
        "            logger.log_info(f\"  NDCG@10:   {metrics['ndcg@10']:.4f}\")\n",
        "            logger.log_info(f\"  Recall@10: {metrics['recall@10']:.4f}\")\n",
        "            logger.log_info(f\"  MAP@10:    {metrics['map@10']:.4f}\")\n",
        "            logger.log_info(f\"  RMSE:      {metrics['rmse']:.4f}\")\n",
        "            logger.log_info(f\"  Eval:      {eval_time:.1f}s\")\n",
        "    \n",
        "    df_results = pl.DataFrame(results)\n",
        "    \n",
        "    # Save results\n",
        "    out_csv = MODELS_DIR / 'item' / f'tuning_{category}.csv'\n",
        "    df_results.write_csv(out_csv)\n",
        "    logger.log_info(f\"\\nSaved tuning results: {out_csv}\")\n",
        "    \n",
        "    return df_results\n",
        "\n",
        "\n",
        "def select_best_k(df_results: pl.DataFrame):\n",
        "    \"\"\"\n",
        "    Select best K using NDCG-primary strategy.\n",
        "    \"\"\"\n",
        "    # Primary: NDCG@10\n",
        "    best_k_ndcg = df_results['K'][df_results['NDCG@10'].arg_max()]\n",
        "    best_ndcg = df_results['NDCG@10'].max()\n",
        "    \n",
        "    logger.log_info(f\"\\nPrimary metric (NDCG@10): K={best_k_ndcg}, score={best_ndcg:.4f}\")\n",
        "    \n",
        "    # Find K values within 2% of best NDCG\n",
        "    threshold = best_ndcg * 0.98\n",
        "    similar_rows = df_results.filter(pl.col('NDCG@10') >= threshold)\n",
        "    similar_k = similar_rows['K'].to_list()\n",
        "    \n",
        "    if len(similar_k) > 1:\n",
        "        logger.log_info(f\"Multiple K with similar NDCG (within 2%): {similar_k}\")\n",
        "        \n",
        "        # Secondary: Recall@10\n",
        "        best_k = similar_rows['K'][similar_rows['Recall@10'].arg_max()]\n",
        "        best_recall = similar_rows['Recall@10'].max()\n",
        "        \n",
        "        logger.log_info(f\"Secondary metric (Recall@10): K={best_k}, score={best_recall:.4f}\")\n",
        "    else:\n",
        "        best_k = best_k_ndcg\n",
        "        logger.log_info(f\"Clear winner based on NDCG@10: K={best_k}\")\n",
        "    \n",
        "    return best_k\n",
        "\n",
        "\n",
        "def analyze_and_select_k(df_results: pl.DataFrame, category: str):\n",
        "    \"\"\"Analyze results and select best K\"\"\"\n",
        "    logger.log_info(f\"\\n{'='*70}\")\n",
        "    logger.log_info(\"K SELECTION ANALYSIS\")\n",
        "    logger.log_info(f\"{'='*70}\")\n",
        "    \n",
        "    # Best K for each metric\n",
        "    best_k_ndcg = df_results['K'][df_results['NDCG@10'].arg_max()]\n",
        "    best_k_recall = df_results['K'][df_results['Recall@10'].arg_max()]\n",
        "    best_k_map = df_results['K'][df_results['MAP@10'].arg_max()]\n",
        "    best_k_rmse = df_results['K'][df_results['RMSE'].arg_min()]\n",
        "    \n",
        "    logger.log_info(f\"\\nBest K by metric:\")\n",
        "    logger.log_info(f\"  NDCG@10:   K={best_k_ndcg:3d} (score={df_results['NDCG@10'].max():.4f})\")\n",
        "    logger.log_info(f\"  Recall@10: K={best_k_recall:3d} (score={df_results['Recall@10'].max():.4f})\")\n",
        "    logger.log_info(f\"  MAP@10:    K={best_k_map:3d} (score={df_results['MAP@10'].max():.4f})\")\n",
        "    logger.log_info(f\"  RMSE:      K={best_k_rmse:3d} (score={df_results['RMSE'].min():.4f})\")\n",
        "    \n",
        "    # Apply NDCG-primary strategy\n",
        "    best_k = select_best_k(df_results)\n",
        "    \n",
        "    # Show final selection details\n",
        "    final_row = df_results.filter(pl.col('K') == best_k).row(0, named=True)\n",
        "    \n",
        "    logger.log_info(f\"\\n{'='*70}\")\n",
        "    logger.log_info(f\"FINAL SELECTION: K={best_k}\")\n",
        "    logger.log_info(f\"{'='*70}\")\n",
        "    logger.log_info(f\"  NDCG@10:   {final_row['NDCG@10']:.4f}\")\n",
        "    logger.log_info(f\"  Recall@10: {final_row['Recall@10']:.4f}\")\n",
        "    logger.log_info(f\"  MAP@10:    {final_row['MAP@10']:.4f}\")\n",
        "    logger.log_info(f\"  RMSE:      {final_row['RMSE']:.4f}\")\n",
        "    logger.log_info(f\"  Accuracy:  {final_row['Accuracy']:.4f}\")\n",
        "    logger.log_info(f\"  Eval:      {final_row['Eval_Time']:.1f}s\")\n",
        "    logger.log_info(f\"{'='*70}\\n\")\n",
        "    \n",
        "    # Update configuration - Sá»¬ Dá»¤NG SEPARATE CONFIG CHO ITEM-BASED\n",
        "    Configurations.save_best_k_item(category, best_k)\n",
        "    logger.log_info(f\"Saved best K to file\\n\")\n",
        "    \n",
        "    return best_k, df_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task: Pipeline and execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _train_single_category(cat, model_dir, K_VALUES, n_eval_tune):\n",
        "    \"\"\"Helper: Train and tune a single category for item-based CF\"\"\"\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 1: BUILD BASE MODEL (if not exists)\n",
        "    # ========================================================================\n",
        "    \n",
        "    if not (model_dir / \"R.npz\").exists():\n",
        "        logger.log_info(\"STEP 1: TRAINING BASE MODEL\")\n",
        "        logger.log_info(\"-\"*70)\n",
        "        logger.log_info(\"Building item-item similarity matrix (done once)\\n\")\n",
        "        \n",
        "        # Load training data\n",
        "        df_train = load_5core_data(cat, split=\"train\")\n",
        "        \n",
        "        # Build item-based model\n",
        "        logger.log_info(\"Computing item-item similarities...\")\n",
        "        R, Rc, user_idx, item_idx, user_rev, item_rev, user_means, item_similarity = build_item_model(\n",
        "            df_train, mean_center=MEAN_CENTER, max_users=MAX_USERS, max_items=MAX_ITEMS\n",
        "        )\n",
        "        \n",
        "        # Save artifacts\n",
        "        save_item_artifacts(model_dir, R, Rc, user_rev, item_rev,\n",
        "                          user_idx, item_idx, user_means, item_similarity)\n",
        "        \n",
        "        logger.log_info(f\"Base model saved to {model_dir}\\n\")\n",
        "    \n",
        "    else:\n",
        "        logger.log_info(\"STEP 1: BASE MODEL EXISTS\")\n",
        "        logger.log_info(\"-\"*70)\n",
        "        logger.log_info(f\"Loading from {model_dir}\\n\")\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 2: HYPERPARAMETER TUNING (if not done)\n",
        "    # ========================================================================\n",
        "    \n",
        "    if not Configurations.has_tuning_results_item(cat):\n",
        "        logger.log_info(\"STEP 2: HYPERPARAMETER TUNING (VALIDATION)\")\n",
        "        logger.log_info(\"-\"*70)\n",
        "        logger.log_info(f\"K values: {K_VALUES}\")\n",
        "        logger.log_info(f\"Validation users: {n_eval_tune}\\n\")\n",
        "        \n",
        "        # Load base artifacts (similarity matrix already computed!)\n",
        "        artifacts = load_item_artifacts(model_dir)\n",
        "        \n",
        "        results = []\n",
        "        \n",
        "        # Test each K value\n",
        "        for i, k in enumerate(K_VALUES, 1):\n",
        "            logger.log_info(f\"\\n[{i}/{len(K_VALUES)}] Testing K={k}\")\n",
        "            logger.log_info(\"-\"*70)\n",
        "            \n",
        "            # Create artifacts for this K\n",
        "            eval_artifacts = artifacts.copy()\n",
        "            eval_artifacts['top_k_similar'] = k\n",
        "            \n",
        "            # Evaluate on validation\n",
        "            start_time = time.time()\n",
        "            metrics = evaluate_item_based(\n",
        "                cat, eval_artifacts,\n",
        "                k_values=[10, 20, 50],\n",
        "                split=\"valid\",\n",
        "                sample_users=n_eval_tune\n",
        "            )\n",
        "            eval_time = time.time() - start_time\n",
        "            \n",
        "            if metrics:\n",
        "                results.append({\n",
        "                    'K': k,\n",
        "                'NDCG@10': metrics['ndcg@10'],\n",
        "                'NDCG@20': metrics['ndcg@20'],\n",
        "                'NDCG@50': metrics['ndcg@50'],\n",
        "                'Recall@10': metrics['recall@10'],\n",
        "                'Recall@20': metrics['recall@20'],\n",
        "                'Recall@50': metrics['recall@50'],\n",
        "                'MAP@10': metrics['map@10'],\n",
        "                'MAP@20': metrics['map@20'],\n",
        "                'MAP@50': metrics['map@50'],\n",
        "                'RMSE': metrics['rmse'],\n",
        "                'Accuracy': metrics['accuracy'],\n",
        "                'Eval_Time': eval_time\n",
        "                })\n",
        "                \n",
        "                logger.log_info(f\"Results:\")\n",
        "                logger.log_info(f\"  NDCG@10:   {metrics['ndcg@10']:.4f}\")\n",
        "                logger.log_info(f\"  Recall@10: {metrics['recall@10']:.4f}\")\n",
        "                logger.log_info(f\"  Eval:      {eval_time:.1f}s\")\n",
        "        \n",
        "        # Save tuning results\n",
        "        df_results = pl.DataFrame(results)\n",
        "        out_csv = MODELS_DIR / 'item' / f'tuning_{cat}.csv'\n",
        "        df_results.write_csv(out_csv)\n",
        "        logger.log_info(f\"\\nSaved tuning results: {out_csv}\")\n",
        "        \n",
        "        # Select best K\n",
        "        best_k = select_best_k(df_results)\n",
        "        \n",
        "        # Log selection\n",
        "        final_row = df_results.filter(pl.col('K') == best_k).row(0, named=True)\n",
        "        \n",
        "        logger.log_info(f\"\\n{'='*70}\")\n",
        "        logger.log_info(f\"BEST K SELECTED: {best_k}\")\n",
        "        logger.log_info(f\"{'='*70}\")\n",
        "        logger.log_info(f\"  NDCG@10:   {final_row['NDCG@10']:.4f}\")\n",
        "        logger.log_info(f\"  Recall@10: {final_row['Recall@10']:.4f}\")\n",
        "        logger.log_info(f\"  MAP@10:    {final_row['MAP@10']:.4f}\")\n",
        "        logger.log_info(f\"{'='*70}\\n\")\n",
        "        \n",
        "        # Save best K\n",
        "        Configurations.save_best_k_item(cat, best_k)\n",
        "        logger.log_info(f\"Saved best K to file\\n\")\n",
        "        \n",
        "        # Visualize tuning\n",
        "        logger.log_info(\"Generating K tuning visualization...\")\n",
        "        visualize_hyperparameter_tuning(\n",
        "                                        df_results,\n",
        "                                        category=cat,\n",
        "                                        param_col='K',\n",
        "                                        param_name='K (Similar Items - Co-rating)',\n",
        "                                        save_dir=MODELS_DIR / 'item',\n",
        "                                        algo_name='Item-Based'\n",
        "                                    )\n",
        "        logger.log_info(f\"Saved: k_tuning_{cat}.png\\n\")\n",
        "        \n",
        "        return {'tuned_now': True, 'best_k': best_k}\n",
        "    \n",
        "    else:\n",
        "        best_k = Configurations.load_best_k_item(cat)\n",
        "        logger.log_info(\"STEP 2: TUNING ALREADY DONE\")\n",
        "        logger.log_info(\"-\"*70)\n",
        "        logger.log_info(f\"Best K (loaded): {best_k}\\n\")\n",
        "        \n",
        "        return {'tuned_now': False, 'best_k': best_k}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Phase 1: Training + Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PHASE 1: TRAINING + TUNING ALL CATEGORIES\n",
        "# ============================================================================\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"PHASE 1: TRAINING + TUNING ALL CATEGORIES (ITEM-BASED)\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Configuration\n",
        "if not Configurations.has_tuning_results_item(CATEGORY[0]):\n",
        "    K_VALUES = Configurations.K_VALUES_ITEM\n",
        "    logger.log_info(f\"K values to test: {K_VALUES}\\n\")\n",
        "\n",
        "workflow_results = {}\n",
        "\n",
        "for cat in CATEGORY:\n",
        "    logger.log_info(f\"\\n{'='*70}\\nCATEGORY: {cat}\\n{'='*70}\\n\")\n",
        "    \n",
        "    model_dir = MODELS_DIR / \"item\" / cat\n",
        "    workflow_results[cat] = _train_single_category(\n",
        "        cat, model_dir, K_VALUES, Configurations.get_eval_samples_tuning()\n",
        "    )\n",
        "\n",
        "# ============================================================================\n",
        "# PHASE 1 SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"PHASE 1 COMPLETE: ALL MODELS TRAINED AND TUNED\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "logger.log_info(\"Tuning Summary:\")\n",
        "for cat in CATEGORY:\n",
        "    status = 'newly tuned' if workflow_results[cat]['tuned_now'] else 'loaded from cache'\n",
        "    logger.log_info(f\"  {cat}: K={workflow_results[cat]['best_k']} ({status})\")\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"Ready for Phase 2: Final Evaluation\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Phase 2: Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PHASE 2: FINAL EVALUATION ON TEST SET (ALL CATEGORIES)\n",
        "# ============================================================================\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"PHASE 2: FINAL EVALUATION ON TEST SET (ITEM-BASED)\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD WORKFLOW RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "if 'workflow_results' not in locals():\n",
        "    workflow_results = {}\n",
        "    for cat in CATEGORY:\n",
        "        best_k = Configurations.load_best_k_item(cat)\n",
        "        workflow_results[cat] = {'best_k': best_k, 'tuned_now': False}\n",
        "    logger.log_info(\"Loaded best K values from configuration\\n\")\n",
        "\n",
        "n_eval_final = Configurations.get_eval_samples_final()\n",
        "logger.log_info(f\"Test users per category: {n_eval_final}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# RUN TEST EVALUATION FOR ALL CATEGORIES\n",
        "# ============================================================================\n",
        "\n",
        "for cat in CATEGORY:\n",
        "    logger.log_info(f\"\\n{'='*70}\\nTESTING: {cat}\\n{'='*70}\\n\")\n",
        "    \n",
        "    model_dir = MODELS_DIR / \"item\" / cat\n",
        "    best_k = workflow_results[cat]['best_k']\n",
        "    logger.log_info(f\"Using K: {best_k}\")\n",
        "    \n",
        "    # Load model and evaluate\n",
        "    final_artifacts = load_item_artifacts(model_dir)\n",
        "    final_artifacts['top_k_similar'] = best_k\n",
        "    \n",
        "    logger.log_info(\"Evaluating on test set...\\n\")\n",
        "    results = evaluate_item_based(cat, final_artifacts, k_values=[10, 20, 50],\n",
        "                                 split=\"test\", sample_users=n_eval_final)\n",
        "    \n",
        "    if results:\n",
        "        workflow_results[cat]['test_results'] = results\n",
        "        logger.log_info(f\"\\nTest Results (K={best_k}):\")\n",
        "        logger.log_info(f\"  NDCG@10: {results['ndcg@10']:.4f}, \"\n",
        "                       f\"Recall@10: {results['recall@10']:.4f}, \"\n",
        "                       f\"MAP@10: {results['map@10']:.4f}\")\n",
        "        logger.log_info(f\"  RMSE: {results['rmse']:.4f}, \"\n",
        "                       f\"Accuracy: {results['accuracy']:.4f}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# SAVE FINAL RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"SAVING FINAL RESULTS\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "test_results_list = [workflow_results[cat]['test_results'] \n",
        "                     for cat in CATEGORY \n",
        "                     if 'test_results' in workflow_results[cat]]\n",
        "\n",
        "if test_results_list:\n",
        "    df_final_results = pl.DataFrame(test_results_list)\n",
        "    \n",
        "    logger.log_info(\"Final Test Results:\")\n",
        "    display(df_final_results)\n",
        "    \n",
        "    out_csv = MODELS_DIR / 'item' / 'final_test_results.csv'\n",
        "    df_final_results.write_csv(out_csv)\n",
        "    logger.log_info(f\"\\nSaved: {out_csv}\")\n",
        "    \n",
        "    logger.log_info(\"Generating final evaluation plot...\")\n",
        "    visualize_final_results(\n",
        "                            test_results_list,\n",
        "                            save_dir=MODELS_DIR / 'item',\n",
        "                            algo_name='Item-Based',\n",
        "                            k_values=[10, 20, 50]\n",
        "                        )\n",
        "    logger.log_info(f\"Saved: evaluation_results.png\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# POST-ANALYSIS VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"POST-ANALYSIS VISUALIZATION\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "for cat in CATEGORY:\n",
        "    tuning_csv = MODELS_DIR / 'item' / f'tuning_{cat}.csv'\n",
        "    \n",
        "    if not tuning_csv.exists():\n",
        "        logger.log_info(f\"No tuning results for {cat}, skipping\\n\")\n",
        "        continue\n",
        "    \n",
        "    if 'test_results' not in workflow_results[cat]:\n",
        "        continue\n",
        "    \n",
        "    logger.log_info(f\"Generating Val vs Test comparison for {cat}...\")\n",
        "    \n",
        "    df_tuning = pl.read_csv(tuning_csv)\n",
        "    best_k = workflow_results[cat]['best_k']\n",
        "    \n",
        "    tuning_row = df_tuning.filter(pl.col('K') == best_k).row(0, named=True)\n",
        "    final_row = df_final_results.filter(pl.col('category') == cat).row(0, named=True)\n",
        "    \n",
        "    visualize_val_test_comparison(\n",
        "                                    cat=cat,\n",
        "                                    param_val=best_k,\n",
        "                                    tuning_row=tuning_row,\n",
        "                                    final_row=final_row,\n",
        "                                    save_dir=MODELS_DIR / 'item',\n",
        "                                    param_name='K',\n",
        "                                    algo_name='Item-Based'\n",
        "                                )\n",
        "    logger.log_info(f\"  Saved: val_vs_test_{cat}.png\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL WORKFLOW SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"COMPLETE WORKFLOW SUMMARY (ITEM-BASED)\")\n",
        "logger.log_info(\"=\"*70)\n",
        "\n",
        "for cat in CATEGORY:\n",
        "    logger.log_info(f\"\\n{cat}: K={workflow_results[cat]['best_k']}\")\n",
        "    if 'test_results' in workflow_results[cat]:\n",
        "        test = workflow_results[cat]['test_results']\n",
        "        logger.log_info(f\"  NDCG@10: {test['ndcg@10']:.4f}, \"\n",
        "                       f\"Recall@10: {test['recall@10']:.4f}, \"\n",
        "                       f\"MAP@10: {test['map@10']:.4f}\")\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"ALL PHASES COMPLETE\")\n",
        "logger.log_info(\"=\"*70)\n",
        "logger.log_info(\"\\nGenerated files:\")\n",
        "logger.log_info(\"  Phase 1: tuning_[category].csv, k_tuning_[category].png\")\n",
        "logger.log_info(\"  Phase 2: final_test_results.csv, evaluation_results.png, val_vs_test_[category].png\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Debug info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_data_quality(category: str):\n",
        "    \"\"\"Check train/valid/test overlap, quality, and sparsity metrics\"\"\"\n",
        "    logger.log_info(f\"\\n{'='*70}\")\n",
        "    logger.log_info(f\"DATA QUALITY CHECK (ITEM-BASED): {category}\")\n",
        "    logger.log_info(f\"{'='*70}\\n\")\n",
        "    \n",
        "    # Load all splits\n",
        "    df_train = load_5core_data(category, split='train')\n",
        "    df_valid = load_5core_data(category, split='valid')\n",
        "    df_test = load_5core_data(category, split='test')\n",
        "    \n",
        "    def get_stats(df, name):\n",
        "        n_ratings = len(df)\n",
        "        n_users = df['user_id'].n_unique()\n",
        "        n_items = df['parent_asin'].n_unique()\n",
        "        \n",
        "        sparsity = 1 - (n_ratings / (n_users * n_items))\n",
        "        avg_items_per_user = n_ratings / n_users\n",
        "        avg_users_per_item = n_ratings / n_items\n",
        "        \n",
        "        logger.log_info(f\"{name}:\")\n",
        "        logger.log_info(f\"  Size:           {n_ratings:7,} ratings\")\n",
        "        logger.log_info(f\"  Users:          {n_users:7,}\")\n",
        "        logger.log_info(f\"  Items:          {n_items:7,}\")\n",
        "        logger.log_info(f\"  Sparsity:       {sparsity:7.2%}\")\n",
        "        logger.log_info(f\"  Avg items/user: {avg_items_per_user:7.2f}\")\n",
        "        logger.log_info(f\"  Avg users/item: {avg_users_per_item:7.2f}\")\n",
        "        logger.log_info(\"\")\n",
        "        \n",
        "        return {\n",
        "            'n_ratings': n_ratings, 'n_users': n_users, 'n_items': n_items,\n",
        "            'sparsity': sparsity, 'avg_items_per_user': avg_items_per_user,\n",
        "            'avg_users_per_item': avg_users_per_item\n",
        "        }\n",
        "    \n",
        "    logger.log_info(\"Dataset Statistics:\")\n",
        "    logger.log_info(\"-\" * 70)\n",
        "    train_stats = get_stats(df_train, \"TRAIN\")\n",
        "    valid_stats = get_stats(df_valid, \"VALID\")\n",
        "    test_stats = get_stats(df_test, \"TEST\")\n",
        "    \n",
        "    # Check overlaps\n",
        "    train_users = set(df_train['user_id'].unique())\n",
        "    valid_users = set(df_valid['user_id'].unique())\n",
        "    test_users = set(df_test['user_id'].unique())\n",
        "    \n",
        "    logger.log_info(\"-\" * 70)\n",
        "    logger.log_info(f\"User Overlap:\")\n",
        "    logger.log_info(f\"  Train âˆ© Valid: {len(train_users & valid_users):6,} ({len(train_users & valid_users)/len(valid_users)*100:5.1f}% of valid)\")\n",
        "    logger.log_info(f\"  Train âˆ© Test:  {len(train_users & test_users):6,} ({len(train_users & test_users)/len(test_users)*100:5.1f}% of test)\")\n",
        "    \n",
        "    train_items = set(df_train['parent_asin'].unique())\n",
        "    valid_items = set(df_valid['parent_asin'].unique())\n",
        "    test_items = set(df_test['parent_asin'].unique())\n",
        "    \n",
        "    logger.log_info(f\"\\nItem Overlap:\")\n",
        "    logger.log_info(f\"  Train âˆ© Valid: {len(train_items & valid_items):6,} ({len(train_items & valid_items)/len(valid_items)*100:5.1f}% of valid)\")\n",
        "    logger.log_info(f\"  Train âˆ© Test:  {len(train_items & test_items):6,} ({len(train_items & test_items)/len(test_items)*100:5.1f}% of test)\")\n",
        "    \n",
        "    # Load model\n",
        "    model_dir = MODELS_DIR / 'item' / category\n",
        "    if model_dir.exists():\n",
        "        logger.log_info(f\"\\n{'-' * 70}\")\n",
        "        logger.log_info(\"Model Statistics:\")\n",
        "        \n",
        "        artifacts = load_item_artifacts(model_dir)\n",
        "        user_idx = artifacts['user_idx']\n",
        "        item_idx = artifacts['item_idx']\n",
        "        item_similarity = artifacts['item_similarity']\n",
        "        \n",
        "        logger.log_info(f\"  Model users:       {len(user_idx):7,}\")\n",
        "        logger.log_info(f\"  Model items:       {len(item_idx):7,}\")\n",
        "        logger.log_info(f\"  Similarity matrix: {item_similarity.shape}\")\n",
        "        logger.log_info(f\"  Similarity nnz:    {item_similarity.nnz:7,}\")\n",
        "        logger.log_info(f\"  Similarity sparsity: {(1 - item_similarity.nnz/(item_similarity.shape[0]*item_similarity.shape[1])):7.2%}\")\n",
        "        logger.log_info(f\"  Model sparsity:    {train_stats['sparsity']:7.2%}\")\n",
        "        logger.log_info(f\"  Avg items/user:    {train_stats['avg_items_per_user']:7.2f}\")\n",
        "        logger.log_info(f\"  Avg users/item:    {train_stats['avg_users_per_item']:7.2f}\")\n",
        "    \n",
        "    logger.log_info(f\"\\n{'='*70}\\n\")\n",
        "\n",
        "# Run check\n",
        "check_data_quality(CATEGORY[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Debug functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def debug_item_based_predictions(category: str):\n",
        "    logger.log_info(f\"\\n{'='*70}\")\n",
        "    logger.log_info(f\"DEBUG ITEM-BASED: {category}\")\n",
        "    logger.log_info(f\"{'='*70}\\n\")\n",
        "    \n",
        "    model_dir = MODELS_DIR / 'item' / category\n",
        "    artifacts = load_item_artifacts(model_dir)\n",
        "    \n",
        "    R = artifacts['R']\n",
        "    Rc = artifacts['Rc']\n",
        "    item_similarity = artifacts['item_similarity']\n",
        "    user_idx = artifacts['user_idx']\n",
        "    item_idx = artifacts['item_idx']\n",
        "    user_means = artifacts['user_means']\n",
        "    \n",
        "    # Check 1: Similarity matrix\n",
        "    logger.log_info(\"1. Item Similarity Matrix:\")\n",
        "    logger.log_info(f\"   Shape: {item_similarity.shape}\")\n",
        "    logger.log_info(f\"   nnz: {item_similarity.nnz:,}\")\n",
        "    logger.log_info(f\"   Sparsity: {(1 - item_similarity.nnz/(item_similarity.shape[0]*item_similarity.shape[1])):7.2%}\")\n",
        "    logger.log_info(f\"   Min: {item_similarity.data.min():.6f}\")\n",
        "    logger.log_info(f\"   Max: {item_similarity.data.max():.6f}\")\n",
        "    logger.log_info(f\"   Mean: {item_similarity.data.mean():.6f}\\n\")\n",
        "    \n",
        "    # Check 2: Test prediction for one user\n",
        "    test_user_id = list(user_idx.keys())[0]\n",
        "    u = user_idx[test_user_id]\n",
        "    \n",
        "    logger.log_info(f\"2. Test User: {test_user_id}\")\n",
        "    logger.log_info(f\"   User index: {u}\")\n",
        "    \n",
        "    # Get rated items\n",
        "    rated_items = R.getrow(u).indices\n",
        "    rated_values = R.getrow(u).data\n",
        "    \n",
        "    logger.log_info(f\"   Rated items: {len(rated_items)}\")\n",
        "    logger.log_info(f\"   Rating range: [{rated_values.min():.1f}, {rated_values.max():.1f}]\")\n",
        "    logger.log_info(f\"   Mean rating: {rated_values.mean():.2f}\\n\")\n",
        "    \n",
        "    # Check 3: Predict scores\n",
        "    logger.log_info(\"3. Prediction Test:\")\n",
        "    scores = predict_item_based(u, R, Rc, item_similarity, user_means, top_k=30)\n",
        "    \n",
        "    logger.log_info(f\"   Scores computed: {len(scores)}\")\n",
        "    logger.log_info(f\"   Non-zero scores: {np.count_nonzero(scores)}\")\n",
        "    logger.log_info(f\"   Score range: [{scores.min():.6f}, {scores.max():.6f}]\")\n",
        "    logger.log_info(f\"   Score mean: {scores.mean():.6f}\")\n",
        "    logger.log_info(f\"   Score std: {scores.std():.6f}\\n\")\n",
        "    \n",
        "    # Check 4: Top recommendations\n",
        "    logger.log_info(\"4. Top 10 Predictions:\")\n",
        "    top_10_idx = np.argsort(-scores)[:10]\n",
        "    for rank, idx in enumerate(top_10_idx, 1):\n",
        "        item_id = artifacts['item_rev'][idx]\n",
        "        score = scores[idx]\n",
        "        logger.log_info(f\"   {rank}. Item {item_id}: score={score:.6f}\")\n",
        "    \n",
        "    logger.log_info(f\"\\n{'='*70}\\n\")\n",
        "\n",
        "# RUN DEBUG\n",
        "debug_item_based_predictions(CATEGORY[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task: Unit test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recommend_item_ui(user_id: str, n_recs: int = 5, models_dir: str | Path | None = None, category: str | None = None) -> pl.DataFrame:\n",
        "    cat = category or CATEGORY[0]\n",
        "    model_dir = Path(models_dir) if models_dir else (MODELS_DIR / \"item\" / cat)\n",
        "    artifacts = load_item_artifacts(model_dir)\n",
        "    \n",
        "    # Get best K from configuration\n",
        "    best_k = Configurations.load_best_k_item(cat)\n",
        "    artifacts['top_k_similar'] = best_k\n",
        "    \n",
        "    return recommend_item_based(user_id, n_recs, artifacts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test All Categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_all_categories():\n",
        "    \"\"\"Unit test: Verify recommendation function works for all categories\"\"\"\n",
        "    logger.log_info(\"\\n\" + \"=\"*70)\n",
        "    logger.log_info(\"[UNIT TEST] Testing Recommendation Function\")\n",
        "    logger.log_info(\"=\"*70 + \"\\n\")\n",
        "    \n",
        "    test_summary = []\n",
        "    \n",
        "    for cat in CATEGORY:\n",
        "        logger.log_info(f\"\\n[Test] {cat}\")\n",
        "        logger.log_info(\"-\"*70)\n",
        "        \n",
        "        try:\n",
        "            model_dir = MODELS_DIR / \"item\" / cat\n",
        "            \n",
        "            # Check model exists\n",
        "            if not model_dir.exists():\n",
        "                logger.log_warning(f\"  âœ— Model not found\")\n",
        "                test_summary.append({'category': cat, 'status': 'FAIL', 'reason': 'Model not found'})\n",
        "                continue\n",
        "            \n",
        "            # Load artifacts\n",
        "            artifacts = load_item_artifacts(model_dir)\n",
        "            best_k = Configurations.load_best_k_item(cat)\n",
        "            artifacts['top_k_similar'] = best_k\n",
        "            \n",
        "            user_rev = artifacts['user_rev']\n",
        "            item_rev = artifacts['item_rev']\n",
        "            \n",
        "            logger.log_info(f\"  Model loaded: {len(user_rev):,} users, {len(item_rev):,} items\")\n",
        "            logger.log_info(f\"  Using top_k_similar: {best_k}\")\n",
        "            \n",
        "            if len(user_rev) == 0:\n",
        "                logger.log_warning(f\"  âœ— No users in model\")\n",
        "                test_summary.append({'category': cat, 'status': 'FAIL', 'reason': 'No users'})\n",
        "                continue\n",
        "            \n",
        "            # Test recommendation\n",
        "            sample_user = user_rev[0]\n",
        "            logger.log_info(f\"  Testing user: {sample_user}\")\n",
        "            \n",
        "            recs = recommend_item_ui(sample_user, n_recs=N_RECS, category=cat)\n",
        "            \n",
        "            # Validate output\n",
        "            assert set(recs.columns) >= {\"parent_asin\", \"score\"}, \"Missing required columns\"\n",
        "            assert len(recs) <= N_RECS, f\"Too many recommendations: {len(recs)}\"\n",
        "            assert all(recs['score'] >= 0), \"Negative scores detected\"\n",
        "            \n",
        "            logger.log_info(f\"  Generated {len(recs)} recommendations\")\n",
        "            logger.log_info(f\"  Score range: [{recs['score'].min():.4f}, {recs['score'].max():.4f}]\")\n",
        "            \n",
        "            test_summary.append({\n",
        "                'category': cat, \n",
        "                'status': 'PASS', \n",
        "                'n_recs': len(recs),\n",
        "                'score_min': float(recs['score'].min()),\n",
        "                'score_max': float(recs['score'].max())\n",
        "            })\n",
        "            \n",
        "            # Display sample recommendations\n",
        "            display(recs.head(5))\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.log_exception(f\"  Error: {e}\")\n",
        "            test_summary.append({'category': cat, 'status': 'FAIL', 'reason': str(e)})\n",
        "    \n",
        "    # Summary\n",
        "    logger.log_info(\"\\n\" + \"=\"*70)\n",
        "    logger.log_info(\"UNIT TEST SUMMARY\")\n",
        "    logger.log_info(\"=\"*70)\n",
        "    \n",
        "    df_summary = pl.DataFrame(test_summary)\n",
        "    display(df_summary)\n",
        "    \n",
        "    passed = sum(1 for r in test_summary if r['status'] == 'PASS')\n",
        "    total = len(test_summary)\n",
        "    \n",
        "    logger.log_info(f\"\\nResults: {passed}/{total} categories passed\")\n",
        "    \n",
        "    if passed == total:\n",
        "        logger.log_info(\"ALL TESTS PASSED\")\n",
        "    else:\n",
        "        logger.log_warning(f\"{total - passed} tests failed\")\n",
        "    \n",
        "    logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "test_all_categories()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Content-Based Collaborative Filtering (5-core â€¢ TRAIN)\n",
        "\n",
        "**Goal**\n",
        "- Build a content-based CF recommender on **5-core / TRAIN** for a given category.\n",
        "- Use TF-IDF vectorization on product metadata (title, description, features, categories, details) + cosine similarity.\n",
        "- Produce Top-N recommendations for one user or a batch of users.\n",
        "\n",
        "**What this notebook does**\n",
        "1. Load 5-core **TRAIN** from `PROCESSED_DIR` with schema: `user_id`, `parent_asin`, `rating`, `timestamp`, `history`\n",
        "2. Load **METADATA** from `RAW_DIR` with product information.\n",
        "3. Combine multiple text fields (title 3x, features 2x, description 1x, categories 1x, details 1x) with appropriate weights.\n",
        "4. Build **TF-IDF item-item** similarity matrix based on combined text.\n",
        "5. Predict scores for **unseen items** based on user's rated items and generate **Top-N** recommendations.\n",
        "6. Evaluate using TEST/VALID sets: Accuracy, RMSE, Recall@K, NDCG@K, MAP@K.\n",
        "7. (Optional) Save recommendations to disk for UI integration.\n",
        "\n",
        "> Notes:\n",
        "> - Metadata columns: ['main_category', 'title', 'average_rating', 'rating_number', 'features', 'description', 'price', 'images', 'videos', 'store', 'categories', 'details', 'parent_asin', 'bought_together']\n",
        "> - Description is LIST, Features is LIST, Categories is FLAT LIST, Details is DICT\n",
        "> - Item-item similarity computed via cosine similarity on TF-IDF vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task: Import modules and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, json, pickle, re, time\n",
        "import numpy as np, polars as pl\n",
        "from pathlib import Path\n",
        "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords if needed\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "module_path = os.path.abspath(os.path.join('..', '../utilities'))\n",
        "sys.path.append(module_path)\n",
        "\n",
        "from logger import Logger\n",
        "from configurations import Configurations\n",
        "from visualization_helpers import (\n",
        "    visualize_hyperparameter_tuning,\n",
        "    visualize_final_results,\n",
        "    visualize_val_test_comparison\n",
        ")\n",
        "from evaluation_metrics import (\n",
        "       compute_rmse_accuracy,\n",
        "       recall_at_k,\n",
        "       ndcg_at_k,\n",
        "       map_at_k\n",
        ")\n",
        "\n",
        "logger = Logger(process_name=\"content_based\", log_file=Configurations.LOG_PATH)\n",
        "\n",
        "PROCESSED_DIR = Path(Configurations.DATA_PROCESSED_PATH)\n",
        "RAW_DIR = Path(Configurations.DATA_RAW_PATH)\n",
        "MODELS_DIR = Path(Configurations.MODELS_PATH)\n",
        "\n",
        "CATEGORY = Configurations.CATEGORIES\n",
        "\n",
        "# Auto-detect phase\n",
        "has_tuning = Configurations.has_tuning_results_content(CATEGORY[0])\n",
        "K_VALUES = Configurations.K_VALUES_CONTENT\n",
        "\n",
        "if has_tuning:\n",
        "    PHASE = 'final'\n",
        "    logger.log_info(\"=\"*70)\n",
        "    logger.log_info(\"PHASE: FINAL EVALUATION (CONTENT-BASED)\")\n",
        "    logger.log_info(\"=\"*70)\n",
        "    \n",
        "    for cat in CATEGORY:\n",
        "        best_k = Configurations.load_best_k_content(cat)\n",
        "        logger.log_info(f\"  {cat}: Best K = {best_k}\")\n",
        "else:\n",
        "    PHASE = 'train_tune'\n",
        "    logger.log_info(\"=\"*70)\n",
        "    logger.log_info(\"PHASE: TRAINING + TUNING (CONTENT-BASED)\")\n",
        "    logger.log_info(\"=\"*70)\n",
        "    logger.log_info(f\"K values: {K_VALUES}\")\n",
        "\n",
        "logger.log_info(\"=\"*70)\n",
        "logger.log_info(f\"Categories: {CATEGORY}\")\n",
        "logger.log_info(f\"Sample size: {Configurations.DEV_SAMPLE_SIZE}\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Settings\n",
        "N_RECS = 10\n",
        "MAX_USERS = None\n",
        "MAX_ITEMS = None\n",
        "TOP_K_SIMILAR = 30  # Default, will be tuned\n",
        "\n",
        "# TF-IDF Settings (can be tuned later)\n",
        "TFIDF_MAX_FEATURES = 5000\n",
        "TFIDF_MIN_DF = 2\n",
        "TFIDF_NGRAM_RANGE = (1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task: Define functions for CF recommendation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _candidate_files(category: str, split: str = \"train\"):\n",
        "    dev_sample_size = Configurations.DEV_SAMPLE_SIZE\n",
        "\n",
        "    if dev_sample_size != 'full':\n",
        "        sample_sizes = Configurations.SAMPLE_SIZES\n",
        "        for size_name in sample_sizes.keys():\n",
        "            if size_name == dev_sample_size:\n",
        "             return PROCESSED_DIR / f\"{category.replace('/', '-')}.5core.{split}.{size_name}.parquet\"\n",
        "    else:\n",
        "        return PROCESSED_DIR / f\"{category.replace('/', '-')}.5core.{split}.parquet\"\n",
        "\n",
        "def _candidate_meta_files(category: str):\n",
        "    safe = category.replace('/', '-')\n",
        "    fname = f\"{safe}.meta.parquet\"\n",
        "    return [PROCESSED_DIR / fname]\n",
        "\n",
        "def load_5core_data(category: str, split: str = \"train\") -> pl.DataFrame:\n",
        "    p = _candidate_files(category, split)\n",
        "    df = pl.read_parquet(p, low_memory=False)\n",
        "    df = df.__copy__()\n",
        "    logger.log_info(f\"[Load-{split.upper()}] shape={df.shape} | users={df['user_id'].n_unique()} | items={df['parent_asin'].n_unique()}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Inspect & Visualize Data Quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inspect_and_visualize_descriptions(df: pl.DataFrame, category: str):\n",
        "    logger.log_info(f\"[Inspect] Analyzing description data for {category}...\")\n",
        "    descriptions = df['description'].to_list()\n",
        "    desc_strings = []\n",
        "    for d in descriptions:\n",
        "        if isinstance(d, list):\n",
        "            desc_strings.append(' '.join(str(item) for item in d if item))\n",
        "        elif d is not None:\n",
        "            desc_strings.append(str(d))\n",
        "        else:\n",
        "            desc_strings.append('')\n",
        "    total_count = len(desc_strings)\n",
        "    none_count = sum(1 for d in descriptions if d is None or d == [])\n",
        "    empty_count = sum(1 for d in desc_strings if d.strip() == '')\n",
        "    valid_count = total_count - none_count - empty_count\n",
        "    valid_descs = [d for d in desc_strings if d.strip() != '']\n",
        "    if valid_descs:\n",
        "        lengths = [len(d) for d in valid_descs]\n",
        "        word_counts = [len(d.split()) for d in valid_descs]\n",
        "        special_char_counts = [len(re.findall(r'[^a-zA-Z0-9\\\\s]', d)) for d in valid_descs]\n",
        "        digit_counts = [len(re.findall(r'\\\\d', d)) for d in valid_descs]\n",
        "        uppercase_ratios = [sum(1 for c in d if c.isupper()) / len(d) if len(d) > 0 else 0 for d in valid_descs]\n",
        "        stats = {'Total': total_count, 'None/Null/Empty List': none_count, 'Empty String': empty_count, 'Valid': valid_count, 'Valid %': f\"{valid_count/total_count*100:.2f}%\", 'Avg Length': f\"{np.mean(lengths):.1f}\", 'Median Length': f\"{np.median(lengths):.1f}\", 'Avg Words': f\"{np.mean(word_counts):.1f}\", 'Avg Special Chars': f\"{np.mean(special_char_counts):.1f}\", 'Avg Digits': f\"{np.mean(digit_counts):.1f}\", 'Avg Uppercase Ratio': f\"{np.mean(uppercase_ratios):.3f}\"}\n",
        "        logger.log_info(f\"[Inspect] Description Statistics:\")\n",
        "        for key, val in stats.items():\n",
        "            logger.log_info(f\"  {key}: {val}\")\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "        fig.suptitle(f'Description Data Quality Analysis - {category}', fontsize=16, fontweight='bold')\n",
        "        ax1 = axes[0, 0]\n",
        "        categories_data = ['Valid', 'None/Null/Empty', 'Empty String']\n",
        "        counts = [valid_count, none_count, empty_count]\n",
        "        colors = ['#2ecc71', '#e74c3c', '#f39c12']\n",
        "        wedges, texts, autotexts = ax1.pie(counts, labels=categories_data, autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "        for autotext in autotexts:\n",
        "            autotext.set_color('white')\n",
        "            autotext.set_fontweight('bold')\n",
        "        ax1.set_title('Data Completeness')\n",
        "        ax2 = axes[0, 1]\n",
        "        ax2.hist(lengths, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "        ax2.axvline(np.mean(lengths), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(lengths):.0f}')\n",
        "        ax2.axvline(np.median(lengths), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(lengths):.0f}')\n",
        "        ax2.set_xlabel('Character Length')\n",
        "        ax2.set_ylabel('Frequency')\n",
        "        ax2.set_title('Description Length Distribution')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        ax3 = axes[0, 2]\n",
        "        ax3.hist(word_counts, bins=50, color='coral', alpha=0.7, edgecolor='black')\n",
        "        ax3.axvline(np.mean(word_counts), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(word_counts):.0f}')\n",
        "        ax3.set_xlabel('Word Count')\n",
        "        ax3.set_ylabel('Frequency')\n",
        "        ax3.set_title('Word Count Distribution')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "        ax4 = axes[1, 0]\n",
        "        ax4.hist(special_char_counts, bins=50, color='purple', alpha=0.7, edgecolor='black')\n",
        "        ax4.axvline(np.mean(special_char_counts), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(special_char_counts):.0f}')\n",
        "        ax4.set_xlabel('Special Character Count')\n",
        "        ax4.set_ylabel('Frequency')\n",
        "        ax4.set_title('Special Characters Distribution')\n",
        "        ax4.legend()\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "        ax5 = axes[1, 1]\n",
        "        ax5.hist(digit_counts, bins=50, color='orange', alpha=0.7, edgecolor='black')\n",
        "        ax5.axvline(np.mean(digit_counts), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(digit_counts):.0f}')\n",
        "        ax5.set_xlabel('Digit Count')\n",
        "        ax5.set_ylabel('Frequency')\n",
        "        ax5.set_title('Digit Distribution')\n",
        "        ax5.legend()\n",
        "        ax5.grid(True, alpha=0.3)\n",
        "        ax6 = axes[1, 2]\n",
        "        ax6.hist(uppercase_ratios, bins=50, color='teal', alpha=0.7, edgecolor='black')\n",
        "        ax6.axvline(np.mean(uppercase_ratios), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(uppercase_ratios):.3f}')\n",
        "        ax6.set_xlabel('Uppercase Ratio')\n",
        "        ax6.set_ylabel('Frequency')\n",
        "        ax6.set_title('Uppercase Character Ratio')\n",
        "        ax6.legend()\n",
        "        ax6.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        out_dir = MODELS_DIR / 'content' / 'inspection'\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "        out_file = out_dir / f'{category.replace(\"/\", \"-\")}_description_analysis.png'\n",
        "        plt.savefig(out_file, dpi=300, bbox_inches='tight')\n",
        "        logger.log_info(f\"[Inspect] Saved visualization to {out_file}\")\n",
        "        plt.close()\n",
        "        logger.log_info(f\"[Inspect] Sample descriptions (first 3):\")\n",
        "        for i, desc in enumerate(valid_descs[:3], 1):\n",
        "            preview = desc[:200] + '...' if len(desc) > 200 else desc\n",
        "            logger.log_info(f\"  Sample {i}: {preview}\")\n",
        "        return stats\n",
        "    else:\n",
        "        logger.log_warning(f\"[Inspect] No valid descriptions found!\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Metadata Loader with Multi-Field Combination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_metadata(category: str) -> pl.DataFrame:\n",
        "    try:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "    except LookupError:\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "    def preprocess_text(text: str) -> str:\n",
        "        if not text or not isinstance(text, str) or text.strip() == '':\n",
        "            return \"\"\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-z0-9\\\\s]', ' ', text)\n",
        "        text = re.sub(r'\\\\s+', ' ', text)\n",
        "        words = text.split()\n",
        "        words = [w for w in words if w not in stop_words and len(w) > 2]\n",
        "        return ' '.join(words).strip()\n",
        "    def combine_text_fields(row: dict) -> str:\n",
        "        title = str(row.get('title', '') or '')\n",
        "        description = row.get('description', [])\n",
        "        features = row.get('features', [])\n",
        "        categories = row.get('categories', [])\n",
        "        # details = row.get('details', {})\n",
        "        if isinstance(description, list):\n",
        "            description_text = ' '.join(str(d) for d in description if d)\n",
        "        else:\n",
        "            description_text = str(description) if description else ''\n",
        "        if isinstance(features, list):\n",
        "            features_text = ' '.join(str(f) for f in features if f)\n",
        "        else:\n",
        "            features_text = str(features) if features else ''\n",
        "        if isinstance(categories, list):\n",
        "            categories_text = ' '.join(str(cat) for cat in categories if cat)\n",
        "        else:\n",
        "            categories_text = str(categories) if categories else ''\n",
        "        # details_text = ''\n",
        "        # if isinstance(details, dict):\n",
        "        #     important_keys = ['Brand', 'Material', 'Color', 'Hair Type', 'Model', 'Type', 'Style', 'Size', 'Manufacturer', 'Fabric Type', 'Pattern', 'Fit Type', 'Sleeve Type', 'Collar Style']\n",
        "        #     details_parts = []\n",
        "        #     for key in important_keys:\n",
        "        #         if key in details and details[key]:\n",
        "        #             details_parts.append(str(details[key]))\n",
        "        #     details_text = ' '.join(details_parts)\n",
        "        combined = f\"{title} {title} {title} \" + f\"{features_text} {features_text} \" + f\"{description_text} \" + f\"{categories_text} \"# + f\"{details_text}\"\n",
        "        return combined.strip()\n",
        "    for p in _candidate_meta_files(category):\n",
        "        if not (p.exists() and p.stat().st_size > 0):\n",
        "            continue\n",
        "        logger.log_info(f\"[Load-META] Reading: {p.name}\")\n",
        "        df = pl.read_parquet(p, low_memory=False)\n",
        "        if 'parent_asin' not in df.columns:\n",
        "            raise ValueError(f\"Missing 'parent_asin' column in {p}\")\n",
        "        logger.log_info(f\"[Load-META] Available columns: {df.columns}\")\n",
        "        text_cols = ['title', 'description', 'features', 'categories']#, 'details']\n",
        "        for col in text_cols:\n",
        "            if col not in df.columns:\n",
        "                logger.log_warning(f\"[Load-META] '{col}' column not found, using empty values\")\n",
        "                if col in ['features', 'categories']:\n",
        "                    df = df.with_columns(pl.lit([]).alias(col))\n",
        "                # elif col == 'details':\n",
        "                #     df = df.with_columns(pl.lit({}).alias(col))\n",
        "                else:\n",
        "                    df = df.with_columns(pl.lit(\"\").alias(col))\n",
        "        logger.log_info(f\"[Load-META] Raw shape={df.shape}\")\n",
        "        logger.log_info(f\"[Load-META] Data structure:\")\n",
        "        logger.log_info(f\"  - title: string\")\n",
        "        logger.log_info(f\"  - description: LIST containing text\")\n",
        "        logger.log_info(f\"  - features: LIST of feature strings\")\n",
        "        logger.log_info(f\"  - categories: FLAT LIST of category strings\")\n",
        "        logger.log_info(f\"  - details: DICT with Brand, Material, Color, etc.\")\n",
        "        if 'description' in df.columns:\n",
        "            inspect_df = df.select(['parent_asin', 'description'])\n",
        "            inspect_and_visualize_descriptions(inspect_df, category)\n",
        "        logger.log_info(f\"[Load-META] Combining text fields with weights:\")\n",
        "        logger.log_info(f\"  - Title: 3x (most important)\")\n",
        "        logger.log_info(f\"  - Features: 2x (technical specs)\")\n",
        "        logger.log_info(f\"  - Description: 1x (detailed info)\")\n",
        "        logger.log_info(f\"  - Categories: 1x (classification)\")\n",
        "        logger.log_info(f\"  - Details: 1x (brand, material, etc.)\")\n",
        "        combined_texts = []\n",
        "        for row in df.iter_rows(named=True):\n",
        "            combined = combine_text_fields(row)\n",
        "            combined_texts.append(combined)\n",
        "        df = df.with_columns(pl.Series(\"combined_text\", combined_texts))\n",
        "        logger.log_info(f\"[Load-META] Preprocessing combined text (lowercase, remove special chars, remove stopwords)...\")\n",
        "        processed_texts = [preprocess_text(text) for text in combined_texts]\n",
        "        df = df.with_columns(pl.Series(\"description\", processed_texts))\n",
        "        original_len = len(df)\n",
        "        df = df.filter(pl.col('description').str.len_chars() > 0)\n",
        "        removed = original_len - len(df)\n",
        "        logger.log_info(f\"[Load-META] Processed shape={df.shape} (removed {removed} empty descriptions)\")\n",
        "        logger.log_info(f\"[Load-META] Preprocessing Impact:\")\n",
        "        logger.log_info(f\"  Before: {original_len} items\")\n",
        "        logger.log_info(f\"  After: {len(df)} items\")\n",
        "        logger.log_info(f\"  Removed: {removed} ({removed/original_len*100:.2f}%)\")\n",
        "        if len(df) > 0:\n",
        "            sample = df['description'].to_list()[0]\n",
        "            logger.log_info(f\"[Load-META] Sample processed text (first 300 chars):\")\n",
        "            logger.log_info(f\"  {sample[:300]}...\")\n",
        "        return df.select(['parent_asin', 'description'])\n",
        "    raise FileNotFoundError(f\"Metadata not found for {category}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Build Content-Based Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_content_model(df_train: pl.DataFrame, df_meta: pl.DataFrame, \n",
        "                       max_users: int | None = None, max_items: int | None = None,\n",
        "                       max_features: int = 5000, min_df: int = 2, \n",
        "                       ngram_range: tuple = (1, 2)):\n",
        "    \"\"\"Build content-based model with TF-IDF on product metadata\"\"\"\n",
        "    \n",
        "    df = df_train.select(['user_id', 'parent_asin', 'rating']).with_columns(\n",
        "        pl.col('rating').cast(pl.Float32)\n",
        "    )\n",
        "    \n",
        "    if max_users is not None:\n",
        "        first_users = df['user_id'].unique()[:max_users].to_list()\n",
        "        df = df.filter(pl.col('user_id').is_in(first_users))\n",
        "    if max_items is not None:\n",
        "        first_items = df['parent_asin'].unique()[:max_items].to_list()\n",
        "        df = df.filter(pl.col('parent_asin').is_in(first_items))\n",
        "    \n",
        "    # Get items from training data\n",
        "    item_list = df['parent_asin'].unique().to_list()\n",
        "    \n",
        "    # Filter metadata to only items in training\n",
        "    df_meta_filtered = df_meta.filter(pl.col('parent_asin').is_in(item_list))\n",
        "    \n",
        "    logger.log_info(f\"[Content] Items in train: {len(item_list):,}\")\n",
        "    logger.log_info(f\"[Content] Items with metadata: {len(df_meta_filtered):,}\")\n",
        "    \n",
        "    # Create indices\n",
        "    item_rev = item_list\n",
        "    item_idx = {a_id: idx for idx, a_id in enumerate(item_rev)}\n",
        "    user_rev = df['user_id'].unique().to_list()\n",
        "    user_idx = {u_id: idx for idx, u_id in enumerate(user_rev)}\n",
        "    \n",
        "    # Build rating matrix\n",
        "    u = np.array([user_idx[x] for x in df['user_id'].to_list()], dtype=np.int32)\n",
        "    i = np.array([item_idx[x] for x in df['parent_asin'].to_list()], dtype=np.int32)\n",
        "    v = np.array(df['rating'].to_list(), dtype=np.float32)\n",
        "    \n",
        "    nU = len(user_rev)\n",
        "    nI = len(item_rev)\n",
        "    R = csr_matrix((v, (u, i)), shape=(nU, nI), dtype=np.float32)\n",
        "    \n",
        "    # Create description mapping\n",
        "    meta_dict = {row['parent_asin']: row['description'] \n",
        "                 for row in df_meta_filtered.iter_rows(named=True)}\n",
        "    \n",
        "    descriptions = [meta_dict.get(asin, \"\") for asin in item_rev]\n",
        "    \n",
        "    # Count missing descriptions\n",
        "    missing_count = sum(1 for d in descriptions if not d or d.strip() == \"\")\n",
        "    logger.log_info(f\"[Content] Descriptions: {len(descriptions) - missing_count:,} valid, \"\n",
        "                   f\"{missing_count:,} missing ({missing_count/len(descriptions)*100:.1f}%)\")\n",
        "    \n",
        "    # TF-IDF Vectorization\n",
        "    logger.log_info(f\"[TF-IDF] Vectorizing with max_features={max_features}, \"\n",
        "                   f\"min_df={min_df}, ngram_range={ngram_range}...\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=max_features,\n",
        "        stop_words='english',\n",
        "        min_df=min_df,\n",
        "        ngram_range=ngram_range\n",
        "    )\n",
        "    tfidf_matrix = vectorizer.fit_transform(descriptions)\n",
        "    \n",
        "    logger.log_info(f\"[TF-IDF] Matrix shape: {tfidf_matrix.shape}, \"\n",
        "                   f\"nnz: {tfidf_matrix.nnz:,}, \"\n",
        "                   f\"sparsity: {(1 - tfidf_matrix.nnz/(tfidf_matrix.shape[0]*tfidf_matrix.shape[1])):.2%}\")\n",
        "    logger.log_info(f\"[TF-IDF] Vocabulary size: {len(vectorizer.vocabulary_):,}\")\n",
        "    \n",
        "    # Compute similarity\n",
        "    logger.log_info(f\"[Similarity] Computing cosine similarity for {nI} items...\")\n",
        "    item_similarity = cosine_similarity(tfidf_matrix, dense_output=False)\n",
        "    \n",
        "    # Filter low similarities (optional, for speed and quality)\n",
        "    min_sim_threshold = 0.01\n",
        "    item_similarity.data[item_similarity.data < min_sim_threshold] = 0\n",
        "    item_similarity.eliminate_zeros()\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    logger.log_info(f\"[Similarity] Shape: {item_similarity.shape}, \"\n",
        "                   f\"nnz: {item_similarity.nnz:,}, \"\n",
        "                   f\"sparsity: {(1 - item_similarity.nnz/(nI*nI)):.2%}\")\n",
        "    logger.log_info(f\"[Similarity] Range: [{item_similarity.data.min():.4f}, \"\n",
        "                   f\"{item_similarity.data.max():.4f}]\")\n",
        "    logger.log_info(f\"[Content] Built in {elapsed:.1f}s\")\n",
        "    \n",
        "    user_rev_arr = np.array(user_rev, dtype=object)\n",
        "    item_rev_arr = np.array(item_rev, dtype=object)\n",
        "    \n",
        "    logger.log_info(f\"[Content-Model] R{R.shape} nnz={R.nnz} | Similarity{item_similarity.shape}\")\n",
        "    \n",
        "    return R, user_idx, item_idx, user_rev_arr, item_rev_arr, item_similarity, vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prediction & Recommendation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_content_based(user_idx_val: int, R: csr_matrix, item_similarity: csr_matrix, \n",
        "                         top_k: int = 30) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Predict ratings for all items using content-based CF\n",
        "    SAFE VECTORIZED VERSION (same as fixed item-based)\n",
        "    \n",
        "    Args:\n",
        "        user_idx_val: User index\n",
        "        R: Rating matrix\n",
        "        item_similarity: Item-item similarity from TF-IDF\n",
        "        top_k: Number of similar items to use\n",
        "    \n",
        "    Returns:\n",
        "        Predicted scores for all items\n",
        "    \"\"\"\n",
        "    user_ratings = R.getrow(user_idx_val).toarray().ravel()\n",
        "    rated_items = np.nonzero(user_ratings)[0]\n",
        "    \n",
        "    if len(rated_items) == 0:\n",
        "        return np.zeros(R.shape[1], dtype=np.float32)\n",
        "    \n",
        "    rated_ratings = user_ratings[rated_items]\n",
        "    n_items = R.shape[1]\n",
        "    scores = np.zeros(n_items, dtype=np.float32)\n",
        "    \n",
        "    # Get similarity matrix: all items vs rated items\n",
        "    sim_matrix = item_similarity[:, rated_items].toarray()\n",
        "    \n",
        "    for i in range(n_items):\n",
        "        sims = sim_matrix[i, :]\n",
        "        \n",
        "        # Only use POSITIVE similarities\n",
        "        positive_mask = sims > 0\n",
        "        n_positive = positive_mask.sum()\n",
        "        \n",
        "        if n_positive == 0:\n",
        "            continue\n",
        "        \n",
        "        sims_positive = sims[positive_mask]\n",
        "        ratings_positive = rated_ratings[positive_mask]\n",
        "        \n",
        "        # Select top-K\n",
        "        k_use = min(top_k, n_positive)\n",
        "        \n",
        "        if k_use <= 0:\n",
        "            continue\n",
        "        \n",
        "        if k_use < n_positive:\n",
        "            if k_use == 1:\n",
        "                top_idx = np.array([np.argmax(sims_positive)])\n",
        "            else:\n",
        "                top_idx = np.argpartition(-sims_positive, min(k_use-1, len(sims_positive)-1))[:k_use]\n",
        "        else:\n",
        "            top_idx = np.arange(n_positive)\n",
        "        \n",
        "        if len(top_idx) == 0:\n",
        "            continue\n",
        "        \n",
        "        final_sims = sims_positive[top_idx]\n",
        "        final_ratings = ratings_positive[top_idx]\n",
        "        \n",
        "        if len(final_sims) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Weighted average\n",
        "        sim_sum = np.sum(final_sims)\n",
        "        if sim_sum > 1e-8:\n",
        "            scores[i] = np.dot(final_sims, final_ratings) / sim_sum\n",
        "    \n",
        "    return scores\n",
        "\n",
        "def recommend_content_based(user_id: str, n_recs: int, artifacts: dict) -> pl.DataFrame:\n",
        "    R = artifacts['R']\n",
        "    user_idx = artifacts['user_idx']\n",
        "    item_rev = artifacts['item_rev']\n",
        "    item_similarity = artifacts['item_similarity']\n",
        "    top_k = artifacts.get('top_k_similar', TOP_K_SIMILAR)\n",
        "    if user_id not in user_idx:\n",
        "        logger.log_warning(f\"[Recommend] user_id={user_id} not found.\")\n",
        "        return pl.DataFrame(columns=[\"parent_asin\", \"score\"])\n",
        "    u = user_idx[user_id]\n",
        "    scores = predict_content_based(u, R, item_similarity, top_k=top_k)\n",
        "    rated = set(R.getrow(u).indices.tolist())\n",
        "    cand_mask = np.ones(R.shape[1], dtype=bool)\n",
        "    if rated:\n",
        "        cand_mask[list(rated)] = False\n",
        "    cand_scores = scores[cand_mask]\n",
        "    if cand_scores.size == 0:\n",
        "        return pl.DataFrame(columns=[\"parent_asin\", \"score\"])\n",
        "    n_top = min(n_recs, cand_scores.size)\n",
        "    cand_indices = np.nonzero(cand_mask)[0]\n",
        "    top_pos = np.argpartition(-cand_scores, n_top - 1)[:n_top]\n",
        "    picked = sorted([(int(cand_indices[p]), float(cand_scores[p])) for p in top_pos], key=lambda x: -x[1])\n",
        "    rec_asins = [item_rev[i] for i, _ in picked]\n",
        "    rec_scores = [s for _, s in picked]\n",
        "    return pl.DataFrame({\"parent_asin\": rec_asins, \"score\": rec_scores})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save/Load Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_content_artifacts(out_dir: Path, R, user_rev, item_rev, user_idx, item_idx, item_similarity, vectorizer):\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    save_npz(out_dir / \"R.npz\", R)\n",
        "    save_npz(out_dir / \"item_similarity.npz\", item_similarity)\n",
        "    with open(out_dir / \"user_rev.pkl\", \"wb\") as f: pickle.dump(user_rev, f)\n",
        "    with open(out_dir / \"item_rev.pkl\", \"wb\") as f: pickle.dump(item_rev, f)\n",
        "    (out_dir / \"user_idx.json\").write_text(json.dumps({str(k): int(v) for k, v in user_idx.items()}))\n",
        "    (out_dir / \"item_idx.json\").write_text(json.dumps({str(k): int(v) for k, v in item_idx.items()}))\n",
        "    with open(out_dir / \"vectorizer.pkl\", \"wb\") as f: pickle.dump(vectorizer, f)\n",
        "    logger.log_info(f\"[Saved-Content] {out_dir}\")\n",
        "\n",
        "def load_content_artifacts(model_dir: str | Path):\n",
        "    md = Path(model_dir)\n",
        "    R = load_npz(md / \"R.npz\")\n",
        "    item_similarity = load_npz(md / \"item_similarity.npz\")\n",
        "    with open(md / \"user_rev.pkl\", \"rb\") as f: user_rev = pickle.load(f)\n",
        "    with open(md / \"item_rev.pkl\", \"rb\") as f: item_rev = pickle.load(f)\n",
        "    user_idx = {k: int(v) for k, v in json.loads((md / \"user_idx.json\").read_text()).items()}\n",
        "    item_idx = {k: int(v) for k, v in json.loads((md / \"item_idx.json\").read_text()).items()}\n",
        "    with open(md / \"vectorizer.pkl\", \"rb\") as f: vectorizer = pickle.load(f)\n",
        "    return dict(R=R, item_similarity=item_similarity, user_rev=user_rev, item_rev=item_rev, user_idx=user_idx, item_idx=item_idx, vectorizer=vectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_content_models_for_categories(categories, top_k_similar=30, models_dir=None, max_users=None, max_items=None):\n",
        "    base = Path(models_dir) if models_dir else MODELS_DIR\n",
        "    out_algo = base / \"content\"\n",
        "    out_algo.mkdir(parents=True, exist_ok=True)\n",
        "    rows = []\n",
        "    for cat in categories:\n",
        "        try:\n",
        "            out_dir = out_algo / cat\n",
        "            if out_dir.exists() and (out_dir / \"R.npz\").exists():\n",
        "                logger.log_info(f\"[Skip] Content based model exists for {cat}\")\n",
        "                rows.append({\"category\": cat, \"algo\": \"user\", \"models_dir\": str(out_dir), \"top_k_similar\": top_k_similar})\n",
        "                continue\n",
        "\n",
        "            logger.log_info(f\"[Content] Training {cat}\")\n",
        "            df_train = load_5core_data(cat, split=\"train\")\n",
        "            df_meta = load_metadata(cat)\n",
        "            R, user_idx, item_idx, user_rev, item_rev, item_similarity, vectorizer = build_content_model(df_train, df_meta, max_users=max_users, max_items=max_items)\n",
        "            \n",
        "            save_content_artifacts(out_dir, R, user_rev, item_rev, user_idx, item_idx, item_similarity, vectorizer)\n",
        "            rows.append({\"category\": cat, \"algo\": \"content\", \"models_dir\": str(out_dir), \"top_k_similar\": top_k_similar, \"R_nnz\": int(R.nnz), \"users\": len(user_rev), \"items\": len(item_rev)})\n",
        "        except Exception as e:\n",
        "            logger.log_exception(f\"[Error-Content] {cat}: {e}\")\n",
        "            rows.append({\"category\": cat, \"algo\": \"content\", \"models_dir\": None, \"top_k_similar\": top_k_similar, \"error\": str(e)})\n",
        "    summary = pl.DataFrame(rows)\n",
        "    logger.log_info(f\"[Summary-Content] Trained={len(rows)} OK={summary['models_dir'].is_not_null().sum()} FAIL={summary['models_dir'].is_null().sum()}\")\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_content_based(category: str, artifacts: dict, k_values: list = [10, 20, 50], \n",
        "                          split: str = \"test\", sample_users: int = 3000):\n",
        "    \n",
        "    logger.log_info(f\"[Eval-Content] {category} on {split.upper()}\")\n",
        "    \n",
        "    df_eval = load_5core_data(category, split=split)\n",
        "    R = artifacts['R']\n",
        "    user_idx = artifacts['user_idx']\n",
        "    item_idx = artifacts['item_idx']\n",
        "    item_similarity = artifacts['item_similarity']\n",
        "    top_k = artifacts.get('top_k_similar', 30)\n",
        "    \n",
        "    # Filter to train users only\n",
        "    train_user_list = list(user_idx.keys())\n",
        "    df_eval = df_eval.filter(pl.col('user_id').is_in(train_user_list))\n",
        "    \n",
        "    if len(df_eval) == 0:\n",
        "        logger.log_warning(f\"[Eval-Content] No data after filtering\")\n",
        "        return None\n",
        "    \n",
        "    logger.log_info(f\"[Eval-Content] After filtering: {len(df_eval):,} ratings, {df_eval['user_id'].n_unique():,} users\")\n",
        "    \n",
        "    # Sample users\n",
        "    eval_users = df_eval['user_id'].unique().to_list()\n",
        "    if len(eval_users) > sample_users:\n",
        "        np.random.seed(42)\n",
        "        eval_users = np.random.choice(eval_users, sample_users, replace=False).tolist()\n",
        "    \n",
        "    logger.log_info(f\"[Eval-Content] Evaluating {len(eval_users)} users with top_k={top_k}...\")\n",
        "    \n",
        "    # Initialize accumulators\n",
        "    metrics_acc = {\n",
        "        'rmse': [], 'accuracy': [],\n",
        "        **{f'recall@{k}': [] for k in k_values},\n",
        "        **{f'ndcg@{k}': [] for k in k_values},\n",
        "        **{f'map@{k}': [] for k in k_values}\n",
        "    }\n",
        "    \n",
        "    evaluated_users = 0\n",
        "    \n",
        "    for user_id in eval_users:\n",
        "        if user_id not in user_idx:\n",
        "            continue\n",
        "        \n",
        "        u = user_idx[user_id]\n",
        "        user_eval = df_eval.filter(pl.col('user_id') == user_id)\n",
        "        actual_items = set(user_eval['parent_asin'].to_list())\n",
        "        actual_ratings = {row['parent_asin']: row['rating'] \n",
        "                         for row in user_eval.iter_rows(named=True)}\n",
        "        \n",
        "        if len(actual_items) == 0:\n",
        "            continue\n",
        "        \n",
        "        known_items = {item for item in actual_items if item in item_idx}\n",
        "        \n",
        "        # Skip if no known items\n",
        "        if len(known_items) == 0:\n",
        "            continue\n",
        "        \n",
        "        evaluated_users += 1\n",
        "        scores = predict_content_based(u, R, item_similarity, top_k=top_k)\n",
        "        \n",
        "        # RMSE & Accuracy\n",
        "        predictions = np.full(R.shape[1], np.nan)\n",
        "        actuals = np.full(R.shape[1], np.nan)\n",
        "        for asin in known_items:\n",
        "            idx = item_idx[asin]\n",
        "            predictions[idx] = scores[idx]\n",
        "            actuals[idx] = actual_ratings[asin]\n",
        "        \n",
        "        rmse, acc = compute_rmse_accuracy(predictions, actuals)\n",
        "        if not np.isnan(rmse):\n",
        "            metrics_acc['rmse'].append(rmse)\n",
        "            metrics_acc['accuracy'].append(acc)\n",
        "        \n",
        "        # Ranking metrics\n",
        "        rated = set(R.getrow(u).indices.tolist())\n",
        "        cand_mask = np.ones(R.shape[1], dtype=bool)\n",
        "        if rated:\n",
        "            cand_mask[list(rated)] = False\n",
        "        \n",
        "        cand_scores = scores[cand_mask]\n",
        "        if cand_scores.size == 0:\n",
        "            continue\n",
        "        \n",
        "        cand_indices = np.nonzero(cand_mask)[0]\n",
        "        max_k = max(k_values)\n",
        "        n_top = min(max_k, cand_scores.size)\n",
        "        top_pos = np.argpartition(-cand_scores, n_top - 1)[:n_top]\n",
        "        sorted_idx = top_pos[np.argsort(-cand_scores[top_pos])]\n",
        "        \n",
        "        item_rev = artifacts['item_rev']\n",
        "        recommended = [item_rev[cand_indices[i]] for i in sorted_idx]\n",
        "        \n",
        "        for k in k_values:\n",
        "            metrics_acc[f'recall@{k}'].append(recall_at_k(recommended, known_items, k))\n",
        "            metrics_acc[f'ndcg@{k}'].append(ndcg_at_k(recommended, known_items, k))\n",
        "            metrics_acc[f'map@{k}'].append(map_at_k(recommended, known_items, k))\n",
        "    \n",
        "    logger.log_info(f\"[Eval-Content] Actually evaluated: {evaluated_users} users\")\n",
        "    \n",
        "    # Aggregate\n",
        "    results = {\n",
        "        'category': category,\n",
        "        'split': split,\n",
        "        'n_users': evaluated_users,\n",
        "        'rmse': np.mean(metrics_acc['rmse']) if metrics_acc['rmse'] else np.nan,\n",
        "        'accuracy': np.mean(metrics_acc['accuracy']) if metrics_acc['accuracy'] else np.nan\n",
        "    }\n",
        "    \n",
        "    for k in k_values:\n",
        "        for metric in ['recall', 'ndcg', 'map']:\n",
        "            key = f'{metric}@{k}'\n",
        "            results[key] = np.mean(metrics_acc[key]) if metrics_acc[key] else 0.0\n",
        "    \n",
        "    logger.log_info(f\"[Eval-Content] RMSE={results['rmse']:.4f}, Acc={results['accuracy']:.4f}\")\n",
        "    logger.log_info(f\"[Eval-Content] NDCG@10={results['ndcg@10']:.4f}, Recall@10={results['recall@10']:.4f}\")\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_best_k(df_results: pl.DataFrame):\n",
        "    \"\"\"Select best K using NDCG-primary strategy\"\"\"\n",
        "    best_k_ndcg = df_results['K'][df_results['NDCG@10'].arg_max()]\n",
        "    best_ndcg = df_results['NDCG@10'].max()\n",
        "    \n",
        "    logger.log_info(f\"\\nPrimary metric (NDCG@10): K={best_k_ndcg}, score={best_ndcg:.4f}\")\n",
        "    \n",
        "    threshold = best_ndcg * 0.98\n",
        "    similar_rows = df_results.filter(pl.col('NDCG@10') >= threshold)\n",
        "    similar_k = similar_rows['K'].to_list()\n",
        "    \n",
        "    if len(similar_k) > 1:\n",
        "        logger.log_info(f\"Multiple K with similar NDCG (within 2%): {similar_k}\")\n",
        "        best_k = similar_rows['K'][similar_rows['Recall@10'].arg_max()]\n",
        "        logger.log_info(f\"Selected K={best_k} based on Recall@10\")\n",
        "    else:\n",
        "        best_k = best_k_ndcg\n",
        "        logger.log_info(f\"Clear winner: K={best_k}\")\n",
        "    \n",
        "    return best_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pipeline and execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _train_single_category(cat, model_dir, K_VALUES, n_eval_tune):\n",
        "    \"\"\"Helper: Train and tune a single category for content-based CF\"\"\"\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 1: BUILD BASE MODEL (if not exists)\n",
        "    # ========================================================================\n",
        "    \n",
        "    if not (model_dir / \"R.npz\").exists():\n",
        "        logger.log_info(\"STEP 1: TRAINING BASE MODEL\")\n",
        "        logger.log_info(\"-\"*70)\n",
        "        logger.log_info(\"Building TF-IDF similarity matrix (done once)\\n\")\n",
        "        \n",
        "        # Load data\n",
        "        df_train = load_5core_data(cat, split=\"train\")\n",
        "        df_meta = load_metadata(cat)\n",
        "        \n",
        "        # Build content-based model\n",
        "        logger.log_info(\"Computing TF-IDF and item similarities...\")\n",
        "        R, user_idx, item_idx, user_rev, item_rev, item_similarity, vectorizer = build_content_model(\n",
        "            df_train, df_meta, max_users=MAX_USERS, max_items=MAX_ITEMS,\n",
        "            max_features=TFIDF_MAX_FEATURES, min_df=TFIDF_MIN_DF, \n",
        "            ngram_range=TFIDF_NGRAM_RANGE\n",
        "        )\n",
        "        \n",
        "        # Save artifacts\n",
        "        save_content_artifacts(model_dir, R, user_rev, item_rev,\n",
        "                             user_idx, item_idx, item_similarity, vectorizer)\n",
        "        \n",
        "        logger.log_info(f\"Base model saved to {model_dir}\\n\")\n",
        "    else:\n",
        "        logger.log_info(\"STEP 1: BASE MODEL EXISTS\")\n",
        "        logger.log_info(\"-\"*70)\n",
        "        logger.log_info(f\"Loading from {model_dir}\\n\")\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 2: HYPERPARAMETER TUNING (if not done)\n",
        "    # ========================================================================\n",
        "    \n",
        "    if not Configurations.has_tuning_results_content(cat):\n",
        "        logger.log_info(\"STEP 2: HYPERPARAMETER TUNING (VALIDATION)\")\n",
        "        logger.log_info(\"-\"*70)\n",
        "        logger.log_info(f\"K values: {K_VALUES}\")\n",
        "        logger.log_info(f\"Validation users: {n_eval_tune}\\n\")\n",
        "        \n",
        "        # Load base artifacts\n",
        "        artifacts = load_content_artifacts(model_dir)\n",
        "        \n",
        "        results = []\n",
        "        \n",
        "        # Test each K value\n",
        "        for i, k in enumerate(K_VALUES, 1):\n",
        "            logger.log_info(f\"\\n[{i}/{len(K_VALUES)}] Testing K={k}\")\n",
        "            logger.log_info(\"-\"*70)\n",
        "            \n",
        "            # Create artifacts for this K\n",
        "            eval_artifacts = artifacts.copy()\n",
        "            eval_artifacts['top_k_similar'] = k\n",
        "            \n",
        "            # Evaluate\n",
        "            start_time = time.time()\n",
        "            metrics = evaluate_content_based(\n",
        "                cat, eval_artifacts,\n",
        "                k_values=[10, 20, 50],\n",
        "                split=\"valid\",\n",
        "                sample_users=n_eval_tune\n",
        "            )\n",
        "            eval_time = time.time() - start_time\n",
        "            \n",
        "            if metrics:\n",
        "                results.append({\n",
        "                    'K': k,\n",
        "                    'NDCG@10': metrics['ndcg@10'],\n",
        "                    'NDCG@20': metrics['ndcg@20'],\n",
        "                    'NDCG@50': metrics['ndcg@50'],\n",
        "                    'Recall@10': metrics['recall@10'],\n",
        "                    'Recall@20': metrics['recall@20'],\n",
        "                    'Recall@50': metrics['recall@50'],\n",
        "                    'MAP@10': metrics['map@10'],\n",
        "                    'MAP@20': metrics['map@20'],\n",
        "                    'MAP@50': metrics['map@50'],\n",
        "                    'RMSE': metrics['rmse'],\n",
        "                    'Accuracy': metrics['accuracy'],\n",
        "                    'Eval_Time': eval_time\n",
        "                })\n",
        "                \n",
        "                logger.log_info(f\"Results:\")\n",
        "                logger.log_info(f\"  NDCG@10:   {metrics['ndcg@10']:.4f}\")\n",
        "                logger.log_info(f\"  Recall@10: {metrics['recall@10']:.4f}\")\n",
        "                logger.log_info(f\"  Eval:      {eval_time:.1f}s\")\n",
        "        \n",
        "        # Save and select best\n",
        "        df_results = pl.DataFrame(results)\n",
        "        df_results.write_csv(MODELS_DIR / 'content' / f'tuning_{cat}.csv')\n",
        "        \n",
        "        best_k = select_best_k(df_results)\n",
        "        Configurations.save_best_k_content(cat, best_k)\n",
        "        \n",
        "        visualize_hyperparameter_tuning(\n",
        "                                        df_results,\n",
        "                                        category=cat,\n",
        "                                        param_col='K',\n",
        "                                        param_name='K (Similar Items - Content)',\n",
        "                                        save_dir=MODELS_DIR / 'content',\n",
        "                                        algo_name='Content-Based'\n",
        "                                    )\n",
        "        \n",
        "        logger.log_info(f\"Final model uses K={best_k}\\n\")\n",
        "        \n",
        "        return {'tuned_now': True, 'best_k': best_k}\n",
        "    else:\n",
        "        best_k = Configurations.load_best_k_content(cat)\n",
        "        logger.log_info(f\"STEP 2: TUNING ALREADY DONE (K={best_k})\")\n",
        "        return {'tuned_now': False, 'best_k': best_k}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Phase 1: Training + Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PHASE 1: TRAINING + TUNING ALL CATEGORIES\n",
        "# ============================================================================\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"PHASE 1: TRAINING + TUNING ALL CATEGORIES (CONTENT-BASED)\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "if not Configurations.has_tuning_results_content(CATEGORY[0]):\n",
        "    logger.log_info(f\"K values to test: {K_VALUES}\\n\")\n",
        "\n",
        "workflow_results = {}\n",
        "\n",
        "for cat in CATEGORY:\n",
        "    logger.log_info(f\"\\n{'='*70}\\nCATEGORY: {cat}\\n{'='*70}\\n\")\n",
        "    \n",
        "    model_dir = MODELS_DIR / \"content\" / cat\n",
        "    workflow_results[cat] = _train_single_category(\n",
        "        cat, model_dir, K_VALUES, Configurations.get_eval_samples_tuning()\n",
        "    )\n",
        "\n",
        "# Summary\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"PHASE 1 COMPLETE: ALL MODELS TRAINED AND TUNED\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "logger.log_info(\"Tuning Summary:\")\n",
        "for cat in CATEGORY:\n",
        "    status = 'newly tuned' if workflow_results[cat]['tuned_now'] else 'loaded from cache'\n",
        "    logger.log_info(f\"  {cat}: K={workflow_results[cat]['best_k']} ({status})\")\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"Ready for Phase 2: Final Evaluation\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Phase 2: Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PHASE 2: FINAL EVALUATION ON TEST SET (ALL CATEGORIES)\n",
        "# ============================================================================\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"PHASE 2: FINAL EVALUATION ON TEST SET (CONTENT-BASED)\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Helper function\n",
        "def _create_val_test_comparison(cat, best_k, tuning_row, final_row):\n",
        "    \"\"\"Create val vs test comparison plot\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    metrics_names = ['NDCG@10', 'Recall@10', 'MAP@10']\n",
        "    val_scores = [tuning_row['NDCG@10'], tuning_row['Recall@10'], tuning_row['MAP@10']]\n",
        "    test_scores = [final_row['ndcg@10'], final_row['recall@10'], final_row['map@10']]\n",
        "    \n",
        "    x = np.arange(len(metrics_names))\n",
        "    width = 0.35\n",
        "    \n",
        "    axes[0].bar(x - width/2, val_scores, width, label='Validation', alpha=0.8, color='#3498DB')\n",
        "    axes[0].bar(x + width/2, test_scores, width, label='Test', alpha=0.8, color='#2ECC71')\n",
        "    axes[0].set_xlabel('Metrics', fontsize=11)\n",
        "    axes[0].set_ylabel('Score', fontsize=11)\n",
        "    axes[0].set_title(f'Validation vs Test (Content-Based) - {cat} (K={best_k})', \n",
        "                    fontsize=12, fontweight='bold')\n",
        "    axes[0].set_xticks(x)\n",
        "    axes[0].set_xticklabels(metrics_names)\n",
        "    axes[0].legend(fontsize=10)\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    for i, (v, t) in enumerate(zip(val_scores, test_scores)):\n",
        "        axes[0].text(i - width/2, v, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n",
        "        axes[0].text(i + width/2, t, f'{t:.4f}', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    # Table\n",
        "    axes[1].axis('off')\n",
        "    improvements = [f\"{(t/v - 1)*100:+.1f}%\" if v > 0 else \"N/A\" \n",
        "                   for v, t in zip(val_scores, test_scores)]\n",
        "    \n",
        "    val_scores_full = val_scores + [tuning_row['RMSE']]\n",
        "    test_scores_full = test_scores + [final_row['rmse']]\n",
        "    improvements_full = improvements + [f\"{(final_row['rmse']/tuning_row['RMSE'] - 1)*100:+.1f}%\"]\n",
        "    metrics_names_full = metrics_names + ['RMSE']\n",
        "    \n",
        "    table_data = [\n",
        "        ['Metric', 'Validation', 'Test', 'Change'],\n",
        "        *[[name, f\"{v:.4f}\", f\"{t:.4f}\", imp]\n",
        "          for name, v, t, imp in zip(metrics_names_full, val_scores_full, \n",
        "                                     test_scores_full, improvements_full)]\n",
        "    ]\n",
        "    \n",
        "    table = axes[1].table(cellText=table_data, cellLoc='center', loc='center',\n",
        "                         colWidths=[0.25, 0.25, 0.25, 0.25])\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    table.scale(1, 2.5)\n",
        "    \n",
        "    for j in range(4):\n",
        "        table[(0, j)].set_facecolor('#34495E')\n",
        "        table[(0, j)].set_text_props(weight='bold', color='white')\n",
        "    \n",
        "    for i in range(1, 5):\n",
        "        change_val = table_data[i][3]\n",
        "        color = '#D5F4E6' if (change_val.startswith('+') and i < 4) or (change_val.startswith('-') and i == 4) else '#FADBD8'\n",
        "        table[(i, 3)].set_facecolor(color)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    out_path = MODELS_DIR / 'content' / f'val_vs_test_{cat}.png'\n",
        "    plt.savefig(out_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Load workflow results\n",
        "if 'workflow_results' not in locals():\n",
        "    workflow_results = {}\n",
        "    for cat in CATEGORY:\n",
        "        best_k = Configurations.load_best_k_content(cat)\n",
        "        workflow_results[cat] = {'best_k': best_k, 'tuned_now': False}\n",
        "    logger.log_info(\"Loaded best K from configuration\\n\")\n",
        "\n",
        "n_eval_final = Configurations.get_eval_samples_final()\n",
        "logger.log_info(f\"Test users per category: {n_eval_final}\\n\")\n",
        "\n",
        "# Run test evaluation\n",
        "for cat in CATEGORY:\n",
        "    logger.log_info(f\"\\n{'='*70}\\nTESTING: {cat}\\n{'='*70}\\n\")\n",
        "    \n",
        "    model_dir = MODELS_DIR / \"content\" / cat\n",
        "    best_k = workflow_results[cat]['best_k']\n",
        "    logger.log_info(f\"Using K: {best_k}\")\n",
        "    \n",
        "    final_artifacts = load_content_artifacts(model_dir)\n",
        "    final_artifacts['top_k_similar'] = best_k\n",
        "    \n",
        "    logger.log_info(\"Evaluating on test set...\\n\")\n",
        "    results = evaluate_content_based(cat, final_artifacts, k_values=[10, 20, 50],\n",
        "                                    split=\"test\", sample_users=n_eval_final)\n",
        "    \n",
        "    if results:\n",
        "        workflow_results[cat]['test_results'] = results\n",
        "        logger.log_info(f\"\\nTest Results (K={best_k}):\")\n",
        "        logger.log_info(f\"  NDCG@10: {results['ndcg@10']:.4f}, \"\n",
        "                       f\"Recall@10: {results['recall@10']:.4f}, \"\n",
        "                       f\"MAP@10: {results['map@10']:.4f}\")\n",
        "        logger.log_info(f\"  RMSE: {results['rmse']:.4f}, \"\n",
        "                       f\"Accuracy: {results['accuracy']:.4f}\\n\")\n",
        "\n",
        "# Save final results\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"SAVING FINAL RESULTS\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "test_results_list = [workflow_results[cat]['test_results'] \n",
        "                     for cat in CATEGORY \n",
        "                     if 'test_results' in workflow_results[cat]]\n",
        "\n",
        "if test_results_list:\n",
        "    df_final_results = pl.DataFrame(test_results_list)\n",
        "    \n",
        "    logger.log_info(\"Final Test Results:\")\n",
        "    display(df_final_results)\n",
        "    \n",
        "    out_csv = MODELS_DIR / 'content' / 'final_test_results.csv'\n",
        "    df_final_results.write_csv(out_csv)\n",
        "    logger.log_info(f\"\\nSaved: {out_csv}\")\n",
        "    \n",
        "    logger.log_info(\"Generating final evaluation plot...\")\n",
        "    visualize_final_results(\n",
        "                            test_results_list,\n",
        "                            save_dir=MODELS_DIR / 'content',\n",
        "                            algo_name='Content-Based',\n",
        "                            k_values=[10, 20, 50]\n",
        "                        )\n",
        "    logger.log_info(f\"Saved: evaluation_results.png\\n\")\n",
        "\n",
        "# Post-analysis visualization\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"POST-ANALYSIS VISUALIZATION\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "for cat in CATEGORY:\n",
        "    tuning_csv = MODELS_DIR / 'content' / f'tuning_{cat}.csv'\n",
        "    \n",
        "    if not tuning_csv.exists() or 'test_results' not in workflow_results[cat]:\n",
        "        continue\n",
        "    \n",
        "    logger.log_info(f\"Generating Val vs Test comparison for {cat}...\")\n",
        "    \n",
        "    df_tuning = pl.read_csv(tuning_csv)\n",
        "    best_k = workflow_results[cat]['best_k']\n",
        "    \n",
        "    tuning_row = df_tuning.filter(pl.col('K') == best_k).row(0, named=True)\n",
        "    final_row = df_final_results.filter(pl.col('category') == cat).row(0, named=True)\n",
        "    \n",
        "    visualize_val_test_comparison(\n",
        "                                    cat=cat,\n",
        "                                    param_val=best_k,\n",
        "                                    tuning_row=tuning_row,\n",
        "                                    final_row=final_row,\n",
        "                                    save_dir=MODELS_DIR / 'content',\n",
        "                                    param_name='K',\n",
        "                                    algo_name='Content-Based'\n",
        "                                )\n",
        "    logger.log_info(f\"  Saved: val_vs_test_{cat}.png\\n\")\n",
        "\n",
        "# Summary\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"COMPLETE WORKFLOW SUMMARY (CONTENT-BASED)\")\n",
        "logger.log_info(\"=\"*70)\n",
        "\n",
        "for cat in CATEGORY:\n",
        "    logger.log_info(f\"\\n{cat}: K={workflow_results[cat]['best_k']}\")\n",
        "    if 'test_results' in workflow_results[cat]:\n",
        "        test = workflow_results[cat]['test_results']\n",
        "        logger.log_info(f\"  NDCG@10: {test['ndcg@10']:.4f}, \"\n",
        "                       f\"Recall@10: {test['recall@10']:.4f}\")\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"ALL PHASES COMPLETE\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Debug info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_content_quality(category: str):\n",
        "    \"\"\"Check metadata quality and TF-IDF statistics\"\"\"\n",
        "    logger.log_info(f\"\\n{'='*70}\")\n",
        "    logger.log_info(f\"CONTENT QUALITY CHECK: {category}\")\n",
        "    logger.log_info(f\"{'='*70}\\n\")\n",
        "    \n",
        "    # Load metadata\n",
        "    df_meta = load_metadata(category)\n",
        "    \n",
        "    logger.log_info(\"Metadata Statistics:\")\n",
        "    logger.log_info(\"-\" * 70)\n",
        "    logger.log_info(f\"  Total items: {len(df_meta):,}\")\n",
        "    \n",
        "    # Analyze descriptions\n",
        "    descriptions = df_meta['description'].to_list()\n",
        "    valid_descs = [d for d in descriptions if d and d.strip()]\n",
        "    \n",
        "    logger.log_info(f\"  Valid descriptions: {len(valid_descs):,} ({len(valid_descs)/len(df_meta)*100:.1f}%)\")\n",
        "    \n",
        "    if valid_descs:\n",
        "        lengths = [len(d) for d in valid_descs]\n",
        "        word_counts = [len(d.split()) for d in valid_descs]\n",
        "        \n",
        "        logger.log_info(f\"\\nText Statistics:\")\n",
        "        logger.log_info(f\"  Avg length: {np.mean(lengths):.1f} chars\")\n",
        "        logger.log_info(f\"  Avg words:  {np.mean(word_counts):.1f}\")\n",
        "        logger.log_info(f\"  Median words: {np.median(word_counts):.0f}\")\n",
        "    \n",
        "    # Load model\n",
        "    model_dir = MODELS_DIR / 'content' / category\n",
        "    if model_dir.exists():\n",
        "        logger.log_info(f\"\\n{'-' * 70}\")\n",
        "        logger.log_info(\"Model Statistics:\")\n",
        "        \n",
        "        artifacts = load_content_artifacts(model_dir)\n",
        "        vectorizer = artifacts['vectorizer']\n",
        "        item_similarity = artifacts['item_similarity']\n",
        "        \n",
        "        logger.log_info(f\"  Vocabulary size: {len(vectorizer.vocabulary_):,}\")\n",
        "        logger.log_info(f\"  TF-IDF matrix: {tfidf_matrix.shape if 'tfidf_matrix' in locals() else 'N/A'}\")\n",
        "        logger.log_info(f\"  Similarity matrix: {item_similarity.shape}\")\n",
        "        logger.log_info(f\"  Similarity nnz: {item_similarity.nnz:,}\")\n",
        "        logger.log_info(f\"  Similarity sparsity: {(1 - item_similarity.nnz/(item_similarity.shape[0]*item_similarity.shape[1])):.2%}\")\n",
        "        logger.log_info(f\"  Similarity range: [{item_similarity.data.min():.4f}, {item_similarity.data.max():.4f}]\")\n",
        "        \n",
        "        # Top features\n",
        "        logger.log_info(f\"\\nTop 20 TF-IDF Features:\")\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        for i, feat in enumerate(feature_names[:20], 1):\n",
        "            logger.log_info(f\"  {i}. {feat}\")\n",
        "    \n",
        "    logger.log_info(f\"\\n{'='*70}\\n\")\n",
        "\n",
        "check_content_quality(CATEGORY[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task: Unit test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### UI Recommendation Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recommend_content_ui(user_id: str, n_recs: int = 5, models_dir: str | Path | None = None, category: str | None = None) -> pl.DataFrame:\n",
        "    cat = category or CATEGORY[0]\n",
        "    model_dir = Path(models_dir) if models_dir else (MODELS_DIR / \"content\" / cat)\n",
        "    artifacts = load_content_artifacts(model_dir)\n",
        "    best_k = Configurations.load_best_k_content(cat)\n",
        "    artifacts['top_k_similar'] = best_k\n",
        "    return recommend_content_based(user_id, n_recs, artifacts)\n",
        "\n",
        "def unit_test_ui_content_recommend(user_id: str, n_recs: int = 5, models_dir: str | Path | None = None, category: str | None = None):\n",
        "    cat = category or CATEGORY[0]\n",
        "    md = models_dir if models_dir else (MODELS_DIR / \"content\" / cat)\n",
        "    logger.log_info(f\"[UnitTest-UI-CONTENT] model_dir={md} | user_id={user_id} | n_recs={n_recs}\")\n",
        "    recs = recommend_content_ui(user_id=user_id, n_recs=n_recs, models_dir=md, category=cat)\n",
        "    cols = set(recs.columns)\n",
        "    assert {\"parent_asin\", \"score\"}.issubset(cols), \"recs missing required columns\"\n",
        "    assert len(recs) <= n_recs, f\"recs length should be â‰¤ {n_recs}\"\n",
        "    logger.log_info(f\"[UnitTest-UI-CONTENT] returned {len(recs)} items âœ…\")\n",
        "    display(recs)\n",
        "    return recs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test All Categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_all_categories():\n",
        "    \"\"\"Unit test: Verify recommendation function\"\"\"\n",
        "    logger.log_info(\"\\n\" + \"=\"*70)\n",
        "    logger.log_info(\"[UNIT TEST] Testing Recommendation Function (Content-Based)\")\n",
        "    logger.log_info(\"=\"*70 + \"\\n\")\n",
        "    \n",
        "    test_summary = []\n",
        "    \n",
        "    for cat in CATEGORY:\n",
        "        logger.log_info(f\"\\n[Test] {cat}\")\n",
        "        logger.log_info(\"-\"*70)\n",
        "        \n",
        "        try:\n",
        "            model_dir = MODELS_DIR / \"content\" / cat\n",
        "            \n",
        "            if not model_dir.exists():\n",
        "                logger.log_warning(f\"  âœ— Model not found\")\n",
        "                test_summary.append({'category': cat, 'status': 'FAIL', 'reason': 'Model not found'})\n",
        "                continue\n",
        "            \n",
        "            artifacts = load_content_artifacts(model_dir)\n",
        "            best_k = Configurations.load_best_k_content(cat)\n",
        "            artifacts['top_k_similar'] = best_k\n",
        "            \n",
        "            user_rev = artifacts['user_rev']\n",
        "            item_rev = artifacts['item_rev']\n",
        "            \n",
        "            logger.log_info(f\"  Model loaded: {len(user_rev):,} users, {len(item_rev):,} items\")\n",
        "            logger.log_info(f\"  Using top_k_similar: {best_k}\")\n",
        "            \n",
        "            if len(user_rev) == 0:\n",
        "                logger.log_warning(f\"  âœ— No users\")\n",
        "                test_summary.append({'category': cat, 'status': 'FAIL', 'reason': 'No users'})\n",
        "                continue\n",
        "            \n",
        "            sample_user = user_rev[0]\n",
        "            logger.log_info(f\"  Testing user: {sample_user}\")\n",
        "            \n",
        "            recs = recommend_content_ui(sample_user, n_recs=N_RECS, category=cat)\n",
        "            \n",
        "            assert set(recs.columns) >= {\"parent_asin\", \"score\"}, \"Missing columns\"\n",
        "            assert len(recs) <= N_RECS, f\"Too many recs: {len(recs)}\"\n",
        "            \n",
        "            logger.log_info(f\"  âœ“ Generated {len(recs)} recommendations\")\n",
        "            logger.log_info(f\"  Score range: [{recs['score'].min():.4f}, {recs['score'].max():.4f}]\")\n",
        "            \n",
        "            test_summary.append({\n",
        "                'category': cat,\n",
        "                'status': 'PASS',\n",
        "                'n_recs': len(recs),\n",
        "                'score_min': float(recs['score'].min()),\n",
        "                'score_max': float(recs['score'].max())\n",
        "            })\n",
        "            \n",
        "            display(recs.head(5))\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.log_exception(f\"  âœ— Error: {e}\")\n",
        "            test_summary.append({'category': cat, 'status': 'FAIL', 'reason': str(e)})\n",
        "    \n",
        "    # Summary\n",
        "    logger.log_info(\"\\n\" + \"=\"*70)\n",
        "    logger.log_info(\"UNIT TEST SUMMARY\")\n",
        "    logger.log_info(\"=\"*70)\n",
        "    \n",
        "    df_summary = pl.DataFrame(test_summary)\n",
        "    display(df_summary)\n",
        "    \n",
        "    passed = sum(1 for r in test_summary if r['status'] == 'PASS')\n",
        "    logger.log_info(f\"\\nResults: {passed}/{len(test_summary)} passed\")\n",
        "    \n",
        "    logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "test_all_categories()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

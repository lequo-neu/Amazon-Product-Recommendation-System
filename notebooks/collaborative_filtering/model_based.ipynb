{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model-Based Collaborative Filtering (Matrix Factorization, ALS)\n",
        "\n",
        "**Goal**\n",
        "- Learn latent factors (U, V) from 5-core TRAIN to predict ratings and recommend Top-N.\n",
        "\n",
        "**Pipeline**\n",
        "1. Load 5-core TRAIN from `PROCESSED_DIR`\n",
        "2. Build sparse user-item matrix R (CSR)\n",
        "3. Train Matrix Factorization with TruncatedSVD\n",
        "4. Save artifacts (U, V, indexers) for fast UI inference\n",
        "5. Evaluate using TEST/VALID sets: Accuracy, RMSE, Recall@K, NDCG@K, MAP@K\n",
        "6. Load & recommend Top-N for users\n",
        "\n",
        "**Why MF?**\n",
        "- Captures hidden tastes/themes\n",
        "- Scales better than KNN\n",
        "- Fast inference via dot products"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task: Import modules and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, numpy as np, polars as pl, pickle, json, time\n",
        "from pathlib import Path\n",
        "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "module_path = os.path.abspath(os.path.join('..', '../utilities'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "from logger import Logger\n",
        "from configurations import Configurations\n",
        "from visualization_helpers import (\n",
        "    visualize_hyperparameter_tuning,\n",
        "    visualize_final_results,\n",
        "    visualize_val_test_comparison\n",
        ")\n",
        "from evaluation_metrics import (\n",
        "       compute_rmse_accuracy,\n",
        "       recall_at_k,\n",
        "       ndcg_at_k,\n",
        "       map_at_k\n",
        ")\n",
        "\n",
        "logger = Logger(process_name=\"model_based\", log_file=Configurations.LOG_PATH)\n",
        "PROCESSED_DIR = Path(Configurations.DATA_PROCESSED_PATH)\n",
        "RAW_DIR = Path(Configurations.DATA_RAW_PATH)\n",
        "MODELS_DIR = Path(Configurations.MODELS_PATH)\n",
        "\n",
        "CATEGORY = Configurations.CATEGORIES\n",
        "\n",
        "# Auto-detect phase\n",
        "has_tuning = Configurations.has_tuning_results_model(CATEGORY[0])\n",
        "FACTORS_VALUES = Configurations.FACTORS_VALUES  # [10, 20, 30, 50, 75, 100]\n",
        "\n",
        "if has_tuning:\n",
        "    PHASE = 'final'\n",
        "    logger.log_info(\"=\"*70)\n",
        "    logger.log_info(\"PHASE: FINAL EVALUATION (MODEL-BASED)\")\n",
        "    logger.log_info(\"=\"*70)\n",
        "    \n",
        "    for cat in CATEGORY:\n",
        "        best_factors = Configurations.load_best_factors(cat)\n",
        "        logger.log_info(f\"  {cat}: Best n_factors = {best_factors}\")\n",
        "else:\n",
        "    PHASE = 'train_tune'\n",
        "    logger.log_info(\"=\"*70)\n",
        "    logger.log_info(\"PHASE: TRAINING + TUNING (MODEL-BASED)\")\n",
        "    logger.log_info(\"=\"*70)\n",
        "    logger.log_info(f\"n_factors values: {FACTORS_VALUES}\")\n",
        "\n",
        "logger.log_info(\"=\"*70)\n",
        "logger.log_info(f\"Categories: {CATEGORY}\")\n",
        "logger.log_info(f\"Sample size: {Configurations.DEV_SAMPLE_SIZE}\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Settings\n",
        "N_RECS = 10\n",
        "MEAN_CENTER = True\n",
        "MAX_USERS = None\n",
        "MAX_ITEMS = None\n",
        "N_FACTORS = 50  # Default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task: Define functions for CF recommendation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "\n",
        "def _candidate_files(category: str, split: str = \"train\"):\n",
        "    dev_sample_size = Configurations.DEV_SAMPLE_SIZE\n",
        "\n",
        "    if dev_sample_size != 'full':\n",
        "        sample_sizes = Configurations.SAMPLE_SIZES\n",
        "        for size_name in sample_sizes.keys():\n",
        "            if size_name == dev_sample_size:\n",
        "             return PROCESSED_DIR / f\"{category.replace('/', '-')}.5core.{split}.{size_name}.parquet\"\n",
        "    else:\n",
        "        return PROCESSED_DIR / f\"{category.replace('/', '-')}.5core.{split}.parquet\"\n",
        "\n",
        "def load_5core_data(category: str, split: str = \"train\") -> pl.DataFrame:\n",
        "    p = _candidate_files(category, split)\n",
        "    \n",
        "    df = pl.read_parquet(p, low_memory=False)\n",
        "    df = df.with_columns([\n",
        "        pl.when(pl.col(\"rating\").cast(pl.Float32) < 1.0).then(1.0)\n",
        "            .when(pl.col(\"rating\").cast(pl.Float32) > 5.0).then(5.0)\n",
        "            .otherwise(pl.col(\"rating\").cast(pl.Float32)).alias(\"rating\")\n",
        "    ])\n",
        "    logger.log_info(f\"[Load-{split.upper()}] shape={df.shape} | users={df['user_id'].n_unique()} | items={df['parent_asin'].n_unique()}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Build Matrix, train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_matrix_model(df_train: pl.DataFrame, max_users: int | None = None, \n",
        "                      max_items: int | None = None):\n",
        "    df = df_train.drop_nulls(subset=['user_id', 'parent_asin', 'rating'])\n",
        "    \n",
        "    if max_users is not None:\n",
        "        keep_users = df['user_id'].unique().to_list()[:max_users]\n",
        "        df = df.filter(pl.col('user_id').is_in(keep_users))\n",
        "    if max_items is not None:\n",
        "        keep_items = df['parent_asin'].unique().to_list()[:max_items]\n",
        "        df = df.filter(pl.col('parent_asin').is_in(keep_items))\n",
        "    \n",
        "    user_rev = np.array(df['user_id'].unique().to_list(), dtype=object)\n",
        "    item_rev = np.array(df['parent_asin'].unique().to_list(), dtype=object)\n",
        "    user_idx = {uid: idx for idx, uid in enumerate(user_rev)}\n",
        "    item_idx = {iid: idx for idx, iid in enumerate(item_rev)}\n",
        "    \n",
        "    u = np.array([user_idx[x] for x in df['user_id'].to_list()], dtype=np.int32)\n",
        "    i = np.array([item_idx[x] for x in df['parent_asin'].to_list()], dtype=np.int32)\n",
        "    v = np.array(df['rating'].to_list(), dtype=np.float32)\n",
        "    \n",
        "    nU, nI = user_rev.size, item_rev.size\n",
        "    R = csr_matrix((v, (u, i)), shape=(nU, nI), dtype=np.float32)\n",
        "    \n",
        "    logger.log_info(f\"[Matrix-Model] R{R.shape} nnz={R.nnz}\")\n",
        "    return R, user_idx, item_idx, user_rev, item_rev\n",
        "\n",
        "\n",
        "def train_svd_model(R_train, n_factors=50):\n",
        "    n_users, n_items = R_train.shape\n",
        "    max_factors = min(n_users, n_items) - 1  # SVD constraint: k < min(m,n)\n",
        "    \n",
        "    original_factors = n_factors\n",
        "    \n",
        "    # Auto-adjust if too large\n",
        "    if n_factors >= max_factors:\n",
        "        n_factors = max(1, max_factors)  # At least 1 factor\n",
        "        logger.log_warning(f\"n_factors={original_factors} too large for matrix {R_train.shape}\")\n",
        "        logger.log_warning(f\"Auto-adjusted to n_factors={n_factors}\")\n",
        "    \n",
        "    # Validate adjusted value\n",
        "    if n_factors < 1:\n",
        "        raise ValueError(\n",
        "            f\"Matrix too small {R_train.shape}! Cannot perform SVD. \"\n",
        "            f\"Need larger dataset (use 'small' or 'medium' size).\"\n",
        "        )\n",
        "\n",
        "    logger.log_info(f\"Training SVD: matrix {R_train.shape} with {n_factors} factors...\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Perform truncated SVD\n",
        "        U, sigma, Vt = svds(R_train, k=n_factors)\n",
        "        \n",
        "        # Reverse order (svds returns smallest singular values first)\n",
        "        U = U[:, ::-1]\n",
        "        sigma = sigma[::-1]\n",
        "        Vt = Vt[::-1, :]\n",
        "        \n",
        "        # Convert sigma to diagonal matrix\n",
        "        sigma_diag = np.diag(sigma)\n",
        "        train_time = time.time() - start_time\n",
        "        \n",
        "        logger.log_info(f\"SVD completed in {train_time:.2f}s\")\n",
        "        logger.log_info(f\"  U shape:  {U.shape}\")\n",
        "        logger.log_info(f\"  Vt shape: {Vt.shape}\")\n",
        "        logger.log_info(f\"  Singular values: [{sigma.min():.4f}, {sigma.max():.4f}]\")\n",
        "        \n",
        "        return U, Vt, sigma_diag\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"SVD training failed: {e}\")\n",
        "        logger.log_error(f\"Matrix info: shape={R_train.shape}, nnz={R_train.nnz}, n_factors={n_factors}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save/Load Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model_artifacts(out_dir: Path, R, U, V, user_rev, item_rev, user_idx, item_idx):\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    save_npz(out_dir / \"R.npz\", R)\n",
        "    np.save(out_dir / \"U.npy\", U)\n",
        "    np.save(out_dir / \"V.npy\", V)\n",
        "    with open(out_dir / \"user_rev.pkl\", \"wb\") as f: pickle.dump(user_rev, f)\n",
        "    with open(out_dir / \"item_rev.pkl\", \"wb\") as f: pickle.dump(item_rev, f)\n",
        "    (out_dir / \"user_idx.json\").write_text(json.dumps({str(k): int(v) for k, v in user_idx.items()}))\n",
        "    (out_dir / \"item_idx.json\").write_text(json.dumps({str(k): int(v) for k, v in item_idx.items()}))\n",
        "    logger.log_info(f\"[Saved-Model] {out_dir}\")\n",
        "\n",
        "def load_model_artifacts(model_dir: str | Path):\n",
        "    md = Path(model_dir)\n",
        "    R = load_npz(md / \"R.npz\")\n",
        "    U = np.load(md / \"U.npy\")\n",
        "    V = np.load(md / \"V.npy\")\n",
        "    with open(md / \"user_rev.pkl\", \"rb\") as f: user_rev = pickle.load(f)\n",
        "    with open(md / \"item_rev.pkl\", \"rb\") as f: item_rev = pickle.load(f)\n",
        "    user_idx = {k: int(v) for k, v in json.loads((md / \"user_idx.json\").read_text()).items()}\n",
        "    item_idx = {k: int(v) for k, v in json.loads((md / \"item_idx.json\").read_text()).items()}\n",
        "    return dict(R=R, U=U, V=V, user_rev=user_rev, item_rev=item_rev, user_idx=user_idx, item_idx=item_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model_based_for_categories(categories, n_factors=50, models_dir=None, max_users=None, max_items=None):\n",
        "    base = Path(models_dir) if models_dir else MODELS_DIR\n",
        "    out_algo = base / \"model\"\n",
        "    out_algo.mkdir(parents=True, exist_ok=True)\n",
        "    rows = []\n",
        "    for cat in categories:\n",
        "        out_dir = out_algo / cat\n",
        "        if out_dir.exists() and (out_dir / \"U.npy\").exists():\n",
        "            logger.log_info(f\"[Skip] Model based model exists for {cat}\")\n",
        "            rows.append({\"category\": cat, \"algo\": \"model\", \"models_dir\": str(out_dir), \"n_factors\": n_factors, \"status\": \"skipped\"})\n",
        "            continue\n",
        "        try:\n",
        "            logger.log_info(f\"[MODEL] Training {cat}\")\n",
        "            df_train = load_5core_data(cat, split=\"train\")\n",
        "            R, user_idx, item_idx, user_rev, item_rev = build_matrix_model(df_train, max_users=max_users, max_items=max_items)\n",
        "            svd = TruncatedSVD(n_components=n_factors, random_state=42)\n",
        "            U = svd.fit_transform(R)\n",
        "            V = svd.components_.T\n",
        "            save_model_artifacts(out_dir, R, U, V, user_rev, item_rev, user_idx, item_idx)\n",
        "            rows.append({\"category\": cat, \"algo\": \"model\", \"models_dir\": str(out_dir), \"n_factors\": n_factors, \"users\": len(user_rev), \"items\": len(item_rev), \"R_nnz\": int(R.nnz)})\n",
        "        except Exception as e:\n",
        "            logger.log_exception(f\"[Error-MODEL] {cat}: {e}\")\n",
        "            rows.append({\"category\": cat, \"algo\": \"model\", \"models_dir\": None, \"n_factors\": n_factors, \"error\": str(e)})\n",
        "    summary = pl.DataFrame(rows)\n",
        "    ok_count = sum(1 for r in rows if r.get(\"models_dir\"))\n",
        "    logger.log_info(f\"[Summary-MODEL] Total={len(rows)} OK={ok_count} FAIL={len(rows)-ok_count}\")\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prediction & Recommendation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_model_based(user_idx_val: int, U: np.ndarray, V: np.ndarray, R: csr_matrix) -> np.ndarray:\n",
        "    scores = U[user_idx_val] @ V\n",
        "    return scores\n",
        "\n",
        "def recommend_model_based(user_id: str, n_recs: int, artifacts: dict) -> pl.DataFrame:\n",
        "    U, V = artifacts['U'], artifacts['V']\n",
        "    R = artifacts['R']\n",
        "    user_idx = artifacts['user_idx']\n",
        "    item_rev = artifacts['item_rev']\n",
        "    if user_id not in user_idx:\n",
        "        logger.log_warning(f\"[Recommend] user_id={user_id} not found.\")\n",
        "        return pl.DataFrame(columns=[\"parent_asin\", \"score\"])\n",
        "    u = user_idx[user_id]\n",
        "    scores = predict_model_based(u, U, V, R)\n",
        "    rated = set(R.getrow(u).indices.tolist())\n",
        "    cand_mask = np.ones(len(scores), dtype=bool)\n",
        "    if rated:\n",
        "        cand_mask[list(rated)] = False\n",
        "    cand_scores = scores[cand_mask]\n",
        "    if cand_scores.size == 0:\n",
        "        return pl.DataFrame(columns=[\"parent_asin\", \"score\"])\n",
        "    n_top = min(n_recs, cand_scores.size)\n",
        "    cand_indices = np.nonzero(cand_mask)[0]\n",
        "    top_pos = np.argpartition(-cand_scores, n_top - 1)[:n_top]\n",
        "    picked = sorted([(int(cand_indices[p]), float(cand_scores[p])) for p in top_pos], key=lambda x: -x[1])\n",
        "    rec_asins = [item_rev[i] for i, _ in picked]\n",
        "    rec_scores = [s for _, s in picked]\n",
        "    return pl.DataFrame({\"parent_asin\": rec_asins, \"score\": rec_scores})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model_based(category: str, artifacts: dict, k_values: list = [10, 20, 50], \n",
        "                        split: str = \"test\", sample_users: int = 3000):\n",
        "    \n",
        "    logger.log_info(f\"[Eval-Model] {category} on {split.upper()}\")\n",
        "    \n",
        "    df_eval = load_5core_data(category, split=split)\n",
        "    U, V, R = artifacts['U'], artifacts['V'], artifacts['R']\n",
        "    user_idx, item_idx = artifacts['user_idx'], artifacts['item_idx']\n",
        "    \n",
        "    # Filter to train users only\n",
        "    train_user_list = list(user_idx.keys())\n",
        "    df_eval = df_eval.filter(pl.col('user_id').is_in(train_user_list))\n",
        "    \n",
        "    if len(df_eval) == 0:\n",
        "        logger.log_warning(f\"[Eval-Model] No data after filtering\")\n",
        "        return None\n",
        "    \n",
        "    logger.log_info(f\"[Eval-Model] After filtering: {len(df_eval):,} ratings, {df_eval['user_id'].n_unique():,} users\")\n",
        "    \n",
        "    # Sample users for evaluation\n",
        "    eval_users = df_eval['user_id'].unique().to_list()\n",
        "    if len(eval_users) > sample_users:\n",
        "        np.random.seed(42)\n",
        "        eval_users = np.random.choice(eval_users, sample_users, replace=False).tolist()\n",
        "    \n",
        "    logger.log_info(f\"[Eval-Model] Evaluating {len(eval_users)} users...\")\n",
        "    \n",
        "    # Initialize accumulators\n",
        "    metrics_acc = {\n",
        "        'rmse': [], 'accuracy': [],\n",
        "        **{f'recall@{k}': [] for k in k_values},\n",
        "        **{f'ndcg@{k}': [] for k in k_values},\n",
        "        **{f'map@{k}': [] for k in k_values}\n",
        "    }\n",
        "    \n",
        "    evaluated_users = 0\n",
        "    \n",
        "    for user_id in eval_users:\n",
        "        if user_id not in user_idx:\n",
        "            continue\n",
        "        \n",
        "        u = user_idx[user_id]\n",
        "        user_eval = df_eval.filter(pl.col('user_id') == user_id)\n",
        "        actual_items = set(user_eval['parent_asin'].to_list())\n",
        "        actual_ratings = {row['parent_asin']: row['rating'] \n",
        "                         for row in user_eval.iter_rows(named=True)}\n",
        "        \n",
        "        if len(actual_items) == 0:\n",
        "            continue\n",
        "        \n",
        "        known_items = {item for item in actual_items if item in item_idx}\n",
        "        \n",
        "        # Skip user if ALL items are unknown\n",
        "        if len(known_items) == 0:\n",
        "            continue\n",
        "        \n",
        "        evaluated_users += 1\n",
        "        scores = predict_model_based(u, U, V, R)\n",
        "        \n",
        "        # RMSE & Accuracy - only for known items\n",
        "        predictions = np.full(R.shape[1], np.nan)\n",
        "        actuals = np.full(R.shape[1], np.nan)\n",
        "        for asin in known_items:\n",
        "            idx = item_idx[asin]\n",
        "            predictions[idx] = scores[idx]\n",
        "            actuals[idx] = actual_ratings[asin]\n",
        "        \n",
        "        rmse, acc = compute_rmse_accuracy(predictions, actuals)\n",
        "        if not np.isnan(rmse):\n",
        "            metrics_acc['rmse'].append(rmse)\n",
        "            metrics_acc['accuracy'].append(acc)\n",
        "        \n",
        "        # Ranking metrics\n",
        "        rated = set(R.getrow(u).indices.tolist())\n",
        "        cand_mask = np.ones(R.shape[1], dtype=bool)\n",
        "        if rated:\n",
        "            cand_mask[list(rated)] = False\n",
        "        \n",
        "        cand_scores = scores[cand_mask]\n",
        "        if cand_scores.size == 0:\n",
        "            continue\n",
        "        \n",
        "        cand_indices = np.nonzero(cand_mask)[0]\n",
        "        max_k = max(k_values)\n",
        "        n_top = min(max_k, cand_scores.size)\n",
        "        top_pos = np.argpartition(-cand_scores, n_top - 1)[:n_top]\n",
        "        sorted_idx = top_pos[np.argsort(-cand_scores[top_pos])]\n",
        "        \n",
        "        item_rev = artifacts['item_rev']\n",
        "        recommended = [item_rev[cand_indices[i]] for i in sorted_idx]\n",
        "        \n",
        "        # Use known_items for ranking metrics\n",
        "        for k in k_values:\n",
        "            metrics_acc[f'recall@{k}'].append(recall_at_k(recommended, known_items, k))\n",
        "            metrics_acc[f'ndcg@{k}'].append(ndcg_at_k(recommended, known_items, k))\n",
        "            metrics_acc[f'map@{k}'].append(map_at_k(recommended, known_items, k))\n",
        "    \n",
        "    logger.log_info(f\"[Eval-Model] Actually evaluated: {evaluated_users} users\")\n",
        "    \n",
        "    # Aggregate results\n",
        "    results = {\n",
        "        'category': category,\n",
        "        'split': split,\n",
        "        'n_users': evaluated_users,\n",
        "        'rmse': np.mean(metrics_acc['rmse']) if metrics_acc['rmse'] else np.nan,\n",
        "        'accuracy': np.mean(metrics_acc['accuracy']) if metrics_acc['accuracy'] else np.nan\n",
        "    }\n",
        "    \n",
        "    for k in k_values:\n",
        "        for metric in ['recall', 'ndcg', 'map']:\n",
        "            key = f'{metric}@{k}'\n",
        "            results[key] = np.mean(metrics_acc[key]) if metrics_acc[key] else 0.0\n",
        "    \n",
        "    logger.log_info(f\"[Eval-Model] RMSE={results['rmse']:.4f}, Acc={results['accuracy']:.4f}\")\n",
        "    logger.log_info(f\"[Eval-Model] NDCG@10={results['ndcg@10']:.4f}, Recall@10={results['recall@10']:.4f}\")\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tune_n_factors_single_category(category: str, factors_values: list, n_eval_users: int):\n",
        "    logger.log_info(f\"\\n{'='*70}\")\n",
        "    logger.log_info(f\"TUNING N_FACTORS: {category}\")\n",
        "    logger.log_info(f\"{'='*70}\")\n",
        "    logger.log_info(f\"n_factors values: {factors_values}\")\n",
        "    logger.log_info(f\"Validation users: {n_eval_users}\\n\")\n",
        "    \n",
        "    # Load training data ONCE\n",
        "    logger.log_info(\"Loading training data...\")\n",
        "    df_train = load_5core_data(category, split=\"train\")\n",
        "    \n",
        "    # Build base matrix ONCE\n",
        "    logger.log_info(\"Building rating matrix...\")\n",
        "    R, user_idx, item_idx, user_rev, item_rev = build_matrix_model(\n",
        "        df_train, max_users=MAX_USERS, max_items=MAX_ITEMS\n",
        "    )\n",
        "    \n",
        "    # Prepare base artifacts for evaluation\n",
        "    base_artifacts = {\n",
        "        'R': R,\n",
        "        'user_rev': user_rev,\n",
        "        'item_rev': item_rev,\n",
        "        'user_idx': user_idx,\n",
        "        'item_idx': item_idx\n",
        "    }\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    # Test each n_factors value\n",
        "    for i, n_factors in enumerate(factors_values, 1):\n",
        "        logger.log_info(f\"\\n[{i}/{len(factors_values)}] Testing n_factors={n_factors}\")\n",
        "        logger.log_info(\"-\"*70)\n",
        "        \n",
        "        # Train SVD with this n_factors\n",
        "        start_time = time.time()\n",
        "        U, V, train_time = train_svd_model(R, n_factors=n_factors)\n",
        "        \n",
        "        # Create artifacts for this configuration\n",
        "        eval_artifacts = base_artifacts.copy()\n",
        "        eval_artifacts['U'] = U\n",
        "        eval_artifacts['V'] = V\n",
        "        eval_artifacts['n_factors'] = n_factors\n",
        "        \n",
        "        # Evaluate on validation\n",
        "        start_time = time.time()\n",
        "        metrics = evaluate_model_based(\n",
        "            category, eval_artifacts,\n",
        "            k_values=[10, 20, 50],\n",
        "            split=\"valid\",\n",
        "            sample_users=n_eval_users\n",
        "        )\n",
        "        eval_time = time.time() - start_time\n",
        "        \n",
        "        if metrics:\n",
        "            # Store all results\n",
        "            result = {\n",
        "                'n_factors': n_factors,\n",
        "                'NDCG@10': metrics['ndcg@10'],\n",
        "                'NDCG@20': metrics['ndcg@20'],\n",
        "                'NDCG@50': metrics['ndcg@50'],\n",
        "                'Recall@10': metrics['recall@10'],\n",
        "                'Recall@20': metrics['recall@20'],\n",
        "                'Recall@50': metrics['recall@50'],\n",
        "                'MAP@10': metrics['map@10'],\n",
        "                'MAP@20': metrics['map@20'],\n",
        "                'MAP@50': metrics['map@50'],\n",
        "                'RMSE': metrics['rmse'],\n",
        "                'Accuracy': metrics['accuracy'],\n",
        "                'Train_Time': train_time,\n",
        "                'Eval_Time': eval_time\n",
        "            }\n",
        "            results.append(result)\n",
        "            \n",
        "            # Print summary\n",
        "            logger.log_info(f\"Results:\")\n",
        "            logger.log_info(f\"  NDCG@10:   {metrics['ndcg@10']:.4f}\")\n",
        "            logger.log_info(f\"  Recall@10: {metrics['recall@10']:.4f}\")\n",
        "            logger.log_info(f\"  MAP@10:    {metrics['map@10']:.4f}\")\n",
        "            logger.log_info(f\"  RMSE:      {metrics['rmse']:.4f}\")\n",
        "            logger.log_info(f\"  Train:     {train_time:.1f}s\")\n",
        "            logger.log_info(f\"  Eval:      {eval_time:.1f}s\")\n",
        "    \n",
        "    df_results = pl.DataFrame(results)\n",
        "    \n",
        "    # Save results\n",
        "    out_csv = MODELS_DIR / 'model' / f'tuning_{category}.csv'\n",
        "    df_results.write_csv(out_csv)\n",
        "    logger.log_info(f\"\\nSaved tuning results: {out_csv}\")\n",
        "    \n",
        "    return df_results\n",
        "\n",
        "\n",
        "def select_best_factors(df_results: pl.DataFrame):\n",
        "    \"\"\"Select best n_factors using NDCG-primary strategy\"\"\"\n",
        "    \n",
        "    # FIND column 'K':\n",
        "    best_factors_ndcg = df_results['n_factors'][df_results['NDCG@10'].arg_max()]  # ← Not 'K'!\n",
        "    best_ndcg = df_results['NDCG@10'].max()\n",
        "    \n",
        "    logger.log_info(f\"\\nPrimary metric (NDCG@10): n_factors={best_factors_ndcg}, score={best_ndcg:.4f}\")\n",
        "    \n",
        "    threshold = best_ndcg * 0.98\n",
        "    similar_rows = df_results.filter(pl.col('NDCG@10') >= threshold)\n",
        "    similar_factors = similar_rows['n_factors'].to_list()\n",
        "    \n",
        "    if len(similar_factors) > 1:\n",
        "        # Prefer SMALLER n_factors (faster, less overfitting)\n",
        "        best_factors = min(similar_factors)\n",
        "        logger.log_info(f\"Selected n_factors={best_factors} (smallest among similar)\")\n",
        "    else:\n",
        "        best_factors = best_factors_ndcg\n",
        "    \n",
        "    return best_factors\n",
        "\n",
        "\n",
        "def analyze_and_select_factors(df_results: pl.DataFrame, category: str):\n",
        "    \"\"\"Analyze results and select best n_factors\"\"\"\n",
        "    logger.log_info(f\"\\n{'='*70}\")\n",
        "    logger.log_info(\"N_FACTORS SELECTION ANALYSIS\")\n",
        "    logger.log_info(f\"{'='*70}\")\n",
        "    \n",
        "    # Best n_factors for each metric\n",
        "    best_ndcg = df_results['n_factors'][df_results['NDCG@10'].arg_max()]\n",
        "    best_recall = df_results['n_factors'][df_results['Recall@10'].arg_max()]\n",
        "    best_map = df_results['n_factors'][df_results['MAP@10'].arg_max()]\n",
        "    best_rmse = df_results['n_factors'][df_results['RMSE'].arg_min()]\n",
        "    \n",
        "    logger.log_info(f\"\\nBest n_factors by metric:\")\n",
        "    logger.log_info(f\"  NDCG@10:   n_factors={best_ndcg:3d} (score={df_results['NDCG@10'].max():.4f})\")\n",
        "    logger.log_info(f\"  Recall@10: n_factors={best_recall:3d} (score={df_results['Recall@10'].max():.4f})\")\n",
        "    logger.log_info(f\"  MAP@10:    n_factors={best_map:3d} (score={df_results['MAP@10'].max():.4f})\")\n",
        "    logger.log_info(f\"  RMSE:      n_factors={best_rmse:3d} (score={df_results['RMSE'].min():.4f})\")\n",
        "    \n",
        "    # Select best\n",
        "    best_factors = select_best_factors(df_results)\n",
        "    \n",
        "    # Show final selection details\n",
        "    final_row = df_results.filter(pl.col('n_factors') == best_factors).row(0, named=True)\n",
        "    \n",
        "    logger.log_info(f\"\\n{'='*70}\")\n",
        "    logger.log_info(f\"FINAL SELECTION: n_factors={best_factors}\")\n",
        "    logger.log_info(f\"{'='*70}\")\n",
        "    logger.log_info(f\"  NDCG@10:   {final_row['NDCG@10']:.4f}\")\n",
        "    logger.log_info(f\"  Recall@10: {final_row['Recall@10']:.4f}\")\n",
        "    logger.log_info(f\"  MAP@10:    {final_row['MAP@10']:.4f}\")\n",
        "    logger.log_info(f\"  RMSE:      {final_row['RMSE']:.4f}\")\n",
        "    logger.log_info(f\"  Accuracy:  {final_row['Accuracy']:.4f}\")\n",
        "    logger.log_info(f\"  Train:     {final_row['Train_Time']:.1f}s\")\n",
        "    logger.log_info(f\"{'='*70}\\n\")\n",
        "    \n",
        "    # Save to configuration\n",
        "    Configurations.save_best_factors(category, best_factors)\n",
        "    logger.log_info(f\"Saved best n_factors to file\\n\")\n",
        "    \n",
        "    return best_factors, df_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pipeline and execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _train_single_category(cat, model_dir, FACTORS_VALUES, n_eval_tune):  \n",
        "    # ========================================================================\n",
        "    # STEP 1: BUILD BASE MATRIX (if not exists)\n",
        "    # ========================================================================\n",
        "    \n",
        "    base_exists = (model_dir / \"R.npz\").exists()\n",
        "    \n",
        "    if not base_exists:\n",
        "        logger.log_info(\"STEP 1: BUILDING BASE MATRIX\")\n",
        "        logger.log_info(\"-\"*70)\n",
        "        \n",
        "        # Load training data\n",
        "        df_train = load_5core_data(cat, split=\"train\")\n",
        "        \n",
        "        # Build rating matrix\n",
        "        R, user_idx, item_idx, user_rev, item_rev = build_matrix_model(\n",
        "            df_train, max_users=MAX_USERS, max_items=MAX_ITEMS\n",
        "        )\n",
        "        \n",
        "        # Check matrix dimensions\n",
        "        n_users, n_items = R.shape\n",
        "        max_factors = min(n_users, n_items) - 1  # SVD constraint\n",
        "        \n",
        "        logger.log_info(f\"Matrix shape: {R.shape} (users × items)\")\n",
        "        logger.log_info(f\"Max allowed factors: {max_factors}\")\n",
        "        \n",
        "        # Warning if dataset too small\n",
        "        if max_factors < 10:\n",
        "            logger.log_warning(\"=\"*70)\n",
        "            logger.log_warning(f\"WARNING: Matrix too small ({R.shape})!\")\n",
        "            logger.log_warning(f\"Max factors: {max_factors} (very limited)\")\n",
        "            logger.log_warning(\"Recommendation: Use 'small' or 'medium' dataset size\")\n",
        "            logger.log_warning(\"=\"*70)\n",
        "        \n",
        "        # Adjust initial n_factors\n",
        "        initial_factors = min(50, max_factors)\n",
        "        logger.log_info(f\"Using n_factors={initial_factors} for base model\\n\")\n",
        "        \n",
        "        # Train with adjusted n_factors\n",
        "        U, V, _ = train_svd_model(R, n_factors=initial_factors)\n",
        "        \n",
        "        # Save artifacts\n",
        "        save_model_artifacts(model_dir, R, U, V, user_rev, item_rev, user_idx, item_idx)\n",
        "        logger.log_info(f\"Base model saved to {model_dir}\\n\")\n",
        "    \n",
        "    else:\n",
        "        logger.log_info(\"STEP 1: BASE MATRIX EXISTS\")\n",
        "        logger.log_info(\"-\"*70)\n",
        "        logger.log_info(f\"Loading from {model_dir}\\n\")\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 2: HYPERPARAMETER TUNING (if not done)\n",
        "    # ========================================================================\n",
        "    if not Configurations.has_tuning_results_model(cat):\n",
        "        logger.log_info(\"STEP 2: HYPERPARAMETER TUNING (VALIDATION)\")\n",
        "        logger.log_info(\"-\"*70)\n",
        "        \n",
        "        # Load base artifacts\n",
        "        artifacts = load_model_artifacts(model_dir)\n",
        "        R = artifacts['R']\n",
        "        n_users, n_items = R.shape\n",
        "        max_factors = min(n_users, n_items) - 1\n",
        "        \n",
        "        # Filter out invalid factor values\n",
        "        valid_factors = [f for f in FACTORS_VALUES if f <= max_factors]\n",
        "        \n",
        "        if len(valid_factors) == 0:\n",
        "            logger.log_warning(\"No valid n_factors values! Using max allowed.\")\n",
        "            valid_factors = [max_factors]\n",
        "        \n",
        "        if len(valid_factors) < len(FACTORS_VALUES):\n",
        "            skipped = [f for f in FACTORS_VALUES if f > max_factors]\n",
        "            logger.log_warning(f\"Skipped invalid n_factors: {skipped} (max={max_factors})\")\n",
        "        \n",
        "        logger.log_info(f\"Valid n_factors values: {valid_factors}\")\n",
        "        logger.log_info(f\"Validation users: {n_eval_tune}\\n\")\n",
        "        \n",
        "        # Prepare base artifacts\n",
        "        base_artifacts = {\n",
        "            'R': R,\n",
        "            'user_rev': artifacts['user_rev'],\n",
        "            'item_rev': artifacts['item_rev'],\n",
        "            'user_idx': artifacts['user_idx'],\n",
        "            'item_idx': artifacts['item_idx']\n",
        "        }\n",
        "        \n",
        "        results = []\n",
        "        \n",
        "        # Test each valid n_factors value\n",
        "        for i, n_factors in enumerate(valid_factors, 1):\n",
        "            logger.log_info(f\"\\n[{i}/{len(valid_factors)}] Testing n_factors={n_factors}\")\n",
        "            logger.log_info(\"-\"*70)\n",
        "            \n",
        "            # Train SVD\n",
        "            train_start = time.time()\n",
        "            U, V, _ = train_svd_model(R, n_factors=n_factors)\n",
        "            train_time = time.time() - train_start\n",
        "            \n",
        "            # Create artifacts for evaluation\n",
        "            eval_artifacts = base_artifacts.copy()\n",
        "            eval_artifacts['U'] = U\n",
        "            eval_artifacts['V'] = V\n",
        "            eval_artifacts['n_factors'] = n_factors\n",
        "            \n",
        "            # Evaluate on validation set\n",
        "            eval_start = time.time()\n",
        "            metrics = evaluate_model_based(\n",
        "                cat, eval_artifacts,\n",
        "                k_values=[10, 20, 50],\n",
        "                split=\"valid\",\n",
        "                sample_users=n_eval_tune\n",
        "            )\n",
        "            eval_time = time.time() - eval_start\n",
        "            \n",
        "            if metrics:\n",
        "                results.append({\n",
        "                    'n_factors': n_factors,\n",
        "                    'NDCG@10': metrics['ndcg@10'],\n",
        "                    'NDCG@20': metrics['ndcg@20'],\n",
        "                    'NDCG@50': metrics['ndcg@50'],\n",
        "                    'Recall@10': metrics['recall@10'],\n",
        "                    'Recall@20': metrics['recall@20'],\n",
        "                    'Recall@50': metrics['recall@50'],\n",
        "                    'MAP@10': metrics['map@10'],\n",
        "                    'MAP@20': metrics['map@20'],\n",
        "                    'MAP@50': metrics['map@50'],\n",
        "                    'RMSE': metrics['rmse'],\n",
        "                    'Accuracy': metrics['accuracy'],\n",
        "                    'Train_Time': train_time,\n",
        "                    'Eval_Time': eval_time\n",
        "                })\n",
        "                \n",
        "                logger.log_info(f\"NDCG@10: {metrics['ndcg@10']:.4f}, \"\n",
        "                              f\"Recall@10: {metrics['recall@10']:.4f}, \"\n",
        "                              f\"Train: {train_time:.1f}s\")\n",
        "        \n",
        "        # Check if we got any results\n",
        "        if len(results) == 0:\n",
        "            logger.log_error(\"No tuning results! Using max_factors as best.\")\n",
        "            best_factors = max_factors\n",
        "            \n",
        "            # Train final model with max_factors\n",
        "            U_best, V_best, _ = train_svd_model(R, n_factors=max_factors)\n",
        "            save_model_artifacts(model_dir, R, U_best, V_best,\n",
        "                               artifacts['user_rev'], artifacts['item_rev'],\n",
        "                               artifacts['user_idx'], artifacts['item_idx'])\n",
        "            \n",
        "            Configurations.save_best_factors(cat, best_factors)\n",
        "            \n",
        "            return {'tuned_now': True, 'best_factors': best_factors}\n",
        "        \n",
        "        # Save tuning results\n",
        "        df_results = pl.DataFrame(results)\n",
        "        df_results.write_csv(MODELS_DIR / 'model' / f'tuning_{cat}.csv')\n",
        "        logger.log_info(f\"\\nSaved tuning results to: tuning_{cat}.csv\")\n",
        "        \n",
        "        # Select best n_factors\n",
        "        best_factors = select_best_factors(df_results)\n",
        "        \n",
        "        # Log final selection\n",
        "        final_row = df_results.filter(pl.col('n_factors') == best_factors).row(0, named=True)\n",
        "        \n",
        "        logger.log_info(f\"\\n{'='*70}\")\n",
        "        logger.log_info(f\"BEST N_FACTORS SELECTED: {best_factors}\")\n",
        "        logger.log_info(f\"{'='*70}\")\n",
        "        logger.log_info(f\"  NDCG@10:   {final_row['NDCG@10']:.4f}\")\n",
        "        logger.log_info(f\"  Recall@10: {final_row['Recall@10']:.4f}\")\n",
        "        logger.log_info(f\"  MAP@10:    {final_row['MAP@10']:.4f}\")\n",
        "        logger.log_info(f\"  RMSE:      {final_row['RMSE']:.4f}\")\n",
        "        logger.log_info(f\"{'='*70}\\n\")\n",
        "        \n",
        "        # Save best n_factors to configuration\n",
        "        Configurations.save_best_factors(cat, best_factors)\n",
        "        logger.log_info(f\"Saved best n_factors to configuration\\n\")\n",
        "        \n",
        "        # Generate visualization\n",
        "        logger.log_info(\"Generating hyperparameter tuning plot...\")\n",
        "        visualize_hyperparameter_tuning(\n",
        "            df_results,\n",
        "            category=cat,\n",
        "            param_col='n_factors',\n",
        "            param_name='n_factors (Latent Dimensions)',\n",
        "            save_dir=MODELS_DIR / 'model',\n",
        "            algo_name='Model-Based'\n",
        "        )\n",
        "        logger.log_info(f\"Saved: factors_tuning_{cat}.png\\n\")\n",
        "        \n",
        "        # Re-train with best n_factors and save\n",
        "        logger.log_info(f\"Re-training final model with n_factors={best_factors}...\")\n",
        "        U_best, V_best, _ = train_svd_model(R, n_factors=best_factors)\n",
        "        save_model_artifacts(model_dir, R, U_best, V_best,\n",
        "                           artifacts['user_rev'], artifacts['item_rev'],\n",
        "                           artifacts['user_idx'], artifacts['item_idx'])\n",
        "        logger.log_info(f\"Final model saved to {model_dir}\\n\")\n",
        "        \n",
        "        return {'tuned_now': True, 'best_factors': best_factors}\n",
        "    \n",
        "    else:\n",
        "        # Tuning already done - load from configuration\n",
        "        best_factors = Configurations.load_best_factors(cat)\n",
        "        logger.log_info(\"STEP 2: TUNING ALREADY DONE\")\n",
        "        logger.log_info(\"-\"*70)\n",
        "        logger.log_info(f\"Best n_factors (loaded): {best_factors}\\n\")\n",
        "        \n",
        "        return {'tuned_now': False, 'best_factors': best_factors}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Phase 1: Training + Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PHASE 1: TRAINING + TUNING ALL CATEGORIES\n",
        "# ============================================================================\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"PHASE 1: TRAINING + TUNING ALL CATEGORIES (MODEL-BASED)\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Configuration\n",
        "if not Configurations.has_tuning_results_model(CATEGORY[0]):\n",
        "    FACTORS_VALUES = Configurations.FACTORS_VALUES  # [10, 20, 30, 50, 75, 100]\n",
        "    logger.log_info(f\"n_factors to test: {FACTORS_VALUES}\\n\")\n",
        "\n",
        "workflow_results = {}\n",
        "\n",
        "for cat in CATEGORY:\n",
        "    logger.log_info(f\"\\n{'='*70}\\nCATEGORY: {cat}\\n{'='*70}\\n\")\n",
        "    \n",
        "    model_dir = MODELS_DIR / \"model\" / cat\n",
        "    workflow_results[cat] = _train_single_category(\n",
        "        cat, model_dir, FACTORS_VALUES, Configurations.get_eval_samples_tuning()\n",
        "    )\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"PHASE 1 COMPLETE: ALL MODELS TRAINED AND TUNED\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "logger.log_info(\"Tuning Summary:\")\n",
        "for cat in CATEGORY:\n",
        "    status = 'newly tuned' if workflow_results[cat]['tuned_now'] else 'loaded from cache'\n",
        "    logger.log_info(f\"  {cat}: n_factors={workflow_results[cat]['best_factors']} ({status})\")\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"Ready for Phase 2: Final Evaluation\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Phase 2: Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PHASE 2: FINAL EVALUATION ON TEST SET (ALL CATEGORIES)\n",
        "# ============================================================================\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"PHASE 2: FINAL EVALUATION ON TEST SET (MODEL-BASED)\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD WORKFLOW RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "if 'workflow_results' not in locals():\n",
        "    workflow_results = {}\n",
        "    for cat in CATEGORY:\n",
        "        best_factors = Configurations.load_best_factors(cat)\n",
        "        workflow_results[cat] = {'best_factors': best_factors, 'tuned_now': False}\n",
        "    logger.log_info(\"Loaded best n_factors from configuration\\n\")\n",
        "\n",
        "n_eval_final = Configurations.get_eval_samples_final()\n",
        "logger.log_info(f\"Test users per category: {n_eval_final}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# RUN TEST EVALUATION FOR ALL CATEGORIES\n",
        "# ============================================================================\n",
        "\n",
        "for cat in CATEGORY:\n",
        "    logger.log_info(f\"\\n{'='*70}\\nTESTING: {cat}\\n{'='*70}\\n\")\n",
        "    \n",
        "    model_dir = MODELS_DIR / \"model\" / cat\n",
        "    best_factors = workflow_results[cat]['best_factors']\n",
        "    logger.log_info(f\"Using n_factors: {best_factors}\")\n",
        "    \n",
        "    # Load model\n",
        "    final_artifacts = load_model_artifacts(model_dir)\n",
        "    \n",
        "    logger.log_info(\"Evaluating on test set...\\n\")\n",
        "    results = evaluate_model_based(cat, final_artifacts, k_values=[10, 20, 50],\n",
        "                                  split=\"test\", sample_users=n_eval_final)\n",
        "    \n",
        "    if results:\n",
        "        workflow_results[cat]['test_results'] = results\n",
        "        logger.log_info(f\"\\nTest Results (n_factors={best_factors}):\")\n",
        "        logger.log_info(f\"  NDCG@10: {results['ndcg@10']:.4f}, \"\n",
        "                       f\"Recall@10: {results['recall@10']:.4f}, \"\n",
        "                       f\"MAP@10: {results['map@10']:.4f}\")\n",
        "        logger.log_info(f\"  RMSE: {results['rmse']:.4f}, \"\n",
        "                       f\"Accuracy: {results['accuracy']:.4f}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# SAVE FINAL RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"SAVING FINAL RESULTS\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "test_results_list = [workflow_results[cat]['test_results'] \n",
        "                     for cat in CATEGORY \n",
        "                     if 'test_results' in workflow_results[cat]]\n",
        "\n",
        "if test_results_list:\n",
        "    df_final_results = pl.DataFrame(test_results_list)\n",
        "    \n",
        "    logger.log_info(\"Final Test Results:\")\n",
        "    display(df_final_results)\n",
        "    \n",
        "    out_csv = MODELS_DIR / 'model' / 'final_test_results.csv'\n",
        "    df_final_results.write_csv(out_csv)\n",
        "    logger.log_info(f\"\\nSaved: {out_csv}\")\n",
        "    \n",
        "    logger.log_info(\"Generating final evaluation plot...\")\n",
        "    visualize_final_results(\n",
        "                            test_results_list,\n",
        "                            save_dir=MODELS_DIR / 'model',\n",
        "                            algo_name='Model-Based',\n",
        "                            k_values=[10, 20, 50]\n",
        "                        )\n",
        "    logger.log_info(f\"Saved: evaluation_results.png\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# POST-ANALYSIS VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"POST-ANALYSIS VISUALIZATION\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "for cat in CATEGORY:\n",
        "    tuning_csv = MODELS_DIR / 'model' / f'tuning_{cat}.csv'\n",
        "    \n",
        "    if not tuning_csv.exists():\n",
        "        logger.log_info(f\"No tuning results for {cat}, skipping\\n\")\n",
        "        continue\n",
        "    \n",
        "    if 'test_results' not in workflow_results[cat]:\n",
        "        continue\n",
        "    \n",
        "    logger.log_info(f\"Generating Val vs Test comparison for {cat}...\")\n",
        "    \n",
        "    df_tuning = pl.read_csv(tuning_csv)\n",
        "    best_factors = workflow_results[cat]['best_factors']\n",
        "    \n",
        "    tuning_row = df_tuning.filter(pl.col('n_factors') == best_factors).row(0, named=True)\n",
        "    final_row = df_final_results.filter(pl.col('category') == cat).row(0, named=True)\n",
        "    \n",
        "    visualize_val_test_comparison(\n",
        "                                    cat=cat,\n",
        "                                    param_val=best_factors,\n",
        "                                    tuning_row=tuning_row,\n",
        "                                    final_row=final_row,\n",
        "                                    save_dir=MODELS_DIR / 'model',\n",
        "                                    param_name='n_factors',\n",
        "                                    algo_name='Model-Based'\n",
        "                                )\n",
        "    logger.log_info(f\"  Saved: val_vs_test_{cat}.png\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL WORKFLOW SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"COMPLETE WORKFLOW SUMMARY (MODEL-BASED)\")\n",
        "logger.log_info(\"=\"*70)\n",
        "\n",
        "for cat in CATEGORY:\n",
        "    logger.log_info(f\"\\n{cat}: n_factors={workflow_results[cat]['best_factors']}\")\n",
        "    if 'test_results' in workflow_results[cat]:\n",
        "        test = workflow_results[cat]['test_results']\n",
        "        logger.log_info(f\"  NDCG@10: {test['ndcg@10']:.4f}, \"\n",
        "                       f\"Recall@10: {test['recall@10']:.4f}, \"\n",
        "                       f\"MAP@10: {test['map@10']:.4f}\")\n",
        "\n",
        "logger.log_info(\"\\n\" + \"=\"*70)\n",
        "logger.log_info(\"ALL PHASES COMPLETE\")\n",
        "logger.log_info(\"=\"*70)\n",
        "logger.log_info(\"\\nGenerated files:\")\n",
        "logger.log_info(\"  Phase 1: tuning_[category].csv, factors_tuning_[category].png\")\n",
        "logger.log_info(\"  Phase 2: final_test_results.csv, evaluation_results.png, val_vs_test_[category].png\")\n",
        "logger.log_info(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Debug info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_model_quality(category: str):\n",
        "    logger.log_info(f\"\\n{'='*70}\")\n",
        "    logger.log_info(f\"MODEL QUALITY CHECK: {category}\")\n",
        "    logger.log_info(f\"{'='*70}\\n\")\n",
        "    \n",
        "    # Load data\n",
        "    df_train = load_5core_data(category, split='train')\n",
        "    df_valid = load_5core_data(category, split='valid')\n",
        "    df_test = load_5core_data(category, split='test')\n",
        "    \n",
        "    def get_stats(df, name):\n",
        "        n_ratings = len(df)\n",
        "        n_users = df['user_id'].n_unique()\n",
        "        n_items = df['parent_asin'].n_unique()\n",
        "        sparsity = 1 - (n_ratings / (n_users * n_items))\n",
        "        \n",
        "        logger.log_info(f\"{name}:\")\n",
        "        logger.log_info(f\"  Ratings: {n_ratings:7,}\")\n",
        "        logger.log_info(f\"  Users:   {n_users:7,}\")\n",
        "        logger.log_info(f\"  Items:   {n_items:7,}\")\n",
        "        logger.log_info(f\"  Sparsity: {sparsity:6.2%}\\n\")\n",
        "        \n",
        "        return {'n_ratings': n_ratings, 'n_users': n_users, \n",
        "                'n_items': n_items, 'sparsity': sparsity}\n",
        "    \n",
        "    logger.log_info(\"Dataset Statistics:\")\n",
        "    logger.log_info(\"-\" * 70)\n",
        "    train_stats = get_stats(df_train, \"TRAIN\")\n",
        "    valid_stats = get_stats(df_valid, \"VALID\")\n",
        "    test_stats = get_stats(df_test, \"TEST\")\n",
        "    \n",
        "    # Load model\n",
        "    model_dir = MODELS_DIR / 'model' / category\n",
        "    if model_dir.exists():\n",
        "        logger.log_info(\"-\" * 70)\n",
        "        logger.log_info(\"Model Analysis:\")\n",
        "        \n",
        "        artifacts = load_model_artifacts(model_dir)\n",
        "        U, V = artifacts['U'], artifacts['V']\n",
        "        R = artifacts['R']\n",
        "        \n",
        "        logger.log_info(f\"  U (user factors): {U.shape}\")\n",
        "        logger.log_info(f\"  V (item factors): {V.shape}\")\n",
        "        logger.log_info(f\"  n_factors: {U.shape[1]}\")\n",
        "        \n",
        "        # Analyze factor magnitudes\n",
        "        logger.log_info(f\"\\nFactor Statistics:\")\n",
        "        logger.log_info(f\"  U magnitude: mean={np.abs(U).mean():.4f}, std={np.abs(U).std():.4f}\")\n",
        "        logger.log_info(f\"  V magnitude: mean={np.abs(V).mean():.4f}, std={np.abs(V).std():.4f}\")\n",
        "        \n",
        "        # Test reconstruction error on training data\n",
        "        R_pred = U @ V.T\n",
        "        train_rated = R.nonzero()\n",
        "        actual_vals = R.data\n",
        "        pred_vals = R_pred[train_rated[0], train_rated[1]]\n",
        "        \n",
        "        train_rmse = np.sqrt(mean_squared_error(actual_vals, pred_vals))\n",
        "        logger.log_info(f\"\\nTraining Set Reconstruction:\")\n",
        "        logger.log_info(f\"  RMSE: {train_rmse:.4f}\")\n",
        "        logger.log_info(f\"  MAE:  {np.mean(np.abs(actual_vals - pred_vals)):.4f}\")\n",
        "        \n",
        "        # Sparsity info\n",
        "        logger.log_info(f\"\\nCoverage:\")\n",
        "        logger.log_info(f\"  Model users: {len(artifacts['user_idx']):,}\")\n",
        "        logger.log_info(f\"  Model items: {len(artifacts['item_idx']):,}\")\n",
        "        logger.log_info(f\"  Matrix sparsity: {train_stats['sparsity']:.2%}\")\n",
        "    \n",
        "    logger.log_info(f\"\\n{'='*70}\\n\")\n",
        "\n",
        "# Run check\n",
        "check_model_quality(CATEGORY[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task: Unit test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### UI Recommendation Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recommend_model_ui(user_id: str, n_recs: int = 5, models_dir: str | Path | None = None, category: str | None = None) -> pl.DataFrame:\n",
        "    cat = category or CATEGORY[0]\n",
        "    model_dir = Path(models_dir) if models_dir else (MODELS_DIR / \"model\" / cat)\n",
        "    artifacts = load_model_artifacts(model_dir)\n",
        "    return recommend_model_based(user_id, n_recs, artifacts)\n",
        "\n",
        "def unit_test_ui_model_recommend(user_id: str, n_recs: int = 5, models_dir: str | Path | None = None, category: str | None = None):\n",
        "    cat = category or CATEGORY[0]\n",
        "    md = models_dir if models_dir else (MODELS_DIR / \"model\" / cat)\n",
        "    logger.log_info(f\"[UnitTest-UI-MODEL] model_dir={md} | user_id={user_id} | n_recs={n_recs}\")\n",
        "    recs = recommend_model_ui(user_id=user_id, n_recs=n_recs, models_dir=md, category=cat)\n",
        "    cols = set(recs.columns)\n",
        "    assert {\"parent_asin\", \"score\"}.issubset(cols), \"recs missing required columns\"\n",
        "    assert len(recs) <= n_recs, f\"recs length should be <= {n_recs}\"\n",
        "    logger.log_info(f\"[UnitTest-UI-MODEL] returned {len(recs)} items\")\n",
        "    display(recs)\n",
        "    return recs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test All Categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_all_categories():\n",
        "    \"\"\"Unit test: Verify recommendation function works for all categories\"\"\"\n",
        "    logger.log_info(\"\\n\" + \"=\"*70)\n",
        "    logger.log_info(\"[UNIT TEST] Testing Recommendation Function (Model-Based)\")\n",
        "    logger.log_info(\"=\"*70 + \"\\n\")\n",
        "    \n",
        "    test_summary = []\n",
        "    \n",
        "    for cat in CATEGORY:\n",
        "        logger.log_info(f\"\\n[Test] {cat}\")\n",
        "        logger.log_info(\"-\"*70)\n",
        "        \n",
        "        try:\n",
        "            model_dir = MODELS_DIR / \"model\" / cat\n",
        "            \n",
        "            if not model_dir.exists():\n",
        "                logger.log_warning(f\"  Model not found\")\n",
        "                test_summary.append({'category': cat, 'status': 'FAIL', 'reason': 'Model not found'})\n",
        "                continue\n",
        "            \n",
        "            # Load artifacts\n",
        "            artifacts = load_model_artifacts(model_dir)\n",
        "            best_factors = Configurations.load_best_factors(cat)\n",
        "            \n",
        "            user_rev = artifacts['user_rev']\n",
        "            item_rev = artifacts['item_rev']\n",
        "            U, V = artifacts['U'], artifacts['V']\n",
        "            \n",
        "            logger.log_info(f\"  Model loaded: {len(user_rev):,} users, {len(item_rev):,} items\")\n",
        "            logger.log_info(f\"  n_factors: {U.shape[1]}\")\n",
        "            logger.log_info(f\"  Best n_factors: {best_factors}\")\n",
        "            \n",
        "            if len(user_rev) == 0:\n",
        "                logger.log_warning(f\"  No users in model\")\n",
        "                test_summary.append({'category': cat, 'status': 'FAIL', 'reason': 'No users'})\n",
        "                continue\n",
        "            \n",
        "            # Test recommendation\n",
        "            sample_user = user_rev[0]\n",
        "            logger.log_info(f\"  Testing user: {sample_user}\")\n",
        "            \n",
        "            recs = recommend_model_ui(sample_user, n_recs=N_RECS, category=cat)\n",
        "            \n",
        "            # Validate output\n",
        "            assert set(recs.columns) >= {\"parent_asin\", \"score\"}, \"Missing required columns\"\n",
        "            assert len(recs) <= N_RECS, f\"Too many recommendations: {len(recs)}\"\n",
        "            \n",
        "            logger.log_info(f\"  Generated {len(recs)} recommendations\")\n",
        "            logger.log_info(f\"  Score range: [{recs['score'].min():.4f}, {recs['score'].max():.4f}]\")\n",
        "            \n",
        "            test_summary.append({\n",
        "                'category': cat, \n",
        "                'status': 'PASS', \n",
        "                'n_recs': len(recs),\n",
        "                'n_factors': U.shape[1],\n",
        "                'score_min': float(recs['score'].min()),\n",
        "                'score_max': float(recs['score'].max())\n",
        "            })\n",
        "            \n",
        "            display(recs.head(5))\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.log_exception(f\"  Error: {e}\")\n",
        "            test_summary.append({'category': cat, 'status': 'FAIL', 'reason': str(e)})\n",
        "    \n",
        "    # Summary\n",
        "    logger.log_info(\"\\n\" + \"=\"*70)\n",
        "    logger.log_info(\"UNIT TEST SUMMARY\")\n",
        "    logger.log_info(\"=\"*70)\n",
        "    \n",
        "    df_summary = pl.DataFrame(test_summary)\n",
        "    display(df_summary)\n",
        "    \n",
        "    passed = sum(1 for r in test_summary if r['status'] == 'PASS')\n",
        "    total = len(test_summary)\n",
        "    \n",
        "    logger.log_info(f\"\\nResults: {passed}/{total} categories passed\")\n",
        "    \n",
        "    if passed == total:\n",
        "        logger.log_info(\"ALL TESTS PASSED\")\n",
        "    else:\n",
        "        logger.log_warning(f\"{total - passed} tests failed\")\n",
        "    \n",
        "    logger.log_info(\"=\"*70 + \"\\n\")\n",
        "\n",
        "test_all_categories()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trending-Based Recommendation (Baseline + Cold-Start Handler)\n",
    "**Goal**\n",
    "- Build trending-based recommender for cold-start scenarios\n",
    "- Compute time-aware popularity scores with recency boost\n",
    "- Evaluate as baseline and cold-start fallback\n",
    "- Save artifacts for hybrid integration and API\n",
    "\n",
    "**What this notebook does**\n",
    "- 1. Load 5-core TRAIN from PROCESSED_DIR\n",
    "- 2. Compute trending scores: log(count) * avg_rating * recency_weight\n",
    "- 3. Build sparse matrix for consistency with other models\n",
    "- 4. Generate Top-N trending recommendations\n",
    "- 5. Evaluate on TEST/VALID sets\n",
    "- 6. Save artifacts for API integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Import libraries, define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, pickle\n",
    "import numpy as np, polars as pl\n",
    "from pathlib import Path\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..', '../utilities'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "from logger import Logger\n",
    "from configurations import Configurations\n",
    "from visualization_helpers import visualize_final_results\n",
    "\n",
    "m_log_file = Configurations.LOG_PATH\n",
    "logger = Logger(process_name=\"trending\", log_file=m_log_file)\n",
    "\n",
    "PROCESSED_DIR = Path(Configurations.DATA_PROCESSED_PATH)\n",
    "MODELS_DIR = Path(Configurations.MODELS_PATH)\n",
    "CATEGORY = Configurations.CATEGORIES\n",
    "\n",
    "logger.log_info(\"=\"*70)\n",
    "logger.log_info(\"TRENDING-BASED RECOMMENDATION\")\n",
    "logger.log_info(\"=\"*70)\n",
    "logger.log_info(f\"Categories: {CATEGORY}\")\n",
    "logger.log_info(f\"Sample size: {Configurations.DEV_SAMPLE_SIZE}\")\n",
    "logger.log_info(\"=\"*70 + \"\\n\")\n",
    "\n",
    "N_RECS = 10\n",
    "RECENCY_DAYS = 90  # Consider last 90 days as \"recent\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _candidate_files(category: str, split: str = \"train\"):\n",
    "    dev_sample_size = Configurations.DEV_SAMPLE_SIZE\n",
    "    safe_cat = category.replace('/', '-')\n",
    "    \n",
    "    if dev_sample_size != 'full':\n",
    "        for size_name in Configurations.SAMPLE_SIZES.keys():\n",
    "            if size_name == dev_sample_size:\n",
    "                return PROCESSED_DIR / f\"{safe_cat}.5core.{split}.{size_name}.parquet\"\n",
    "    else:\n",
    "        return PROCESSED_DIR / f\"{safe_cat}.5core.{split}.parquet\"\n",
    "\n",
    "def load_5core_data(category: str, split: str = \"train\") -> pl.DataFrame:\n",
    "    p = _candidate_files(category, split)\n",
    "    logger.log_info(f\"[Load-{split.upper()}] {p}\")\n",
    "    df = pl.read_parquet(p, low_memory=False)\n",
    "    logger.log_info(f\"[Load-{split.upper()}] shape={df.shape} | \"\n",
    "                   f\"users={df['user_id'].n_unique()} | \"\n",
    "                   f\"items={df['parent_asin'].n_unique()}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Build Trending Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trending_scores(df_train: pl.DataFrame, recency_days: int = 90):\n",
    "    \"\"\"\n",
    "    Compute trending scores with recency boost\n",
    "    Score = log(count) * avg_rating * recency_weight\n",
    "    recency_weight = 1.0 + 0.5 * (recent_count / total_count)\n",
    "    \"\"\"\n",
    "    logger.log_info(\"[Trending] Computing scores...\")\n",
    "    \n",
    "    # Handle timestamp conversion\n",
    "    if 'timestamp' in df_train.columns:\n",
    "        logger.log_info(\"[Trending] Checking timestamp format...\")\n",
    "        \n",
    "        # Sample to detect time unit\n",
    "        sample_ts = df_train['timestamp'].head(5).to_list()\n",
    "        logger.log_info(f\"[Trending] Sample timestamps: {sample_ts}\")\n",
    "        time_unit = 'ms'\n",
    "        \n",
    "        # Convert with correct unit\n",
    "        df_train = df_train.with_columns([\n",
    "            pl.from_epoch(pl.col('timestamp'), time_unit=time_unit).alias('datetime')\n",
    "        ])\n",
    "        \n",
    "        # Verify conversion\n",
    "        sample_dt = df_train['datetime'].head(3).to_list()\n",
    "        logger.log_info(f\"[Trending] Converted sample: {sample_dt}\")\n",
    "        \n",
    "        # Get date range\n",
    "        max_date = df_train['datetime'].max()\n",
    "        min_date = df_train['datetime'].min()\n",
    "        \n",
    "        logger.log_info(f\"[Trending] Date range: {min_date} to {max_date}\")\n",
    "        \n",
    "        # Sanity check: years should be 1970-2025\n",
    "        max_year = max_date.year if hasattr(max_date, 'year') else 0\n",
    "        \n",
    "        if max_year < 1970 or max_year > 2030:\n",
    "            logger.log_warning(f\"[Trending] Suspicious year: {max_year}, using popularity only\")\n",
    "            df_train = df_train.with_columns([pl.lit(False).alias('is_recent')])\n",
    "        else:\n",
    "            # Calculate cutoff\n",
    "            cutoff_date = max_date - timedelta(days=recency_days)\n",
    "            logger.log_info(f\"[Trending] Recent cutoff: {cutoff_date} ({recency_days} days)\")\n",
    "            \n",
    "            # Mark recent\n",
    "            df_train = df_train.with_columns([\n",
    "                (pl.col('datetime') >= cutoff_date).alias('is_recent')\n",
    "            ])\n",
    "            \n",
    "            n_recent = df_train['is_recent'].sum()\n",
    "            logger.log_info(f\"[Trending] Recent interactions: {n_recent:,} \"\n",
    "                           f\"({n_recent/len(df_train)*100:.1f}%)\")\n",
    "    \n",
    "    else:\n",
    "        logger.log_warning(\"[Trending] No timestamp, using simple popularity\")\n",
    "        df_train = df_train.with_columns([pl.lit(False).alias('is_recent')])\n",
    "    \n",
    "    # Compute statistics per item\n",
    "    item_stats = df_train.group_by('parent_asin').agg([\n",
    "        pl.count('rating').alias('n_ratings'),\n",
    "        pl.mean('rating').alias('avg_rating'),\n",
    "        pl.sum('is_recent').cast(pl.Int32).alias('n_recent')\n",
    "    ])\n",
    "    \n",
    "    # Calculate recency weight\n",
    "    item_stats = item_stats.with_columns([\n",
    "        (1.0 + 0.5 * (pl.col('n_recent') / pl.col('n_ratings'))).alias('recency_weight')\n",
    "    ])\n",
    "    \n",
    "    # Calculate trending score\n",
    "    item_stats = item_stats.with_columns([\n",
    "        (pl.col('n_ratings').log() * pl.col('avg_rating') * pl.col('recency_weight')).alias('trending_score')\n",
    "    ]).sort('trending_score', descending=True)\n",
    "    \n",
    "    logger.log_info(f\"[Trending] Computed for {len(item_stats):,} items\")\n",
    "    logger.log_info(f\"[Trending] Top item: {item_stats['parent_asin'][0]}, \"\n",
    "                   f\"score: {item_stats['trending_score'][0]:.2f}\")\n",
    "    logger.log_info(f\"[Trending] Avg recency weight: {item_stats['recency_weight'].mean():.2f}\")\n",
    "    \n",
    "    return item_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Build Sparse Matrix & Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trending_model(df_train: pl.DataFrame, item_stats: pl.DataFrame):    \n",
    "    logger.log_info(\"[Trending] Building sparse matrix...\")\n",
    "    \n",
    "    # Use items from statistics (sorted by trending score)\n",
    "    items = item_stats['parent_asin'].to_list()\n",
    "    users = df_train['user_id'].unique().to_list()\n",
    "    \n",
    "    user_idx = {u: i for i, u in enumerate(users)}\n",
    "    item_idx = {a: i for i, a in enumerate(items)}\n",
    "    \n",
    "    # Build rating matrix\n",
    "    u = np.array([user_idx[x] for x in df_train['user_id'].to_list()], dtype=np.int32)\n",
    "    i = np.array([item_idx[x] for x in df_train['parent_asin'].to_list()], dtype=np.int32)\n",
    "    v = np.array(df_train['rating'].to_list(), dtype=np.float32)\n",
    "    \n",
    "    nU, nI = len(users), len(items)\n",
    "    R = csr_matrix((v, (u, i)), shape=(nU, nI), dtype=np.float32)\n",
    "    \n",
    "    logger.log_info(f\"[Trending] Matrix: {R.shape}, nnz={R.nnz:,}, \"\n",
    "                   f\"sparsity={(1 - R.nnz/(nU*nI)):.2%}\")\n",
    "    \n",
    "    user_rev = np.array(users, dtype=object)\n",
    "    item_rev = np.array(items, dtype=object)\n",
    "    \n",
    "    return R, user_idx, item_idx, user_rev, item_rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_trending(user_id: str, n_recs: int, artifacts: dict) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate trending recommendations\n",
    "    Excludes items user already rated\n",
    "    \"\"\"\n",
    "    item_stats = artifacts['item_stats']\n",
    "    R = artifacts['R']\n",
    "    user_idx = artifacts['user_idx']\n",
    "    \n",
    "    # Get user's rated items\n",
    "    rated_items = set()\n",
    "    if user_id in user_idx:\n",
    "        u = int(user_idx[user_id])\n",
    "        rated_items = set(R.getrow(u).indices.tolist())\n",
    "    \n",
    "    # Get trending items\n",
    "    trending_items = item_stats['parent_asin'].to_list()\n",
    "    trending_scores = item_stats['trending_score'].to_list()\n",
    "    \n",
    "    # Filter out rated\n",
    "    recommendations = []\n",
    "    for item, score in zip(trending_items, trending_scores):\n",
    "        if item not in rated_items:\n",
    "            recommendations.append((item, score))\n",
    "            if len(recommendations) >= n_recs:\n",
    "                break\n",
    "    \n",
    "    if recommendations:\n",
    "        rec_items, rec_scores = zip(*recommendations)\n",
    "        return pl.DataFrame({\n",
    "            \"parent_asin\": list(rec_items),\n",
    "            \"score\": list(rec_scores)\n",
    "        })\n",
    "    \n",
    "    return pl.DataFrame({\"parent_asin\": [], \"score\": []})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Save/Load Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trending_artifacts(out_dir: Path, item_stats, R, user_rev, item_rev, \n",
    "                           user_idx, item_idx):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    item_stats.write_parquet(out_dir / \"item_stats.parquet\")\n",
    "    save_npz(out_dir / \"R.npz\", R)\n",
    "    \n",
    "    with open(out_dir / \"user_rev.pkl\", \"wb\") as f:\n",
    "        pickle.dump(user_rev, f)\n",
    "    with open(out_dir / \"item_rev.pkl\", \"wb\") as f:\n",
    "        pickle.dump(item_rev, f)\n",
    "    \n",
    "    (out_dir / \"user_idx.json\").write_text(\n",
    "        json.dumps({str(k): int(v) for k, v in user_idx.items()})\n",
    "    )\n",
    "    (out_dir / \"item_idx.json\").write_text(\n",
    "        json.dumps({str(k): int(v) for k, v in item_idx.items()})\n",
    "    )\n",
    "    \n",
    "    logger.log_info(f\"[Saved-Trending] {out_dir}\")\n",
    "\n",
    "\n",
    "def load_trending_artifacts(model_dir: Path):\n",
    "    item_stats = pl.read_parquet(model_dir / \"item_stats.parquet\")\n",
    "    R = load_npz(model_dir / \"R.npz\")\n",
    "    \n",
    "    with open(model_dir / \"user_rev.pkl\", \"rb\") as f:\n",
    "        user_rev = pickle.load(f)\n",
    "    with open(model_dir / \"item_rev.pkl\", \"rb\") as f:\n",
    "        item_rev = pickle.load(f)\n",
    "    \n",
    "    user_idx = json.loads((model_dir / \"user_idx.json\").read_text())\n",
    "    item_idx = json.loads((model_dir / \"item_idx.json\").read_text())\n",
    "    \n",
    "    return {\n",
    "        'item_stats': item_stats,\n",
    "        'R': R,\n",
    "        'user_rev': user_rev,\n",
    "        'item_rev': item_rev,\n",
    "        'user_idx': user_idx,\n",
    "        'item_idx': item_idx\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse_accuracy(predictions: np.ndarray, actuals: np.ndarray, threshold: float = 3.5):\n",
    "    mask = ~np.isnan(actuals)\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan, np.nan\n",
    "    pred_filtered = predictions[mask]\n",
    "    actual_filtered = actuals[mask]\n",
    "    rmse = np.sqrt(mean_squared_error(actual_filtered, pred_filtered))\n",
    "    accuracy = np.mean((pred_filtered >= threshold) == (actual_filtered >= threshold))\n",
    "    return rmse, accuracy\n",
    "\n",
    "def recall_at_k(recommended: list, relevant: set, k: int):\n",
    "    if len(relevant) == 0:\n",
    "        return 0.0\n",
    "    recommended_k = set(recommended[:k])\n",
    "    return len(recommended_k & relevant) / len(relevant)\n",
    "\n",
    "def ndcg_at_k(recommended: list, relevant: set, k: int):\n",
    "    recommended_k = recommended[:k]\n",
    "    dcg = sum([1.0 / np.log2(i + 2) if item in relevant else 0.0 \n",
    "              for i, item in enumerate(recommended_k)])\n",
    "    idcg = sum([1.0 / np.log2(i + 2) for i in range(min(len(relevant), k))])\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def map_at_k(recommended: list, relevant: set, k: int):\n",
    "    recommended_k = recommended[:k]\n",
    "    if len(relevant) == 0:\n",
    "        return 0.0\n",
    "    score, num_hits = 0.0, 0.0\n",
    "    for i, item in enumerate(recommended_k):\n",
    "        if item in relevant:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / min(len(relevant), k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trending(category: str, artifacts: dict, k_values: list = [10, 20, 50],\n",
    "                     split: str = \"test\", sample_users: int = 1000):    \n",
    "    logger.log_info(f\"[Eval-Trending] {category} on {split.upper()}\")\n",
    "    \n",
    "    df_eval = load_5core_data(category, split=split)\n",
    "    item_stats = artifacts['item_stats']\n",
    "    R = artifacts['R']\n",
    "    user_idx = artifacts['user_idx']\n",
    "    item_idx = artifacts['item_idx']\n",
    "    \n",
    "    # Filter to train users\n",
    "    train_users = list(user_idx.keys())\n",
    "    df_eval = df_eval.filter(pl.col('user_id').is_in(train_users))\n",
    "    \n",
    "    if len(df_eval) == 0:\n",
    "        logger.log_warning(\"[Eval-Trending] No data after filtering\")\n",
    "        return None\n",
    "    \n",
    "    logger.log_info(f\"[Eval-Trending] {len(df_eval):,} ratings, \"\n",
    "                   f\"{df_eval['user_id'].n_unique():,} users\")\n",
    "    \n",
    "    # Sample users\n",
    "    eval_users = df_eval['user_id'].unique().to_list()\n",
    "    if len(eval_users) > sample_users:\n",
    "        np.random.seed(42)\n",
    "        eval_users = np.random.choice(eval_users, sample_users, replace=False).tolist()\n",
    "    \n",
    "    logger.log_info(f\"[Eval-Trending] Evaluating {len(eval_users)} users...\")\n",
    "    \n",
    "    # Get trending order\n",
    "    trending_order = item_stats['parent_asin'].to_list()\n",
    "    \n",
    "    # Initialize metrics\n",
    "    metrics_acc = {\n",
    "        **{f'recall@{k}': [] for k in k_values},\n",
    "        **{f'ndcg@{k}': [] for k in k_values},\n",
    "        **{f'map@{k}': [] for k in k_values}\n",
    "    }\n",
    "    \n",
    "    evaluated = 0\n",
    "    \n",
    "    for user_id in eval_users:\n",
    "        if user_id not in user_idx:\n",
    "            continue\n",
    "        \n",
    "        # Get relevant items\n",
    "        user_eval = df_eval.filter(pl.col('user_id') == user_id)\n",
    "        relevant = set(user_eval.filter(pl.col('rating') >= 4)['parent_asin'].to_list())\n",
    "        \n",
    "        if len(relevant) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get user's rated items\n",
    "        u = int(user_idx[user_id])\n",
    "        rated = set(R.getrow(u).indices.tolist())\n",
    "        \n",
    "        # Recommend trending items (exclude rated)\n",
    "        recommended = [item for item in trending_order if item not in rated]\n",
    "        \n",
    "        evaluated += 1\n",
    "        \n",
    "        # Calculate metrics\n",
    "        for k in k_values:\n",
    "            metrics_acc[f'recall@{k}'].append(recall_at_k(recommended, relevant, k))\n",
    "            metrics_acc[f'ndcg@{k}'].append(ndcg_at_k(recommended, relevant, k))\n",
    "            metrics_acc[f'map@{k}'].append(map_at_k(recommended, relevant, k))\n",
    "    \n",
    "    logger.log_info(f\"[Eval-Trending] Actually evaluated: {evaluated} users\")\n",
    "    \n",
    "    # Aggregate\n",
    "    results = {\n",
    "        'category': category,\n",
    "        'split': split,\n",
    "        'n_users': evaluated,\n",
    "        'rmse': np.nan,  # Trending doesn't predict ratings\n",
    "        'accuracy': np.nan\n",
    "    }\n",
    "    \n",
    "    for k in k_values:\n",
    "        for metric in ['recall', 'ndcg', 'map']:\n",
    "            key = f'{metric}@{k}'\n",
    "            results[key] = np.mean(metrics_acc[key]) if metrics_acc[key] else 0.0\n",
    "    \n",
    "    logger.log_info(f\"[Eval-Trending] NDCG@10={results['ndcg@10']:.4f}, \"\n",
    "                   f\"Recall@10={results['recall@10']:.4f}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_trending_model(category: str):\n",
    "    \"\"\"Train trending model for category\"\"\"\n",
    "    \n",
    "    logger.log_info(f\"\\n{'='*70}\\nCATEGORY: {category}\\n{'='*70}\\n\")\n",
    "    \n",
    "    model_dir = MODELS_DIR / \"trending\" / category\n",
    "    \n",
    "    # Check if already exists\n",
    "    if (model_dir / \"R.npz\").exists():\n",
    "        logger.log_info(f\"[Skip] Model exists for {category}\\n\")\n",
    "        return\n",
    "    \n",
    "    # Load data\n",
    "    logger.log_info(\"[STEP 1] Loading data...\")\n",
    "    df_train = load_5core_data(category, split=\"train\")\n",
    "    \n",
    "    # Compute scores\n",
    "    logger.log_info(\"\\n[STEP 2] Computing trending scores...\")\n",
    "    item_stats = compute_trending_scores(df_train, recency_days=RECENCY_DAYS)\n",
    "    \n",
    "    # Build matrix\n",
    "    logger.log_info(\"\\n[STEP 3] Building sparse matrix...\")\n",
    "    R, user_idx, item_idx, user_rev, item_rev = build_trending_model(df_train, item_stats)\n",
    "    \n",
    "    # Save\n",
    "    logger.log_info(\"\\n[STEP 4] Saving artifacts...\")\n",
    "    save_trending_artifacts(model_dir, item_stats, R, user_rev, item_rev,\n",
    "                          user_idx, item_idx)\n",
    "    \n",
    "    logger.log_info(f\"\\n Model saved to: {model_dir}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traing all categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.log_info(\"\\n\" + \"=\"*70)\n",
    "logger.log_info(\"TRAINING ALL CATEGORIES\")\n",
    "logger.log_info(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for cat in CATEGORY:\n",
    "    try:\n",
    "        train_trending_model(cat)\n",
    "    except Exception as e:\n",
    "        logger.log_exception(f\"[Error] {cat}: {e}\")\n",
    "\n",
    "logger.log_info(\"\\n\" + \"=\"*70)\n",
    "logger.log_info(\"TRAINING COMPLETE\")\n",
    "logger.log_info(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.log_info(\"\\n\" + \"=\"*70)\n",
    "logger.log_info(\"EVALUATION ON TEST SET\")\n",
    "logger.log_info(\"=\"*70 + \"\\n\")\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for cat in CATEGORY:\n",
    "    logger.log_info(f\"\\n[Eval] {cat}\")\n",
    "    logger.log_info(\"-\"*70)\n",
    "    \n",
    "    try:\n",
    "        model_dir = MODELS_DIR / \"trending\" / cat\n",
    "        \n",
    "        if not model_dir.exists():\n",
    "            logger.log_warning(f\"  Model not found\")\n",
    "            continue\n",
    "        \n",
    "        # Load artifacts\n",
    "        artifacts = load_trending_artifacts(model_dir)\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluate_trending(\n",
    "            cat, artifacts,\n",
    "            k_values=[10, 20, 50],\n",
    "            split=\"test\",\n",
    "            sample_users=Configurations.get_eval_samples_final()\n",
    "        )\n",
    "        \n",
    "        if results:\n",
    "            test_results.append(results)\n",
    "            logger.log_info(f\"  NDCG@50: {results['ndc' \\\n",
    "            '0']:.4f}, \"\n",
    "                          f\"Recall@50: {results['recall@50']:.4f}\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.log_exception(f\"  Error: {e}\")\n",
    "\n",
    "# Save results\n",
    "if test_results:\n",
    "    df_results = pl.DataFrame(test_results)\n",
    "    \n",
    "    logger.log_info(\"\\n\" + \"=\"*70)\n",
    "    logger.log_info(\"FINAL RESULTS\")\n",
    "    logger.log_info(\"=\"*70)\n",
    "    display(df_results)\n",
    "    \n",
    "    out_csv = MODELS_DIR / 'trending' / 'final_test_results.csv'\n",
    "    df_results.write_csv(out_csv)\n",
    "    logger.log_info(f\"\\n Saved: {out_csv}\")\n",
    "    \n",
    "    # Visualize\n",
    "    visualize_final_results(\n",
    "        test_results,\n",
    "        save_dir=MODELS_DIR / 'trending',\n",
    "        algo_name='Trending-Based',\n",
    "        k_values=[10, 20, 50]\n",
    "    )\n",
    "    logger.log_info(\" Saved: evaluation_results.png\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_trending_ui(user_id: str, n_recs: int = 5, \n",
    "                         models_dir: Path = None, category: str = None) -> pl.DataFrame:\n",
    "    cat = category or CATEGORY[0]\n",
    "    model_dir = models_dir if models_dir else (MODELS_DIR / \"trending\" / cat)\n",
    "    artifacts = load_trending_artifacts(model_dir)\n",
    "    return recommend_trending(user_id, n_recs, artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all_categories():\n",
    "    logger.log_info(\"\\n\" + \"=\"*70)\n",
    "    logger.log_info(\"[UNIT TEST] Testing Recommendation Function\")\n",
    "    logger.log_info(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    test_summary = []\n",
    "    \n",
    "    for cat in CATEGORY:\n",
    "        logger.log_info(f\"\\n[Test] {cat}\")\n",
    "        logger.log_info(\"-\"*70)\n",
    "        \n",
    "        try:\n",
    "            model_dir = MODELS_DIR / \"trending\" / cat\n",
    "            \n",
    "            if not model_dir.exists():\n",
    "                logger.log_warning(\"  Model not found\")\n",
    "                test_summary.append({'category': cat, 'status': 'FAIL', 'reason': 'Not found'})\n",
    "                continue\n",
    "            \n",
    "            # Load artifacts\n",
    "            artifacts = load_trending_artifacts(model_dir)\n",
    "            user_rev = artifacts['user_rev']\n",
    "            item_stats = artifacts['item_stats']\n",
    "            \n",
    "            logger.log_info(f\"  Loaded: {len(user_rev):,} users, {len(item_stats):,} items\")\n",
    "            \n",
    "            if len(user_rev) == 0:\n",
    "                logger.log_warning(\"  No users\")\n",
    "                test_summary.append({'category': cat, 'status': 'FAIL', 'reason': 'No users'})\n",
    "                continue\n",
    "            \n",
    "            # Test with sample user\n",
    "            sample_user = user_rev[0]\n",
    "            logger.log_info(f\"  Testing user: {sample_user}\")\n",
    "            \n",
    "            recs = recommend_trending_ui(sample_user, n_recs=N_RECS, category=cat)\n",
    "            \n",
    "            assert set(recs.columns) >= {\"parent_asin\", \"score\"}, \"Missing columns\"\n",
    "            assert len(recs) <= N_RECS, f\"Too many recs: {len(recs)}\"\n",
    "            \n",
    "            logger.log_info(f\"  Generated {len(recs)} recommendations\")\n",
    "            logger.log_info(f\"  Score range: [{recs['score'].min():.2f}, {recs['score'].max():.2f}]\")\n",
    "            \n",
    "            test_summary.append({\n",
    "                'category': cat,\n",
    "                'status': 'PASS',\n",
    "                'n_recs': len(recs),\n",
    "                'score_min': float(recs['score'].min()),\n",
    "                'score_max': float(recs['score'].max())\n",
    "            })\n",
    "            \n",
    "            display(recs.head(5))\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.log_exception(f\"  Error: {e}\")\n",
    "            test_summary.append({'category': cat, 'status': 'FAIL', 'reason': str(e)})\n",
    "    \n",
    "    # Summary\n",
    "    logger.log_info(\"\\n\" + \"=\"*70)\n",
    "    logger.log_info(\"UNIT TEST SUMMARY\")\n",
    "    logger.log_info(\"=\"*70)\n",
    "    \n",
    "    df_summary = pl.DataFrame(test_summary)\n",
    "    display(df_summary)\n",
    "    \n",
    "    passed = sum(1 for r in test_summary if r['status'] == 'PASS')\n",
    "    logger.log_info(f\"\\nResults: {passed}/{len(test_summary)} passed\")\n",
    "    logger.log_info(\"=\"*70 + \"\\n\")\n",
    "\n",
    "test_all_categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_trending_distribution(category: str):\n",
    "    \"\"\"Analyze trending score distribution\"\"\"\n",
    "    \n",
    "    logger.log_info(f\"\\n{'='*70}\")\n",
    "    logger.log_info(f\"TRENDING ANALYSIS: {category}\")\n",
    "    logger.log_info(f\"{'='*70}\\n\")\n",
    "    \n",
    "    model_dir = MODELS_DIR / \"trending\" / category\n",
    "    if not model_dir.exists():\n",
    "        logger.log_warning(\"Model not found\")\n",
    "        return\n",
    "    \n",
    "    artifacts = load_trending_artifacts(model_dir)\n",
    "    item_stats = artifacts['item_stats']\n",
    "    \n",
    "    logger.log_info(\"Score Distribution:\")\n",
    "    logger.log_info(f\"  Total items: {len(item_stats):,}\")\n",
    "    logger.log_info(f\"  Score range: [{item_stats['trending_score'].min():.2f}, \"\n",
    "                   f\"{item_stats['trending_score'].max():.2f}]\")\n",
    "    logger.log_info(f\"  Mean score: {item_stats['trending_score'].mean():.2f}\")\n",
    "    logger.log_info(f\"  Median score: {item_stats['trending_score'].median():.2f}\")\n",
    "    \n",
    "    logger.log_info(\"\\nRecency Impact:\")\n",
    "    logger.log_info(f\"  Avg recency weight: {item_stats['recency_weight'].mean():.2f}\")\n",
    "    logger.log_info(f\"  Max recency weight: {item_stats['recency_weight'].max():.2f}\")\n",
    "    \n",
    "    # Top 10 trending\n",
    "    logger.log_info(\"\\nTop 10 Trending Items:\")\n",
    "    for i, row in enumerate(item_stats.head(10).iter_rows(named=True), 1):\n",
    "        logger.log_info(f\"  {i}. {row['parent_asin']}: score={row['trending_score']:.2f}, \"\n",
    "                       f\"ratings={row['n_ratings']}, recent={row['n_recent']}\")\n",
    "    \n",
    "    logger.log_info(f\"\\n{'='*70}\\n\")\n",
    "\n",
    "analyze_trending_distribution(CATEGORY[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
